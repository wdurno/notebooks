{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## Google's Generative AI search provided the initial draft of this code. \n",
        "## I have modified it since. \n",
        "## It provided these sources: \n",
        "## https://github.com/jk96491/RL_Algorithms?tab=readme-ov-file \n",
        "## https://github.com/abyaadrafid/Deep-Reinforcement-Learning \n",
        "## I couldn't find a draft of this code in the sources, \n",
        "## so Generative AI search may have generated the code. \n",
        "## Update: I expect the initial code was indeed AI-generated \n",
        "## because, while it looked correct, it was absolutely full \n",
        "## of mathematical failings. However, it looked good enough \n",
        "## to have me fooled at first glace - impressive! \n",
        "\n",
        "import random \n",
        "import numpy as np \n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim \n",
        "\n",
        "class Object(object):\n",
        "    pass\n",
        "\n",
        "class ReplayBuffer(): \n",
        "    def __init__(self, capacity=10000): \n",
        "        self.n = 0 \n",
        "        self.capacity = capacity \n",
        "        self.state_list = [] \n",
        "        self.action_list = [] \n",
        "        self.reward_list = [] \n",
        "        self.next_state_list = [] \n",
        "        self.done_list = [] \n",
        "        pass \n",
        "    def __len__(self):\n",
        "         return self.n \n",
        "    def add(self, state, action, reward, next_state, done): \n",
        "        if self.n >= self.capacity: \n",
        "            ## discard earliest observation \n",
        "            self.state_list = self.state_list[1:] \n",
        "            self.action_list = self.action_list[1:] \n",
        "            self.reward_list = self.reward_list[1:] \n",
        "            self.next_state_list = self.next_state_list[1:] \n",
        "            self.done_list = self.done_list[1:] \n",
        "            self.n -= 1 \n",
        "        pass \n",
        "        ## cast to torch  \n",
        "        state = torch.tensor(state) \n",
        "        action = torch.tensor(action) \n",
        "        reward = torch.tensor(reward) \n",
        "        next_state = torch.tensor(next_state) \n",
        "        done = torch.tensor(done) \n",
        "        ## append to buffer \n",
        "        self.state_list.append(state) \n",
        "        self.action_list.append(action) \n",
        "        self.reward_list.append(reward) \n",
        "        self.next_state_list.append(next_state) \n",
        "        self.done_list.append(done) \n",
        "        self.n += 1 \n",
        "        pass \n",
        "    def sample(self, batch_size=32): \n",
        "        ## sample lists \n",
        "        out = Object() ## transitions \n",
        "        out.state = [] \n",
        "        out.action = [] \n",
        "        out.reward = [] \n",
        "        out.next_state = [] \n",
        "        out.done = [] \n",
        "        for _ in range(batch_size): \n",
        "            idx = random.randint(0, self.n-1) \n",
        "            out.state.append(self.state_list[idx]) \n",
        "            out.action.append(self.action_list[idx]) \n",
        "            out.reward.append(self.reward_list[idx]) \n",
        "            out.next_state.append(self.next_state_list[idx]) \n",
        "            out.done.append(self.done_list[idx]) \n",
        "            pass \n",
        "        ## stack  \n",
        "        out.state = torch.stack(out.state) \n",
        "        out.action = torch.stack(out.action) \n",
        "        out.reward = torch.stack(out.reward).reshape([-1,1]) \n",
        "        out.next_state = torch.stack(out.next_state) \n",
        "        out.done = torch.stack(out.done).reshape([-1,1]) \n",
        "        return out \n",
        "    pass \n",
        "\n",
        "# Define the actor and critic networks\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(state_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        action = torch.tanh(self.fc3(x))\n",
        "\n",
        "        return action\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(state_dim + action_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = torch.cat([state, action], dim=1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        value = self.fc3(x)\n",
        "\n",
        "        return value\n",
        "\n",
        "# Create the actor and critic networks\n",
        "actor = Actor(state_dim=4, action_dim=1)\n",
        "critic = Critic(state_dim=4, action_dim=1)\n",
        "target_actor = Actor(state_dim=4, action_dim=1)\n",
        "target_critic = Critic(state_dim=4, action_dim=1)\n",
        "\n",
        "# Define the optimizers\n",
        "actor_optimizer = optim.Adam(actor.parameters(), lr=1e-5)\n",
        "critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)\n",
        "\n",
        "# Create the replay buffer\n",
        "replay_buffer = ReplayBuffer(capacity=100000) \n",
        "\n",
        "# Define the environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "reward_list = [] \n",
        "p_list = [] # DEBUG \n",
        "TAU = -1 ## 0.05  \n",
        "GAMMA = 0.99 \n",
        "\n",
        "# Train the agent\n",
        "for episode in range(1000):\n",
        "    state = env.reset()\n",
        "    target_actor.load_state_dict(actor.state_dict())\n",
        "    target_critic.load_state_dict(critic.state_dict())\n",
        "\n",
        "    for t in range(1000):\n",
        "        action = actor(torch.tensor(state)) \n",
        "        if np.random.binomial(1, max(0,50-episode)/50) > 0: \n",
        "            ## random action\n",
        "            action = torch.tensor(np.random.uniform(low=-1., high=1.)).reshape([1]) \n",
        "            pass \n",
        "        \n",
        "        action_p = action.item() * .5 + .5 \n",
        "        action_int = np.random.binomial(1, action_p) ## must be 0 or 1 \n",
        "        next_state, reward, done, _ = env.step(action_int) \n",
        "\n",
        "        replay_buffer.add(state, action, reward, next_state, done) \n",
        "\n",
        "        if len(replay_buffer) > 1000: \n",
        "            # Sample a batch of transitions from the replay buffer \n",
        "            transitions = replay_buffer.sample(batch_size=256) \n",
        "\n",
        "            # Calculate the target Q-values\n",
        "            target_Q = target_critic(transitions.next_state, target_actor(transitions.next_state))\n",
        "            target_Q = (1 - transitions.done.int()) * target_Q.clone().detach() * GAMMA + transitions.reward \n",
        "\n",
        "            # Calculate the current Q-values\n",
        "            current_Q = critic(transitions.state, transitions.action)\n",
        "\n",
        "            # Calculate the critic loss\n",
        "            critic_loss = torch.mean((target_Q - current_Q).pow(2)) \n",
        "\n",
        "            # Update the critic network \n",
        "            critic_optimizer.zero_grad() \n",
        "            critic_loss.backward() \n",
        "            critic_optimizer.step() \n",
        "            \n",
        "            if len(replay_buffer) > 1000: \n",
        "                # Calculate the actor loss \n",
        "                actor_loss = -torch.mean(critic(transitions.state, actor(transitions.state))) \n",
        "\n",
        "                # Update the actor network \n",
        "                actor_optimizer.zero_grad() \n",
        "                actor_loss.backward() \n",
        "                actor_optimizer.step() \n",
        "                pass \n",
        "            pass \n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Evaluate the agent\n",
        "    episode_reward = 0 \n",
        "    state = env.reset() \n",
        "\n",
        "    for t in range(1000): \n",
        "        action = actor(torch.tensor(state))\n",
        "\n",
        "        action_p = action.item() * .5 + .5 \n",
        "        action_int = np.random.binomial(1, action_p) \n",
        "        next_state, reward, done, _ = env.step(action_int) \n",
        "        p_list.append(action.item()) # DEBUG \n",
        "\n",
        "        episode_reward += reward\n",
        "        state = next_state \n",
        "\n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    reward_list.append(episode_reward)\n",
        "    print(f'Episode {episode}: {episode_reward}, len(replay_buffer): {len(replay_buffer)}')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_7840/3054674665.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  action = torch.tensor(action)\n/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/autograd/__init__.py:266: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n/anaconda/envs/azureml_py38/lib/python3.8/site-packages/astroid/node_classes.py:94: DeprecationWarning: The 'astroid.node_classes' module is deprecated and will be replaced by 'astroid.nodes' in astroid 3.0.0\n  warnings.warn(\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Episode 0: 14.0, len(replay_buffer): 29\nEpisode 1: 17.0, len(replay_buffer): 50\nEpisode 2: 28.0, len(replay_buffer): 66\nEpisode 3: 15.0, len(replay_buffer): 143\nEpisode 4: 22.0, len(replay_buffer): 188\nEpisode 5: 28.0, len(replay_buffer): 210\nEpisode 6: 21.0, len(replay_buffer): 226\nEpisode 7: 30.0, len(replay_buffer): 263\nEpisode 8: 19.0, len(replay_buffer): 282\nEpisode 9: 14.0, len(replay_buffer): 302\nEpisode 10: 12.0, len(replay_buffer): 319\nEpisode 11: 19.0, len(replay_buffer): 341\nEpisode 12: 13.0, len(replay_buffer): 368\nEpisode 13: 22.0, len(replay_buffer): 377\nEpisode 14: 15.0, len(replay_buffer): 399\nEpisode 15: 11.0, len(replay_buffer): 430\nEpisode 16: 29.0, len(replay_buffer): 445\nEpisode 17: 31.0, len(replay_buffer): 463\nEpisode 18: 14.0, len(replay_buffer): 484\nEpisode 19: 45.0, len(replay_buffer): 501\nEpisode 20: 20.0, len(replay_buffer): 509\nEpisode 21: 29.0, len(replay_buffer): 521\nEpisode 22: 9.0, len(replay_buffer): 541\nEpisode 23: 10.0, len(replay_buffer): 561\nEpisode 24: 24.0, len(replay_buffer): 584\nEpisode 25: 20.0, len(replay_buffer): 612\nEpisode 26: 34.0, len(replay_buffer): 627\nEpisode 27: 10.0, len(replay_buffer): 648\nEpisode 28: 10.0, len(replay_buffer): 666\nEpisode 29: 9.0, len(replay_buffer): 681\nEpisode 30: 16.0, len(replay_buffer): 695\nEpisode 31: 13.0, len(replay_buffer): 733\nEpisode 32: 13.0, len(replay_buffer): 756\nEpisode 33: 14.0, len(replay_buffer): 779\nEpisode 34: 11.0, len(replay_buffer): 792\nEpisode 35: 10.0, len(replay_buffer): 823\nEpisode 36: 22.0, len(replay_buffer): 880\nEpisode 37: 12.0, len(replay_buffer): 889\nEpisode 38: 17.0, len(replay_buffer): 900\nEpisode 39: 12.0, len(replay_buffer): 924\nEpisode 40: 13.0, len(replay_buffer): 967\nEpisode 41: 22.0, len(replay_buffer): 1020\nEpisode 42: 11.0, len(replay_buffer): 1034\nEpisode 43: 31.0, len(replay_buffer): 1051\nEpisode 44: 27.0, len(replay_buffer): 1064\nEpisode 45: 21.0, len(replay_buffer): 1091\nEpisode 46: 26.0, len(replay_buffer): 1105\nEpisode 47: 33.0, len(replay_buffer): 1117\nEpisode 48: 21.0, len(replay_buffer): 1143\nEpisode 49: 18.0, len(replay_buffer): 1175\nEpisode 50: 40.0, len(replay_buffer): 1226\nEpisode 51: 36.0, len(replay_buffer): 1325\nEpisode 52: 48.0, len(replay_buffer): 1350\nEpisode 53: 29.0, len(replay_buffer): 1412\nEpisode 54: 15.0, len(replay_buffer): 1451\n"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [1], line 181\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# Update the actor network \u001b[39;00m\n\u001b[1;32m    180\u001b[0m actor_optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \n\u001b[0;32m--> 181\u001b[0m \u001b[43mactor_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    182\u001b[0m actor_optimizer\u001b[38;5;241m.\u001b[39mstep() \n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m \n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1711377759757
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.plot(reward_list)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1711377759860
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.plot(p_list)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1711377759872
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}