{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## try adding memory to these agents \n",
        "\n",
        "import random \n",
        "import numpy as np \n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim \n",
        "from lanczos import l_lanczos, combine_krylov_spaces \n",
        "\n",
        "class Object(object):\n",
        "    pass\n",
        "\n",
        "class ReplayBuffer(): \n",
        "    def __init__(self, capacity=10000): \n",
        "        self.n = 0 \n",
        "        self.capacity = capacity \n",
        "        self.state_list = [] \n",
        "        self.action_list = [] \n",
        "        self.reward_list = [] \n",
        "        self.next_state_list = [] \n",
        "        self.done_list = [] \n",
        "        pass \n",
        "    def __len__(self):\n",
        "         return self.n \n",
        "    def add(self, state, action, reward, next_state, done): \n",
        "        if self.n >= self.capacity: \n",
        "            ## discard earliest observation \n",
        "            self.state_list = self.state_list[1:] \n",
        "            self.action_list = self.action_list[1:] \n",
        "            self.reward_list = self.reward_list[1:] \n",
        "            self.next_state_list = self.next_state_list[1:] \n",
        "            self.done_list = self.done_list[1:] \n",
        "            self.n -= 1 \n",
        "        pass \n",
        "        ## cast to torch  \n",
        "        state = torch.tensor(state) \n",
        "        action = torch.tensor(action) \n",
        "        reward = torch.tensor(reward) \n",
        "        next_state = torch.tensor(next_state) \n",
        "        done = torch.tensor(done) \n",
        "        ## append to buffer \n",
        "        self.state_list.append(state) \n",
        "        self.action_list.append(action) \n",
        "        self.reward_list.append(reward) \n",
        "        self.next_state_list.append(next_state) \n",
        "        self.done_list.append(done) \n",
        "        self.n += 1 \n",
        "        pass \n",
        "    def sample(self, batch_size=32, idx_list=None): \n",
        "        ## sample lists \n",
        "        out = Object() ## transitions \n",
        "        out.state = [] \n",
        "        out.action = [] \n",
        "        out.reward = [] \n",
        "        out.next_state = [] \n",
        "        out.done = [] \n",
        "        if idx_list is not None: \n",
        "            batch_size = len(idx_list) \n",
        "        for i in range(batch_size): \n",
        "            if idx_list is None: \n",
        "                idx = random.randint(0, self.n-1) \n",
        "            else: \n",
        "                idx = idx_list[i]\n",
        "                pass \n",
        "            out.state.append(self.state_list[idx]) \n",
        "            out.action.append(self.action_list[idx]) \n",
        "            out.reward.append(self.reward_list[idx]) \n",
        "            out.next_state.append(self.next_state_list[idx]) \n",
        "            out.done.append(self.done_list[idx]) \n",
        "            pass \n",
        "        ## stack  \n",
        "        out.state = torch.stack(out.state) \n",
        "        out.action = torch.stack(out.action) \n",
        "        out.reward = torch.stack(out.reward).reshape([-1,1]) \n",
        "        out.next_state = torch.stack(out.next_state) \n",
        "        out.done = torch.stack(out.done).reshape([-1,1]) \n",
        "        return out \n",
        "    def clear(self, n=None): \n",
        "        'clears first `n` transitions, or all if `n is None`'\n",
        "        if n is None: \n",
        "            n = self.n \n",
        "            pass \n",
        "        self.state_list = self.state_list[n:] \n",
        "        self.action_list = self.action_list[n:] \n",
        "        self.reward_list = self.reward_list[n:] \n",
        "        self.next_state_list = self.next_state_list[n:] \n",
        "        self.done_list = self.done_list[n:] \n",
        "        self.n = len(self.state_list) \n",
        "        pass \n",
        "    pass \n",
        "\n",
        "# Define the actor and critic networks \n",
        "class SSRAgent(nn.Module): \n",
        "    def __init__(self, replay_buffer, ssr_rank=2): \n",
        "        super(SSRAgent, self).__init__() \n",
        "        self.ssr_rank = ssr_rank \n",
        "        self.ssr_low_rank_matrix = None ## =: A \n",
        "        self.ssr_residual_diagonal = None ## =: resid \n",
        "        self.ssr_center = None \n",
        "        self.ssr_n = None \n",
        "        ## N * Fisher Information \\approx AA^T + resid \n",
        "        self.ssr_model_dimension = None \n",
        "        self.replay_buffer = replay_buffer \n",
        "        pass \n",
        "    def loss(self, transitions): \n",
        "        raise NotImplementedError('ERROR: loss not implemented!') \n",
        "    def memorize(self, n=None): \n",
        "        'memorize oldest `n` transitions, or all if `n is None`' \n",
        "        self.ssr_center = self.__get_param() ## elliptical centroid \n",
        "        if self.ssr_model_dimension is None: \n",
        "            self.ssr_model_dimension = self.ssr_center.shape[0] \n",
        "            pass \n",
        "        ssr_low_rank_matrix, ssr_residual_diagonal = l_lanczos(self.__get_grad_generator(n), self.ssr_rank, self.ssr_model_dimension, calc_diag=True) \n",
        "        if self.ssr_low_rank_matrix is None: \n",
        "            ## first memorization \n",
        "            self.ssr_low_rank_matrix = ssr_low_rank_matrix \n",
        "            self.ssr_residual_diagonal = ssr_residual_diagonal \n",
        "            self.ssr_n = n \n",
        "        else: \n",
        "            ## combine with previous memories \n",
        "            self.ssr_low_rank_matrix = combine_krylov_spaces(self.ssr_low_rank_matrix, ssr_low_rank_matrix) \n",
        "            self.ssr_residual_diagonal += ssr_residual_diagonal \n",
        "            self.ssr_n += n \n",
        "            pass \n",
        "        pass \n",
        "    def ssr(self, lmbda=None): \n",
        "        'get the ssr regularizer'\n",
        "        if self.ssr_low_rank_matrix is None: \n",
        "            return 0. \n",
        "        if lmbda is None: \n",
        "            lmbda = 1. \n",
        "            pass \n",
        "        p = self.__get_param() \n",
        "        p0 = self.ssr_center \n",
        "        d = p - p0 \n",
        "        A = self.ssr_low_rank_matrix \n",
        "        res = self.ssr_residual_diagonal \n",
        "        dA = d.matmul(A) \n",
        "        dAT = dA.transpose(0,1) \n",
        "        dresdT = d.matmul(res).matmul(d.transpose(0,1))\n",
        "        ssr_sum = dA.matmul(dAT) + dresdT \n",
        "        if lmbda is None: \n",
        "            return .5 * ssr_sum ## natural value ## TODO adjust losses away from means to use this \n",
        "        return lmbda * .5 * ssr_sum / self.ssr_n ## adjusted sample size \n",
        "    def __get_param(self): \n",
        "        return torch.cat([p.reshape([-1, 1]) for p in self.parameters()], dim=0) \n",
        "    def __get_get_grad_generator(self, n=None): \n",
        "        ## The double get hides `self` in a function context,  \n",
        "        ## packaging `get_grad_generator` for calling without \n",
        "        ## the SSRAgent instance.  \n",
        "        if n is None: \n",
        "            n = self.replay_buffer.n \n",
        "            pass \n",
        "        def get_grad_generator(): \n",
        "            'l-Lanczos alg uses grad at least `ssr_rank` times'\n",
        "            def grad_generator(): \n",
        "                self.eval() \n",
        "                for idx in range(n): \n",
        "                    transition = self.replay_buffer.sample(idx_list=[idx]) \n",
        "                    loss = self.loss(transition) \n",
        "                    loss.backward() \n",
        "                    grad_vec = torch.cat([p.grad.reshape([-1]) for p in self.parameters()]) \n",
        "                    yield grad_vec \n",
        "                    pass\n",
        "                pass \n",
        "            return grad_generator \n",
        "        pass \n",
        "    pass \n",
        "\n",
        "class Actor(SSRAgent):\n",
        "    def __init__(self, state_dim, action_dim, critic=None, replay_buffer=None, ssr_rank=2): \n",
        "        super(Actor, self).__init__(replay_buffer=replay_buffer, ssr_rank=ssr_rank) \n",
        "        self.fc1 = nn.Linear(state_dim, 256) \n",
        "        self.fc2 = nn.Linear(256, 128) \n",
        "        self.fc3 = nn.Linear(128, action_dim) \n",
        "        self.critic = critic \n",
        "        pass \n",
        "    def forward(self, state): \n",
        "        x = torch.relu(self.fc1(state)) \n",
        "        x = torch.relu(self.fc2(x)) \n",
        "        action = torch.tanh(self.fc3(x)) \n",
        "        return action\n",
        "    def loss(self, transitions): \n",
        "        if self.critic is None: \n",
        "            raise ValueError('ERROR: this model has no associated critic!') \n",
        "        return -torch.mean(self.critic(transitions.state, self(transitions.state))) \n",
        "    pass \n",
        "\n",
        "class Critic(SSRAgent):\n",
        "    def __init__(self, state_dim, action_dim, target_actor=None, target_critic=None, replay_buffer=None, ssr_rank=2): \n",
        "        super(Critic, self).__init__(replay_buffer=replay_buffer, ssr_rank=ssr_rank) \n",
        "        self.fc1 = nn.Linear(state_dim + action_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 1)\n",
        "        self.target_actor = target_actor \n",
        "        self.target_critic = target_critic \n",
        "        pass \n",
        "    def forward(self, state, action):\n",
        "        x = torch.cat([state, action], dim=1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        value = self.fc3(x)\n",
        "        return value\n",
        "    def loss(self, transitions): \n",
        "        if target_actor is None or target_critic is None: \n",
        "            raise ValueError('ERROR: target model missing!') \n",
        "        # Calculate the target Q-values\n",
        "        target_Q = self.target_critic(transitions.next_state, self.target_actor(transitions.next_state))\n",
        "        target_Q = (1 - transitions.done.int()) * target_Q.clone().detach() * GAMMA + transitions.reward \n",
        "        # Calculate the current Q-values\n",
        "        current_Q = self(transitions.state, transitions.action) \n",
        "        # Calculate the critic loss\n",
        "        return torch.mean((target_Q - current_Q).pow(2))\n",
        "    pass \n",
        "\n",
        "# Create the replay buffer \n",
        "replay_buffer = ReplayBuffer(capacity=100000) \n",
        "\n",
        "# Create the actor and critic networks\n",
        "target_critic = Critic(state_dim=4, action_dim=1) \n",
        "target_actor = Actor(state_dim=4, action_dim=1) \n",
        "critic = Critic(state_dim=4, action_dim=1, target_critic=target_critic, target_actor=target_actor) \n",
        "actor = Actor(state_dim=4, action_dim=1, critic=critic) \n",
        "\n",
        "# Define the optimizers\n",
        "actor_optimizer = optim.Adam(actor.parameters(), lr=1e-5)\n",
        "critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)\n",
        "\n",
        "# Define the environment \n",
        "env = gym.make('CartPole-v1') \n",
        "\n",
        "reward_list = [] \n",
        "p_list = [] # DEBUG \n",
        "GAMMA = 0.99 \n",
        "LAMBDA = 1. \n",
        "\n",
        "# Train the agent\n",
        "for episode in range(1000):\n",
        "    state = env.reset()\n",
        "    target_actor.load_state_dict(actor.state_dict())\n",
        "    target_critic.load_state_dict(critic.state_dict())\n",
        "\n",
        "    for t in range(1000):\n",
        "        action = actor(torch.tensor(state)) \n",
        "        if np.random.binomial(1, max(0,50-episode)/50) > 0: \n",
        "            ## random action\n",
        "            action = torch.tensor(np.random.uniform(low=-1., high=1.)).reshape([1]) \n",
        "            pass \n",
        "        \n",
        "        action_p = action.item() * .5 + .5 \n",
        "        action_int = np.random.binomial(1, action_p) ## must be 0 or 1 \n",
        "        next_state, reward, done, _ = env.step(action_int) \n",
        "\n",
        "        replay_buffer.add(state, action, reward, next_state, done) \n",
        "\n",
        "        if len(replay_buffer) > 1000: \n",
        "            # Sample a batch of transitions from the replay buffer \n",
        "            transitions = replay_buffer.sample(batch_size=256) \n",
        "\n",
        "            # Calculate the critic loss \n",
        "            critic_loss = critic.loss(transitions) + critic.ssr(LAMBDA) \n",
        "\n",
        "            # Update the critic network \n",
        "            critic_optimizer.zero_grad() \n",
        "            critic_loss.backward() \n",
        "            critic_optimizer.step() \n",
        "            \n",
        "            # Calculate the actor loss \n",
        "            actor_loss = actor.loss(transitions) + actor.ssr(LAMBDA) \n",
        "\n",
        "            # Update the actor network \n",
        "            actor_optimizer.zero_grad() \n",
        "            actor_loss.backward() \n",
        "            actor_optimizer.step() \n",
        "\n",
        "            if len(replay_buffer) > 2000: \n",
        "                ## memoryize every 1000 transitions \n",
        "                actor.memorize(1000) \n",
        "                critic.memorize(1000) \n",
        "                replay_buffer.clear(1000)  \n",
        "                pass \n",
        "            pass \n",
        "\n",
        "        state = next_state \n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Evaluate the agent\n",
        "    episode_reward = 0 \n",
        "    state = env.reset() \n",
        "\n",
        "    for t in range(1000): \n",
        "        action = actor(torch.tensor(state))\n",
        "\n",
        "        action_p = action.item() * .5 + .5 \n",
        "        action_int = np.random.binomial(1, action_p) \n",
        "        next_state, reward, done, _ = env.step(action_int) \n",
        "        p_list.append(action.item()) # DEBUG \n",
        "\n",
        "        episode_reward += reward\n",
        "        state = next_state \n",
        "\n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    reward_list.append(episode_reward)\n",
        "    print(f'Episode {episode}: {episode_reward}, len(replay_buffer): {len(replay_buffer)}')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_4190/468116197.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  action = torch.tensor(action)\n/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/autograd/__init__.py:266: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Episode 0: 29.0, len(replay_buffer): 44\nEpisode 1: 26.0, len(replay_buffer): 61\nEpisode 2: 12.0, len(replay_buffer): 77\nEpisode 3: 24.0, len(replay_buffer): 109\nEpisode 4: 18.0, len(replay_buffer): 123\nEpisode 5: 13.0, len(replay_buffer): 147\nEpisode 6: 12.0, len(replay_buffer): 160\nEpisode 7: 22.0, len(replay_buffer): 191\nEpisode 8: 15.0, len(replay_buffer): 221\nEpisode 9: 10.0, len(replay_buffer): 241\nEpisode 10: 28.0, len(replay_buffer): 255\nEpisode 11: 19.0, len(replay_buffer): 275\nEpisode 12: 13.0, len(replay_buffer): 307\nEpisode 13: 21.0, len(replay_buffer): 349\nEpisode 14: 12.0, len(replay_buffer): 398\nEpisode 15: 12.0, len(replay_buffer): 408\nEpisode 16: 22.0, len(replay_buffer): 425\nEpisode 17: 20.0, len(replay_buffer): 448\nEpisode 18: 27.0, len(replay_buffer): 461\nEpisode 19: 13.0, len(replay_buffer): 475\nEpisode 20: 16.0, len(replay_buffer): 489\nEpisode 21: 25.0, len(replay_buffer): 498\nEpisode 22: 15.0, len(replay_buffer): 519\nEpisode 23: 11.0, len(replay_buffer): 536\nEpisode 24: 42.0, len(replay_buffer): 560\nEpisode 25: 16.0, len(replay_buffer): 569\nEpisode 26: 9.0, len(replay_buffer): 587\nEpisode 27: 17.0, len(replay_buffer): 599\nEpisode 28: 18.0, len(replay_buffer): 610\nEpisode 29: 14.0, len(replay_buffer): 708\nEpisode 30: 16.0, len(replay_buffer): 727\nEpisode 31: 14.0, len(replay_buffer): 761\nEpisode 32: 10.0, len(replay_buffer): 783\nEpisode 33: 15.0, len(replay_buffer): 796\nEpisode 34: 23.0, len(replay_buffer): 814\nEpisode 35: 18.0, len(replay_buffer): 853\nEpisode 36: 24.0, len(replay_buffer): 872\nEpisode 37: 16.0, len(replay_buffer): 889\nEpisode 38: 21.0, len(replay_buffer): 910\nEpisode 39: 34.0, len(replay_buffer): 924\nEpisode 40: 41.0, len(replay_buffer): 936\nEpisode 41: 24.0, len(replay_buffer): 976\nEpisode 42: 29.0, len(replay_buffer): 992\nEpisode 43: 19.0, len(replay_buffer): 1011\nEpisode 44: 44.0, len(replay_buffer): 1022\nEpisode 45: 24.0, len(replay_buffer): 1037\nEpisode 46: 11.0, len(replay_buffer): 1055\nEpisode 47: 13.0, len(replay_buffer): 1067\nEpisode 48: 14.0, len(replay_buffer): 1085\nEpisode 49: 17.0, len(replay_buffer): 1097\nEpisode 50: 17.0, len(replay_buffer): 1109\nEpisode 51: 14.0, len(replay_buffer): 1124\nEpisode 52: 14.0, len(replay_buffer): 1187\nEpisode 53: 16.0, len(replay_buffer): 1202\nEpisode 54: 38.0, len(replay_buffer): 1217\nEpisode 55: 37.0, len(replay_buffer): 1260\nEpisode 56: 26.0, len(replay_buffer): 1282\nEpisode 57: 15.0, len(replay_buffer): 1312\nEpisode 58: 12.0, len(replay_buffer): 1332\nEpisode 59: 45.0, len(replay_buffer): 1365\nEpisode 60: 36.0, len(replay_buffer): 1382\nEpisode 61: 56.0, len(replay_buffer): 1439\nEpisode 62: 34.0, len(replay_buffer): 1489\nEpisode 63: 32.0, len(replay_buffer): 1525\nEpisode 64: 12.0, len(replay_buffer): 1574\nEpisode 65: 39.0, len(replay_buffer): 1614\nEpisode 66: 25.0, len(replay_buffer): 1649\nEpisode 67: 24.0, len(replay_buffer): 1747\nEpisode 68: 48.0, len(replay_buffer): 1857\nEpisode 69: 70.0, len(replay_buffer): 1888\nEpisode 70: 36.0, len(replay_buffer): 1920\nEpisode 71: 21.0, len(replay_buffer): 2043\nEpisode 72: 51.0, len(replay_buffer): 2137\nEpisode 73: 43.0, len(replay_buffer): 2241\nEpisode 74: 126.0, len(replay_buffer): 2328\nEpisode 75: 107.0, len(replay_buffer): 2460\nEpisode 76: 364.0, len(replay_buffer): 2701\nEpisode 77: 212.0, len(replay_buffer): 2851\nEpisode 78: 139.0, len(replay_buffer): 3020\nEpisode 79: 138.0, len(replay_buffer): 3215\nEpisode 80: 102.0, len(replay_buffer): 3315\nEpisode 81: 215.0, len(replay_buffer): 3473\nEpisode 82: 109.0, len(replay_buffer): 3587\nEpisode 83: 89.0, len(replay_buffer): 3691\nEpisode 84: 129.0, len(replay_buffer): 3778\nEpisode 85: 106.0, len(replay_buffer): 3870\nEpisode 86: 71.0, len(replay_buffer): 3961\nEpisode 87: 134.0, len(replay_buffer): 4074\nEpisode 88: 160.0, len(replay_buffer): 4216\n"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [2], line 185\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# Update the critic network \u001b[39;00m\n\u001b[1;32m    184\u001b[0m critic_optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \n\u001b[0;32m--> 185\u001b[0m \u001b[43mcritic_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    186\u001b[0m critic_optimizer\u001b[38;5;241m.\u001b[39mstep() \n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(replay_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1000\u001b[39m: \n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# Calculate the actor loss \u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1711296287711
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}