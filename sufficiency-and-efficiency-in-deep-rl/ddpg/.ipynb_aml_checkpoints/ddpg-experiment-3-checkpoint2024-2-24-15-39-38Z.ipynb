{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## try adding memory to these agents \n",
        "\n",
        "import random \n",
        "import numpy as np \n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim \n",
        "\n",
        "class Object(object):\n",
        "    pass\n",
        "\n",
        "class ReplayBuffer(): \n",
        "    def __init__(self, capacity=10000): \n",
        "        self.n = 0 \n",
        "        self.capacity = capacity \n",
        "        self.state_list = [] \n",
        "        self.action_list = [] \n",
        "        self.reward_list = [] \n",
        "        self.next_state_list = [] \n",
        "        self.done_list = [] \n",
        "        pass \n",
        "    def __len__(self):\n",
        "         return self.n \n",
        "    def add(self, state, action, reward, next_state, done): \n",
        "        if self.n >= self.capacity: \n",
        "            ## discard earliest observation \n",
        "            self.state_list = self.state_list[1:] \n",
        "            self.action_list = self.action_list[1:] \n",
        "            self.reward_list = self.reward_list[1:] \n",
        "            self.next_state_list = self.next_state_list[1:] \n",
        "            self.done_list = self.done_list[1:] \n",
        "            self.n -= 1 \n",
        "        pass \n",
        "        ## cast to torch  \n",
        "        state = torch.tensor(state) \n",
        "        action = torch.tensor(action) \n",
        "        reward = torch.tensor(reward) \n",
        "        next_state = torch.tensor(next_state) \n",
        "        done = torch.tensor(done) \n",
        "        ## append to buffer \n",
        "        self.state_list.append(state) \n",
        "        self.action_list.append(action) \n",
        "        self.reward_list.append(reward) \n",
        "        self.next_state_list.append(next_state) \n",
        "        self.done_list.append(done) \n",
        "        self.n += 1 \n",
        "        pass \n",
        "    def sample(self, batch_size=32): \n",
        "        ## sample lists \n",
        "        out = Object() ## transitions \n",
        "        out.state = [] \n",
        "        out.action = [] \n",
        "        out.reward = [] \n",
        "        out.next_state = [] \n",
        "        out.done = [] \n",
        "        for _ in range(batch_size): \n",
        "            idx = random.randint(0, self.n-1) \n",
        "            out.state.append(self.state_list[idx]) \n",
        "            out.action.append(self.action_list[idx]) \n",
        "            out.reward.append(self.reward_list[idx]) \n",
        "            out.next_state.append(self.next_state_list[idx]) \n",
        "            out.done.append(self.done_list[idx]) \n",
        "            pass \n",
        "        ## stack  \n",
        "        out.state = torch.stack(out.state) \n",
        "        out.action = torch.stack(out.action) \n",
        "        out.reward = torch.stack(out.reward).reshape([-1,1]) \n",
        "        out.next_state = torch.stack(out.next_state) \n",
        "        out.done = torch.stack(out.done).reshape([-1,1]) \n",
        "        return out \n",
        "    def clear(self): \n",
        "        self.n = 0 \n",
        "        self.state_list = [] \n",
        "        self.action_list = [] \n",
        "        self.reward_list = [] \n",
        "        self.next_state_list = [] \n",
        "        self.done_list = [] \n",
        "        pass \n",
        "    pass \n",
        "\n",
        "# Define the actor and critic networks \n",
        "class SSRAgent(nn.Module): \n",
        "    def __init__(self,  ssr_rank=2): \n",
        "        super(SSRAgent, self).__init__() \n",
        "        self.ssr_rank = ssr_rank \n",
        "        self.ssr_low_rank_matrix = None \n",
        "        self.ssr_diagonal = None \n",
        "        self.ssr_model_dimension = None \n",
        "        pass \n",
        "    def memorize(self): \n",
        "        ## TODO \n",
        "        pass \n",
        "    pass \n",
        "\n",
        "class Actor(SSRAgent):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(state_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        action = torch.tanh(self.fc3(x))\n",
        "\n",
        "        return action\n",
        "\n",
        "class Critic(SSRAgent):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(state_dim + action_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = torch.cat([state, action], dim=1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        value = self.fc3(x)\n",
        "\n",
        "        return value\n",
        "\n",
        "# Create the actor and critic networks\n",
        "actor = Actor(state_dim=4, action_dim=1)\n",
        "critic = Critic(state_dim=4, action_dim=1)\n",
        "target_actor = Actor(state_dim=4, action_dim=1)\n",
        "target_critic = Critic(state_dim=4, action_dim=1)\n",
        "\n",
        "# Define the optimizers\n",
        "actor_optimizer = optim.Adam(actor.parameters(), lr=1e-5)\n",
        "critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)\n",
        "\n",
        "# Create the replay buffer\n",
        "replay_buffer = ReplayBuffer(capacity=100000) \n",
        "\n",
        "# Define the environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "reward_list = [] \n",
        "p_list = [] # DEBUG \n",
        "TAU = -1 ## 0.05  \n",
        "GAMMA = 0.99 \n",
        "\n",
        "# Train the agent\n",
        "for episode in range(1000):\n",
        "    state = env.reset()\n",
        "    target_actor.load_state_dict(actor.state_dict())\n",
        "    target_critic.load_state_dict(critic.state_dict())\n",
        "\n",
        "    for t in range(1000):\n",
        "        action = actor(torch.tensor(state)) \n",
        "        if np.random.binomial(1, max(0,50-episode)/50) > 0: \n",
        "            ## random action\n",
        "            action = torch.tensor(np.random.uniform(low=-1., high=1.)).reshape([1]) \n",
        "            pass \n",
        "        \n",
        "        action_p = action.item() * .5 + .5 \n",
        "        action_int = np.random.binomial(1, action_p) ## must be 0 or 1 \n",
        "        next_state, reward, done, _ = env.step(action_int) \n",
        "\n",
        "        replay_buffer.add(state, action, reward, next_state, done) \n",
        "\n",
        "        if len(replay_buffer) > 1000: \n",
        "            # Sample a batch of transitions from the replay buffer \n",
        "            transitions = replay_buffer.sample(batch_size=256) \n",
        "\n",
        "            # Calculate the target Q-values\n",
        "            target_Q = target_critic(transitions.next_state, target_actor(transitions.next_state))\n",
        "            target_Q = (1 - transitions.done.int()) * target_Q.clone().detach() * GAMMA + transitions.reward \n",
        "\n",
        "            # Calculate the current Q-values\n",
        "            current_Q = critic(transitions.state, transitions.action)\n",
        "\n",
        "            # Calculate the critic loss\n",
        "            critic_loss = torch.mean((target_Q - current_Q).pow(2)) \n",
        "\n",
        "            # Update the critic network \n",
        "            critic_optimizer.zero_grad() \n",
        "            critic_loss.backward() \n",
        "            critic_optimizer.step() \n",
        "            \n",
        "            if len(replay_buffer) > 1000: \n",
        "                # Calculate the actor loss \n",
        "                actor_loss = -torch.mean(critic(transitions.state, actor(transitions.state))) \n",
        "\n",
        "                # Update the actor network \n",
        "                actor_optimizer.zero_grad() \n",
        "                actor_loss.backward() \n",
        "                actor_optimizer.step() \n",
        "                pass \n",
        "            pass \n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Evaluate the agent\n",
        "    episode_reward = 0 \n",
        "    state = env.reset() \n",
        "\n",
        "    for t in range(1000): \n",
        "        action = actor(torch.tensor(state))\n",
        "\n",
        "        action_p = action.item() * .5 + .5 \n",
        "        action_int = np.random.binomial(1, action_p) \n",
        "        next_state, reward, done, _ = env.step(action_int) \n",
        "        p_list.append(action.item()) # DEBUG \n",
        "\n",
        "        episode_reward += reward\n",
        "        state = next_state \n",
        "\n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    reward_list.append(episode_reward)\n",
        "    print(f'Episode {episode}: {episode_reward}, len(replay_buffer): {len(replay_buffer)}')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1711294709632
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}