{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## Google's Generative AI search provided the initial draft of this code. \n",
        "## I have modified it since. \n",
        "## It provided these sources: \n",
        "## https://github.com/jk96491/RL_Algorithms?tab=readme-ov-file \n",
        "## https://github.com/abyaadrafid/Deep-Reinforcement-Learning \n",
        "## I couldn't find a draft of this code in the sources, \n",
        "## so Generative AI search may have generated the code. \n",
        "## Update: I expect the initial code was indeed AI-generated \n",
        "## because, while it looked correct, it was absolutely full \n",
        "## of mathematical failings. However, it looked good enough \n",
        "## to have me fooled at first glace - impressive! \n",
        "\n",
        "import random \n",
        "import numpy as np \n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim \n",
        "\n",
        "class Object(object):\n",
        "    pass\n",
        "\n",
        "class ReplayBuffer(): \n",
        "    def __init__(self, capacity=10000): \n",
        "        self.n = 0 \n",
        "        self.capacity = capacity \n",
        "        self.state_list = [] \n",
        "        self.action_list = [] \n",
        "        self.reward_list = [] \n",
        "        self.next_state_list = [] \n",
        "        self.done_list = [] \n",
        "        pass \n",
        "    def __len__(self):\n",
        "         return self.n \n",
        "    def add(self, state, action, reward, next_state, done): \n",
        "        if self.n >= self.capacity: \n",
        "            ## discard earliest observation \n",
        "            self.state_list = self.state_list[1:] \n",
        "            self.action_list = self.action_list[1:] \n",
        "            self.reward_list = self.reward_list[1:] \n",
        "            self.next_state_list = self.next_state_list[1:] \n",
        "            self.done_list = self.done_list[1:] \n",
        "            self.n -= 1 \n",
        "        pass \n",
        "        ## cast to torch  \n",
        "        state = torch.tensor(state) \n",
        "        action = torch.tensor(action) \n",
        "        reward = torch.tensor(reward) \n",
        "        next_state = torch.tensor(next_state) \n",
        "        done = torch.tensor(done) \n",
        "        ## append to buffer \n",
        "        self.state_list.append(state) \n",
        "        self.action_list.append(action) \n",
        "        self.reward_list.append(reward) \n",
        "        self.next_state_list.append(next_state) \n",
        "        self.done_list.append(done) \n",
        "        self.n += 1 \n",
        "        pass \n",
        "    def sample(self, batch_size=32): \n",
        "        ## sample lists \n",
        "        out = Object() ## transitions \n",
        "        out.state = [] \n",
        "        out.action = [] \n",
        "        out.reward = [] \n",
        "        out.next_state = [] \n",
        "        out.done = [] \n",
        "        for _ in range(batch_size): \n",
        "            idx = random.randint(0, self.n-1) \n",
        "            out.state.append(self.state_list[idx]) \n",
        "            out.action.append(self.action_list[idx]) \n",
        "            out.reward.append(self.reward_list[idx]) \n",
        "            out.next_state.append(self.next_state_list[idx]) \n",
        "            out.done.append(self.done_list[idx]) \n",
        "            pass \n",
        "        ## stack  \n",
        "        out.state = torch.stack(out.state) \n",
        "        out.action = torch.stack(out.action) \n",
        "        out.reward = torch.stack(out.reward).reshape([-1,1]) \n",
        "        out.next_state = torch.stack(out.next_state) \n",
        "        out.done = torch.stack(out.done).reshape([-1,1]) \n",
        "        return out \n",
        "    pass \n",
        "\n",
        "# Define the actor and critic networks\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(state_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        action = torch.tanh(self.fc3(x))\n",
        "\n",
        "        return action\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(state_dim + action_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = torch.cat([state, action], dim=1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        value = self.fc3(x)\n",
        "\n",
        "        return value\n",
        "\n",
        "# Create the actor and critic networks\n",
        "actor = Actor(state_dim=4, action_dim=1)\n",
        "critic = Critic(state_dim=4, action_dim=1)\n",
        "target_actor = Actor(state_dim=4, action_dim=1)\n",
        "target_critic = Critic(state_dim=4, action_dim=1)\n",
        "\n",
        "# Define the optimizers\n",
        "actor_optimizer = optim.Adam(actor.parameters(), lr=1e-5)\n",
        "critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)\n",
        "\n",
        "# Create the replay buffer\n",
        "replay_buffer = ReplayBuffer(capacity=100000) \n",
        "\n",
        "# Define the environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "reward_list = [] \n",
        "p_list = [] # DEBUG \n",
        "TAU = -1 ## 0.05  \n",
        "GAMMA = 0.99 \n",
        "\n",
        "# Train the agent\n",
        "for episode in range(1000):\n",
        "    state = env.reset()\n",
        "    target_actor.load_state_dict(actor.state_dict())\n",
        "    target_critic.load_state_dict(critic.state_dict())\n",
        "\n",
        "    for t in range(1000):\n",
        "        action = actor(torch.tensor(state)) \n",
        "        if np.random.binomial(1, max(0,50-episode)/50) > 0: \n",
        "            ## random action\n",
        "            action = torch.tensor(np.random.uniform(low=-1., high=1.)).reshape([1]) \n",
        "            pass \n",
        "        \n",
        "        action_p = action.item() * .5 + .5 \n",
        "        action_int = np.random.binomial(1, action_p) ## must be 0 or 1 \n",
        "        next_state, reward, done, _ = env.step(action_int) \n",
        "\n",
        "        replay_buffer.add(state, action, reward, next_state, done) \n",
        "\n",
        "        if len(replay_buffer) > 1000: \n",
        "            # Sample a batch of transitions from the replay buffer \n",
        "            transitions = replay_buffer.sample(batch_size=256) \n",
        "\n",
        "            # Calculate the target Q-values\n",
        "            target_Q = target_critic(transitions.next_state, target_actor(transitions.next_state))\n",
        "            target_Q = (1 - transitions.done.int()) * target_Q.clone().detach() * GAMMA + transitions.reward \n",
        "\n",
        "            # Calculate the current Q-values\n",
        "            current_Q = critic(transitions.state, transitions.action)\n",
        "\n",
        "            # Calculate the critic loss\n",
        "            critic_loss = torch.mean((target_Q - current_Q).pow(2)) \n",
        "\n",
        "            # Update the critic network \n",
        "            critic_optimizer.zero_grad() \n",
        "            critic_loss.backward() \n",
        "            critic_optimizer.step() \n",
        "            \n",
        "            if len(replay_buffer) > 1000: \n",
        "                # Calculate the actor loss \n",
        "                actor_loss = -torch.mean(critic(transitions.state, actor(transitions.state))) \n",
        "\n",
        "                # Update the actor network \n",
        "                actor_optimizer.zero_grad() \n",
        "                actor_loss.backward() \n",
        "                actor_optimizer.step() \n",
        "                pass \n",
        "            pass \n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Evaluate the agent\n",
        "    episode_reward = 0 \n",
        "    state = env.reset() \n",
        "\n",
        "    for t in range(1000): \n",
        "        action = actor(torch.tensor(state))\n",
        "\n",
        "        action_p = action.item() * .5 + .5 \n",
        "        action_int = np.random.binomial(1, action_p) \n",
        "        next_state, reward, done, _ = env.step(action_int) \n",
        "        p_list.append(action.item()) # DEBUG \n",
        "\n",
        "        episode_reward += reward\n",
        "        state = next_state \n",
        "\n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    reward_list.append(episode_reward)\n",
        "    print(f'Episode {episode}: {episode_reward}, len(replay_buffer): {len(replay_buffer)}')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_22819/3054674665.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  action = torch.tensor(action)\n/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/autograd/__init__.py:266: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Episode 0: 12.0, len(replay_buffer): 26\nEpisode 1: 16.0, len(replay_buffer): 41\nEpisode 2: 32.0, len(replay_buffer): 50\nEpisode 3: 27.0, len(replay_buffer): 69\nEpisode 4: 12.0, len(replay_buffer): 94\nEpisode 5: 11.0, len(replay_buffer): 118\nEpisode 6: 36.0, len(replay_buffer): 148\nEpisode 7: 27.0, len(replay_buffer): 204\nEpisode 8: 38.0, len(replay_buffer): 217\nEpisode 9: 22.0, len(replay_buffer): 228\nEpisode 10: 15.0, len(replay_buffer): 246\nEpisode 11: 15.0, len(replay_buffer): 262\nEpisode 12: 16.0, len(replay_buffer): 285\nEpisode 13: 29.0, len(replay_buffer): 299\nEpisode 14: 24.0, len(replay_buffer): 322\nEpisode 15: 40.0, len(replay_buffer): 335\nEpisode 16: 15.0, len(replay_buffer): 350\nEpisode 17: 23.0, len(replay_buffer): 441\nEpisode 18: 37.0, len(replay_buffer): 470\nEpisode 19: 14.0, len(replay_buffer): 487\nEpisode 20: 30.0, len(replay_buffer): 547\nEpisode 21: 12.0, len(replay_buffer): 581\nEpisode 22: 13.0, len(replay_buffer): 610\nEpisode 23: 46.0, len(replay_buffer): 630\nEpisode 24: 22.0, len(replay_buffer): 648\nEpisode 25: 23.0, len(replay_buffer): 664\nEpisode 26: 22.0, len(replay_buffer): 744\nEpisode 27: 20.0, len(replay_buffer): 763\nEpisode 28: 27.0, len(replay_buffer): 795\nEpisode 29: 20.0, len(replay_buffer): 805\nEpisode 30: 53.0, len(replay_buffer): 825\nEpisode 31: 16.0, len(replay_buffer): 866\nEpisode 32: 22.0, len(replay_buffer): 879\nEpisode 33: 34.0, len(replay_buffer): 908\nEpisode 34: 18.0, len(replay_buffer): 930\nEpisode 35: 13.0, len(replay_buffer): 973\nEpisode 36: 42.0, len(replay_buffer): 985\nEpisode 37: 19.0, len(replay_buffer): 1016\nEpisode 38: 26.0, len(replay_buffer): 1045\nEpisode 39: 14.0, len(replay_buffer): 1061\nEpisode 40: 12.0, len(replay_buffer): 1081\nEpisode 41: 12.0, len(replay_buffer): 1091\nEpisode 42: 13.0, len(replay_buffer): 1109\nEpisode 43: 22.0, len(replay_buffer): 1130\nEpisode 44: 16.0, len(replay_buffer): 1160\nEpisode 45: 22.0, len(replay_buffer): 1175\nEpisode 46: 20.0, len(replay_buffer): 1200\nEpisode 47: 12.0, len(replay_buffer): 1224\nEpisode 48: 15.0, len(replay_buffer): 1247\nEpisode 49: 20.0, len(replay_buffer): 1280\nEpisode 50: 13.0, len(replay_buffer): 1310\nEpisode 51: 16.0, len(replay_buffer): 1376\nEpisode 52: 17.0, len(replay_buffer): 1405\nEpisode 53: 33.0, len(replay_buffer): 1421\nEpisode 54: 30.0, len(replay_buffer): 1458\nEpisode 55: 37.0, len(replay_buffer): 1486\nEpisode 56: 35.0, len(replay_buffer): 1513\nEpisode 57: 30.0, len(replay_buffer): 1526\nEpisode 58: 31.0, len(replay_buffer): 1542\nEpisode 59: 16.0, len(replay_buffer): 1556\nEpisode 60: 51.0, len(replay_buffer): 1630\nEpisode 61: 63.0, len(replay_buffer): 1703\nEpisode 62: 45.0, len(replay_buffer): 1774\nEpisode 63: 17.0, len(replay_buffer): 1893\nEpisode 64: 40.0, len(replay_buffer): 1937\nEpisode 65: 33.0, len(replay_buffer): 2001\nEpisode 66: 23.0, len(replay_buffer): 2092\nEpisode 67: 34.0, len(replay_buffer): 2129\nEpisode 68: 53.0, len(replay_buffer): 2147\nEpisode 69: 48.0, len(replay_buffer): 2184\nEpisode 70: 46.0, len(replay_buffer): 2205\nEpisode 71: 58.0, len(replay_buffer): 2233\nEpisode 72: 48.0, len(replay_buffer): 2261\nEpisode 73: 39.0, len(replay_buffer): 2295\nEpisode 74: 25.0, len(replay_buffer): 2329\nEpisode 75: 124.0, len(replay_buffer): 2364\nEpisode 76: 23.0, len(replay_buffer): 2421\nEpisode 77: 48.0, len(replay_buffer): 2477\nEpisode 78: 36.0, len(replay_buffer): 2504\nEpisode 79: 132.0, len(replay_buffer): 2604\nEpisode 80: 69.0, len(replay_buffer): 2697\nEpisode 81: 74.0, len(replay_buffer): 2773\nEpisode 82: 94.0, len(replay_buffer): 2919\nEpisode 83: 80.0, len(replay_buffer): 3022\nEpisode 84: 83.0, len(replay_buffer): 3144\nEpisode 85: 99.0, len(replay_buffer): 3251\nEpisode 86: 284.0, len(replay_buffer): 3356\nEpisode 87: 351.0, len(replay_buffer): 3451\nEpisode 88: 363.0, len(replay_buffer): 3951\nEpisode 89: 270.0, len(replay_buffer): 4274\nEpisode 90: 353.0, len(replay_buffer): 4595\nEpisode 91: 342.0, len(replay_buffer): 4826\nEpisode 92: 368.0, len(replay_buffer): 5193\nEpisode 93: 234.0, len(replay_buffer): 5518\nEpisode 94: 150.0, len(replay_buffer): 5696\nEpisode 95: 131.0, len(replay_buffer): 5846\nEpisode 96: 108.0, len(replay_buffer): 5966\nEpisode 97: 123.0, len(replay_buffer): 6064\nEpisode 98: 101.0, len(replay_buffer): 6171\nEpisode 99: 99.0, len(replay_buffer): 6264\nEpisode 100: 112.0, len(replay_buffer): 6371\nEpisode 101: 153.0, len(replay_buffer): 6494\nEpisode 102: 200.0, len(replay_buffer): 6661\nEpisode 103: 142.0, len(replay_buffer): 6819\nEpisode 104: 229.0, len(replay_buffer): 7084\nEpisode 105: 164.0, len(replay_buffer): 7467\nEpisode 106: 132.0, len(replay_buffer): 7618\nEpisode 107: 130.0, len(replay_buffer): 7735\nEpisode 108: 116.0, len(replay_buffer): 7878\nEpisode 109: 113.0, len(replay_buffer): 8011\nEpisode 110: 130.0, len(replay_buffer): 8147\nEpisode 111: 129.0, len(replay_buffer): 8276\nEpisode 112: 116.0, len(replay_buffer): 8398\nEpisode 113: 118.0, len(replay_buffer): 8511\nEpisode 114: 106.0, len(replay_buffer): 8613\nEpisode 115: 109.0, len(replay_buffer): 8632\nEpisode 116: 109.0, len(replay_buffer): 8744\nEpisode 117: 21.0, len(replay_buffer): 8860\nEpisode 118: 113.0, len(replay_buffer): 8975\nEpisode 119: 113.0, len(replay_buffer): 9090\nEpisode 120: 111.0, len(replay_buffer): 9192\nEpisode 121: 110.0, len(replay_buffer): 9301\nEpisode 122: 102.0, len(replay_buffer): 9401\nEpisode 123: 100.0, len(replay_buffer): 9499\nEpisode 124: 110.0, len(replay_buffer): 9604\nEpisode 125: 105.0, len(replay_buffer): 9702\nEpisode 126: 113.0, len(replay_buffer): 9817\nEpisode 127: 116.0, len(replay_buffer): 9924\nEpisode 128: 108.0, len(replay_buffer): 10038\nEpisode 129: 106.0, len(replay_buffer): 10145\nEpisode 130: 117.0, len(replay_buffer): 10244\nEpisode 131: 112.0, len(replay_buffer): 10349\nEpisode 132: 108.0, len(replay_buffer): 10453\nEpisode 133: 126.0, len(replay_buffer): 10555\nEpisode 134: 114.0, len(replay_buffer): 10671\nEpisode 135: 121.0, len(replay_buffer): 10782\nEpisode 136: 118.0, len(replay_buffer): 10894\nEpisode 137: 116.0, len(replay_buffer): 11015\nEpisode 138: 112.0, len(replay_buffer): 11120\nEpisode 139: 106.0, len(replay_buffer): 11224\nEpisode 140: 98.0, len(replay_buffer): 11324\nEpisode 141: 105.0, len(replay_buffer): 11422\nEpisode 142: 111.0, len(replay_buffer): 11525\nEpisode 143: 112.0, len(replay_buffer): 11637\nEpisode 144: 106.0, len(replay_buffer): 11743\nEpisode 145: 103.0, len(replay_buffer): 11854\nEpisode 146: 117.0, len(replay_buffer): 11961\nEpisode 147: 120.0, len(replay_buffer): 12089\nEpisode 148: 129.0, len(replay_buffer): 12206\nEpisode 149: 125.0, len(replay_buffer): 12322\nEpisode 150: 116.0, len(replay_buffer): 12449\nEpisode 151: 102.0, len(replay_buffer): 12570\nEpisode 152: 97.0, len(replay_buffer): 12681\nEpisode 153: 108.0, len(replay_buffer): 12791\nEpisode 154: 105.0, len(replay_buffer): 12899\nEpisode 155: 106.0, len(replay_buffer): 13009\nEpisode 156: 103.0, len(replay_buffer): 13115\nEpisode 157: 112.0, len(replay_buffer): 13228\nEpisode 158: 115.0, len(replay_buffer): 13345\nEpisode 159: 116.0, len(replay_buffer): 13470\nEpisode 160: 136.0, len(replay_buffer): 13601\nEpisode 161: 155.0, len(replay_buffer): 13746\nEpisode 162: 161.0, len(replay_buffer): 13898\nEpisode 163: 166.0, len(replay_buffer): 14057\nEpisode 164: 168.0, len(replay_buffer): 14213\nEpisode 165: 180.0, len(replay_buffer): 14381\nEpisode 166: 185.0, len(replay_buffer): 14546\nEpisode 167: 161.0, len(replay_buffer): 14716\nEpisode 168: 171.0, len(replay_buffer): 14891\nEpisode 169: 157.0, len(replay_buffer): 15042\nEpisode 170: 148.0, len(replay_buffer): 15191\nEpisode 171: 144.0, len(replay_buffer): 15336\nEpisode 172: 127.0, len(replay_buffer): 15467\nEpisode 173: 120.0, len(replay_buffer): 15600\nEpisode 174: 105.0, len(replay_buffer): 15711\nEpisode 175: 103.0, len(replay_buffer): 15821\nEpisode 176: 103.0, len(replay_buffer): 15925\nEpisode 177: 126.0, len(replay_buffer): 16033\nEpisode 178: 137.0, len(replay_buffer): 16173\nEpisode 179: 146.0, len(replay_buffer): 16323\nEpisode 180: 145.0, len(replay_buffer): 16474\nEpisode 181: 147.0, len(replay_buffer): 16621\nEpisode 182: 137.0, len(replay_buffer): 16786\nEpisode 183: 161.0, len(replay_buffer): 16942\nEpisode 184: 148.0, len(replay_buffer): 17086\nEpisode 185: 145.0, len(replay_buffer): 17240\nEpisode 186: 132.0, len(replay_buffer): 17379\nEpisode 187: 121.0, len(replay_buffer): 17503\nEpisode 188: 126.0, len(replay_buffer): 17635\nEpisode 189: 131.0, len(replay_buffer): 17765\nEpisode 190: 122.0, len(replay_buffer): 17895\nEpisode 191: 107.0, len(replay_buffer): 18007\nEpisode 192: 108.0, len(replay_buffer): 18122\nEpisode 193: 125.0, len(replay_buffer): 18232\nEpisode 194: 118.0, len(replay_buffer): 18357\nEpisode 195: 120.0, len(replay_buffer): 18479\nEpisode 196: 125.0, len(replay_buffer): 18591\nEpisode 197: 125.0, len(replay_buffer): 18711\nEpisode 198: 130.0, len(replay_buffer): 18836\nEpisode 199: 138.0, len(replay_buffer): 18952\nEpisode 200: 128.0, len(replay_buffer): 19072\nEpisode 201: 123.0, len(replay_buffer): 19199\nEpisode 202: 131.0, len(replay_buffer): 19326\nEpisode 203: 141.0, len(replay_buffer): 19456\nEpisode 204: 148.0, len(replay_buffer): 19629\nEpisode 205: 151.0, len(replay_buffer): 19778\nEpisode 206: 153.0, len(replay_buffer): 19927\nEpisode 207: 149.0, len(replay_buffer): 20096\nEpisode 208: 136.0, len(replay_buffer): 20248\nEpisode 209: 154.0, len(replay_buffer): 20406\nEpisode 210: 182.0, len(replay_buffer): 20589\nEpisode 211: 271.0, len(replay_buffer): 20799\nEpisode 212: 500.0, len(replay_buffer): 21108\nEpisode 213: 342.0, len(replay_buffer): 21608\nEpisode 214: 247.0, len(replay_buffer): 21932\nEpisode 215: 238.0, len(replay_buffer): 22134\nEpisode 216: 205.0, len(replay_buffer): 22325\nEpisode 217: 200.0, len(replay_buffer): 22529\nEpisode 218: 471.0, len(replay_buffer): 22908\nEpisode 219: 500.0, len(replay_buffer): 23408\nEpisode 220: 500.0, len(replay_buffer): 23908\nEpisode 221: 500.0, len(replay_buffer): 24408\nEpisode 222: 500.0, len(replay_buffer): 24908\nEpisode 223: 500.0, len(replay_buffer): 25408\nEpisode 224: 500.0, len(replay_buffer): 25908\nEpisode 225: 500.0, len(replay_buffer): 26408\nEpisode 226: 500.0, len(replay_buffer): 26908\nEpisode 227: 500.0, len(replay_buffer): 27408\nEpisode 228: 76.0, len(replay_buffer): 27908\nEpisode 229: 500.0, len(replay_buffer): 27978\nEpisode 230: 500.0, len(replay_buffer): 28478\nEpisode 231: 500.0, len(replay_buffer): 28978\nEpisode 232: 500.0, len(replay_buffer): 29478\nEpisode 233: 500.0, len(replay_buffer): 29978\nEpisode 234: 115.0, len(replay_buffer): 30478\nEpisode 235: 103.0, len(replay_buffer): 30591\nEpisode 236: 101.0, len(replay_buffer): 30701\nEpisode 237: 107.0, len(replay_buffer): 30809\nEpisode 238: 100.0, len(replay_buffer): 30917\nEpisode 239: 108.0, len(replay_buffer): 31022\nEpisode 240: 105.0, len(replay_buffer): 31138\nEpisode 241: 131.0, len(replay_buffer): 31255\nEpisode 242: 147.0, len(replay_buffer): 31407\nEpisode 243: 274.0, len(replay_buffer): 31622\nEpisode 244: 303.0, len(replay_buffer): 31902\nEpisode 245: 245.0, len(replay_buffer): 32112\nEpisode 246: 244.0, len(replay_buffer): 32380\nEpisode 247: 249.0, len(replay_buffer): 32656\nEpisode 248: 423.0, len(replay_buffer): 32999\nEpisode 249: 249.0, len(replay_buffer): 33373\nEpisode 250: 362.0, len(replay_buffer): 33714\nEpisode 251: 238.0, len(replay_buffer): 33955\nEpisode 252: 274.0, len(replay_buffer): 34247\nEpisode 253: 332.0, len(replay_buffer): 34497\nEpisode 254: 484.0, len(replay_buffer): 34997\nEpisode 255: 500.0, len(replay_buffer): 35497\nEpisode 256: 500.0, len(replay_buffer): 35997\nEpisode 257: 314.0, len(replay_buffer): 36497\nEpisode 258: 389.0, len(replay_buffer): 36782\nEpisode 259: 467.0, len(replay_buffer): 37207\nEpisode 260: 360.0, len(replay_buffer): 37707\nEpisode 261: 425.0, len(replay_buffer): 38135\nEpisode 262: 500.0, len(replay_buffer): 38607\nEpisode 263: 500.0, len(replay_buffer): 39107\nEpisode 264: 500.0, len(replay_buffer): 39557\nEpisode 265: 429.0, len(replay_buffer): 39980\nEpisode 266: 406.0, len(replay_buffer): 40324\nEpisode 267: 365.0, len(replay_buffer): 40718\nEpisode 268: 317.0, len(replay_buffer): 41036\nEpisode 269: 482.0, len(replay_buffer): 41422\nEpisode 270: 500.0, len(replay_buffer): 41922\nEpisode 271: 500.0, len(replay_buffer): 42422\nEpisode 272: 500.0, len(replay_buffer): 42922\nEpisode 273: 435.0, len(replay_buffer): 43422\nEpisode 274: 440.0, len(replay_buffer): 43833\nEpisode 275: 309.0, len(replay_buffer): 44174\nEpisode 276: 297.0, len(replay_buffer): 44454\nEpisode 277: 366.0, len(replay_buffer): 44783\nEpisode 278: 305.0, len(replay_buffer): 45115\nEpisode 279: 248.0, len(replay_buffer): 45393\nEpisode 280: 228.0, len(replay_buffer): 45591\nEpisode 281: 244.0, len(replay_buffer): 45813\nEpisode 282: 248.0, len(replay_buffer): 46038\nEpisode 283: 269.0, len(replay_buffer): 46308\nEpisode 284: 263.0, len(replay_buffer): 46572\nEpisode 285: 280.0, len(replay_buffer): 46866\nEpisode 286: 260.0, len(replay_buffer): 47121\nEpisode 287: 263.0, len(replay_buffer): 47390\nEpisode 288: 250.0, len(replay_buffer): 47637\nEpisode 289: 294.0, len(replay_buffer): 47899\nEpisode 290: 271.0, len(replay_buffer): 48153\nEpisode 291: 500.0, len(replay_buffer): 48506\nEpisode 292: 500.0, len(replay_buffer): 49006\nEpisode 293: 500.0, len(replay_buffer): 49506\nEpisode 294: 500.0, len(replay_buffer): 50006\nEpisode 295: 500.0, len(replay_buffer): 50506\nEpisode 296: 500.0, len(replay_buffer): 51006\nEpisode 297: 500.0, len(replay_buffer): 51506\nEpisode 298: 500.0, len(replay_buffer): 52006\nEpisode 299: 500.0, len(replay_buffer): 52506\nEpisode 300: 500.0, len(replay_buffer): 53006\nEpisode 301: 500.0, len(replay_buffer): 53506\nEpisode 302: 500.0, len(replay_buffer): 54006\nEpisode 303: 500.0, len(replay_buffer): 54506\nEpisode 304: 500.0, len(replay_buffer): 55006\nEpisode 305: 388.0, len(replay_buffer): 55506\nEpisode 306: 500.0, len(replay_buffer): 56006\nEpisode 307: 384.0, len(replay_buffer): 56404\nEpisode 308: 481.0, len(replay_buffer): 56904\nEpisode 309: 500.0, len(replay_buffer): 57404\nEpisode 310: 500.0, len(replay_buffer): 57904\nEpisode 311: 500.0, len(replay_buffer): 58404\nEpisode 312: 500.0, len(replay_buffer): 58904\nEpisode 313: 500.0, len(replay_buffer): 59404\nEpisode 314: 500.0, len(replay_buffer): 59904\nEpisode 315: 500.0, len(replay_buffer): 60404\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1711377759757
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.plot(reward_list)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1711377759860
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.plot(p_list)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1711377759872
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}