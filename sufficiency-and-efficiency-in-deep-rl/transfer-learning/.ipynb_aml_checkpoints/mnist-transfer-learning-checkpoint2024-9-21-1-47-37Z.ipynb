{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Optimal transfer learning\n\nHere, we create two models and traverse a path on the statistical manifold between them.\nGiven my theory's optimal infinitesimal traversals of continuously changing distributions, \nthe theory should port cleanly from reinforcement learning to transfer learning. \nOf course, only each infinitesimal step along the path with be optimal, \nleaving room for data scientists' intuition to usefully bias retention. \nFor example, if early game information only becomes useful late in play, \nthen a retention spike will need to be added. \nRegardless, infinitesimal optimality is an important step toward a deep and coherent transfer learning theory. \n\nThe experiment will involve fitting a dense net to the MNIST dataset, classifying digits as usual. \nHowever, after the initial fit, we will embed the dense net into a much larger statistical manifold \nby making it part of a mixture model with a different, far more sparse model. \nWe'll then slowly and continuously traverse the mixture probability parameter $q$ from 0 to 1, \noptimally retaining information at each step. \nWe'll measure test set accuracy at each step and compare against different traversal strategies. \nA good result should find accuracy is optimally and usefully sustained despite transferring to a sparse model.\n\nIf successful, this experiment will illustrate the effectiveness of my general theory of transfer learning. \nThus, it'd be possible to optimally translate information optimally between arbitrary models, \nprovided they can handle the same dataset or a continuously transforming dataset between the models.\nThis is a powerful result, so I'm eager to run my experiment, but won't get my hopes up too much. \nThe truth is best found by the pursuit of ambitious targets and sensitively understanding negative results."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Dense net code initially authored by Google's search engine GenAI on 20 Oct 2024. \n",
        "## I've applied minor modifications for generality, but the code worked great on first draft. \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define the DenseNet model\n",
        "class DenseNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DenseNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Linear(784, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.features(x)\n",
        "        return x\n",
        "\n",
        "# Load MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='/tmp/data', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = datasets.MNIST(root='/tmp/data', train=False, transform=transforms.ToTensor())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Initialize the model, loss function and optimizer\n",
        "model = DenseNet()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        output = model(data)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nFailed to download (trying next):\n<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1123)>\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /tmp/data/MNIST/raw/train-images-idx3-ubyte.gz\nExtracting /tmp/data/MNIST/raw/train-images-idx3-ubyte.gz to /tmp/data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nFailed to download (trying next):\n<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1123)>\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /tmp/data/MNIST/raw/train-labels-idx1-ubyte.gz\nExtracting /tmp/data/MNIST/raw/train-labels-idx1-ubyte.gz to /tmp/data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nFailed to download (trying next):\n<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1123)>\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /tmp/data/MNIST/raw/t10k-images-idx3-ubyte.gz\nExtracting /tmp/data/MNIST/raw/t10k-images-idx3-ubyte.gz to /tmp/data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nFailed to download (trying next):\n<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1123)>\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /tmp/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\nExtracting /tmp/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/data/MNIST/raw\n\nEpoch [1/5], Step [1/938], Loss: 2.2956\nEpoch [1/5], Step [101/938], Loss: 0.5349\nEpoch [1/5], Step [201/938], Loss: 0.1705\nEpoch [1/5], Step [301/938], Loss: 0.2335\nEpoch [1/5], Step [401/938], Loss: 0.1434\nEpoch [1/5], Step [501/938], Loss: 0.1999\nEpoch [1/5], Step [601/938], Loss: 0.2834\nEpoch [1/5], Step [701/938], Loss: 0.1555\nEpoch [1/5], Step [801/938], Loss: 0.1351\nEpoch [1/5], Step [901/938], Loss: 0.1124\nEpoch [2/5], Step [1/938], Loss: 0.1061\nEpoch [2/5], Step [101/938], Loss: 0.0604\nEpoch [2/5], Step [201/938], Loss: 0.0220\nEpoch [2/5], Step [301/938], Loss: 0.0590\nEpoch [2/5], Step [401/938], Loss: 0.1055\nEpoch [2/5], Step [501/938], Loss: 0.1804\nEpoch [2/5], Step [601/938], Loss: 0.0180\nEpoch [2/5], Step [701/938], Loss: 0.0287\nEpoch [2/5], Step [801/938], Loss: 0.0764\nEpoch [2/5], Step [901/938], Loss: 0.0200\nEpoch [3/5], Step [1/938], Loss: 0.1220\nEpoch [3/5], Step [101/938], Loss: 0.0380\nEpoch [3/5], Step [201/938], Loss: 0.0268\nEpoch [3/5], Step [301/938], Loss: 0.0086\nEpoch [3/5], Step [401/938], Loss: 0.1116\nEpoch [3/5], Step [501/938], Loss: 0.2541\nEpoch [3/5], Step [601/938], Loss: 0.0187\nEpoch [3/5], Step [701/938], Loss: 0.0536\nEpoch [3/5], Step [801/938], Loss: 0.0867\nEpoch [3/5], Step [901/938], Loss: 0.0600\nEpoch [4/5], Step [1/938], Loss: 0.0226\nEpoch [4/5], Step [101/938], Loss: 0.0369\nEpoch [4/5], Step [201/938], Loss: 0.2107\nEpoch [4/5], Step [301/938], Loss: 0.0355\nEpoch [4/5], Step [401/938], Loss: 0.1003\nEpoch [4/5], Step [501/938], Loss: 0.0322\nEpoch [4/5], Step [601/938], Loss: 0.1245\nEpoch [4/5], Step [701/938], Loss: 0.0995\nEpoch [4/5], Step [801/938], Loss: 0.1301\nEpoch [4/5], Step [901/938], Loss: 0.0149\nEpoch [5/5], Step [1/938], Loss: 0.0875\nEpoch [5/5], Step [101/938], Loss: 0.0064\nEpoch [5/5], Step [201/938], Loss: 0.0106\nEpoch [5/5], Step [301/938], Loss: 0.0172\nEpoch [5/5], Step [401/938], Loss: 0.0102\nEpoch [5/5], Step [501/938], Loss: 0.0335\nEpoch [5/5], Step [601/938], Loss: 0.0195\nEpoch [5/5], Step [701/938], Loss: 0.0450\nEpoch [5/5], Step [801/938], Loss: 0.0115\nEpoch [5/5], Step [901/938], Loss: 0.0088\nAccuracy of the network on the 10000 test images: 97.93 %\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "100%|██████████| 9912422/9912422 [00:00<00:00, 129428628.67it/s]\n100%|██████████| 28881/28881 [00:00<00:00, 14778052.19it/s]\n100%|██████████| 1648877/1648877 [00:00<00:00, 27626104.59it/s]\n100%|██████████| 4542/4542 [00:00<00:00, 2831108.45it/s]\n/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/autograd/graph.py:769: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1729472109423
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Transfer learn to sparse nets \n",
        "from ssr_agent import SSRAgent \n",
        "from replay_buffer import ReplayBuffer \n",
        "\n",
        "class SparseActivation(nn.Module): \n",
        "    '''Softmax that zeros-out small values. \n",
        "    Transfer-learning from a prefit model recommended. \n",
        "    '''\n",
        "    def __init__(self, K=1, next_linearity=None):\n",
        "        'K: determines maximum number of non-zero dimensions.'\n",
        "        super(SparseActivation, self).__init__()\n",
        "        self.K = K \n",
        "        self.softmax = nn.Softmax(dim=1) \n",
        "        self.next_linearity = next_linearity \n",
        "        pass \n",
        "    def forward(self, x, next_linearity=None): \n",
        "        '''inputs:\n",
        "        - x: [n, ...]-shaped tensor \n",
        "        - next_linearity: instance of nn.Linear is applied efficiently to output of this activation. Autodetects next_nonlinearity if it was provided during init. \n",
        "        ouputs: \n",
        "        - idx: indicies of softmax(x) that are greater than 1/K. \n",
        "        - softmax(x)[idx]: values of softmax(x) that are greater than 1/K. Returns None if idx is empty. \n",
        "        '''\n",
        "        ## calculate \n",
        "        x = self.softmax(x) \n",
        "        idx = (x > 1/self.K).nonzero() \n",
        "        x_idx = None \n",
        "        if int(idx.shape[0]) > 0: \n",
        "            x_idx = x[idx] \n",
        "        if next_linearity is None and self.next_linearity is None: \n",
        "            return idx, x_idx \n",
        "        if next_linearity is None and self.next_linearity is not None: \n",
        "            next_linearity = self.next_linearity \n",
        "            pass \n",
        "        ## sparsely apply next_linearity \n",
        "        if int(idx.shape[0]) > 0: \n",
        "            y = x_idx.matmul(next_linearity.weights[idx,:]) \n",
        "        else: \n",
        "            y = 0. \n",
        "            pass \n",
        "        y += next_linearity.bias \n",
        "        return idx, x_idx, y \n",
        "    pass \n",
        "\n",
        "class SparseNet(nn.Module): \n",
        "    def __init__(self, K=10): \n",
        "        super(SparseNet, self).__init__() \n",
        "        self.K = K \n",
        "        self.linear1 = nn.Linear(784, 512) \n",
        "        self.linear2 = nn.Linear(512, 256) \n",
        "        self.linear3 = nn.Linear(256, 128) \n",
        "        self.linear4 = nn.Linear(128, 10) \n",
        "        self.activation1 = SparseActivation(K=self.K, next_linearity=self.linear2) \n",
        "        self.activation2 = SparseActivation(K=self.K, next_linearity=self.linear3) \n",
        "        self.activation3 = SparseActivation(K=self.K, next_linearity=self.linear4) \n",
        "        self.activation4 = nn.Softmax(dim=1) \n",
        "        pass \n",
        "    def forward(self, x): \n",
        "        x = self.activation1(self.linear1(x)) \n",
        "        x = self.activation2(self.linear2(x)) \n",
        "        x = self.activation3(self.linear3(x)) \n",
        "        x = self.activation4(self.linear4(x)) \n",
        "        return x\n",
        "\n",
        "class MixtureModel(nn.Module): \n",
        "    'join two models with a Bernoulli random variable'\n",
        "    def __init__(self, model1, model2, p): \n",
        "        'mix models 1 and 2, selecting model 2 with probability p'\n",
        "        super(MixtureModel, self).__init__() \n",
        "        self.model1 = model1 \n",
        "        self.model2 = model2 \n",
        "        self.p = p \n",
        "        pass \n",
        "    def forward(self, x): \n",
        "        n = x.shape[0] \n",
        "        b = torch.rand(n) >= self.p \n",
        "        b_sum = b.sum() \n",
        "        y = torch.zeros([n,1]) \n",
        "        if b_sum < n: \n",
        "            y[b.logical_not()] = self.model1(x[b.logical_not(),:]) \n",
        "        if b_sum > 0: \n",
        "            y[b] = self.model2(x[b,:]) \n",
        "            pass \n",
        "        return y \n",
        "    pass \n",
        "\n",
        "class Model(SSRAgent): \n",
        "    'Run transfer learning experiments with this model' \n",
        "    def __init__(self, p, K=10, ssr_rank=5): \n",
        "        'p: probability of the mixture model selecting the sparse model'\n",
        "        super(MixtureModel, self).__init__(replay_buffer=ReplayBuffer(), ssr_rank=ssr_rank) \n",
        "        self.mixture_model( \n",
        "            model1 = DenseNet(), \n",
        "            model2 = SparseNet(K=K), \n",
        "            p=p \n",
        "        )\n",
        "        self.criterion = nn.CrossEntropyLoss() \n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=0.001) \n",
        "        pass \n",
        "    def loss(self, data): \n",
        "        y_hat = self.mixture_model(data.x) \n",
        "        return self.criterion(y_hat, data.y) \n",
        "    pass "
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (211910728.py, line 64)",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn [5], line 64\u001b[0;36m\u001b[0m\n\u001b[0;31m    def MixtureModel(nn.Module):\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1729472109486
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.tensor([]).dtype"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "torch.float32"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1729471502950
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}