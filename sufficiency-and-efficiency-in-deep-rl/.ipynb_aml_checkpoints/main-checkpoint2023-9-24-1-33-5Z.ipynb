{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sufficiency and Efficiency in Deep RL\n",
        "\n",
        "_W. Evan Durno, 2023_\n",
        "\n",
        "**TODO:** the whole document contains a mixture of multivariate and univariate representations of $\\theta$. Switch-over to multivariate.  \n",
        "\n",
        "As a reinforcement learning (RL) agent explores its environment, \n",
        "it learns and tries new things, and thus changes its sampling distribution. \n",
        "Thus, the RL paradigm poorly aligns with the foundational identical distribution assumptions \n",
        "underpinning the statistical theories allowing AI to function correctly. \n",
        "For example, it can be useful to discard old observations during fitting. \n",
        "Similarly, we ought to ask, how can we optimally fit our models while adding new samples from new distributions? \n",
        "\n",
        "In this work, we introduce the concept of a sufficient statistic regularizer (SSR), \n",
        "and prove the existence of an optimal regularizing value $\\lambda$ to assist in fitting \n",
        "our models while distributions change smoothly and locally. \n",
        "Further, by relaxing the sufficient statistic definition to accommodate a reasonable approximation, \n",
        "we prove the existence of asymptotically universal sufficient statistics applicable in and beyond RL, \n",
        "assuming reasonable regularity assumptions. \n",
        "The net result is an RL paradigm which \n",
        "(1) incorporates new, non-identically distributed data optimally, and \n",
        "(2) achieves miniaturization by storing data in an $O(p)$-sized space instead of $O(n)$. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction \n",
        "\n",
        "TODO: Describe the problem precisely \n",
        "\n",
        "TODO: Cover similar research "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SSRs \n",
        "\n",
        "**Sufficient statistics as regularizers** \n",
        "\n",
        "Data reduction is the purpose of sufficient statistics. \n",
        "This becomes particularly apparent when studying their behavior under maximum likelihood estimation. \n",
        "The Fisher-Neyman defintion of a sufficient statistic $T(x)$ is that there \n",
        "exists $g$ and $h$ for density $f$ such that $f(x; \\theta) = h(x) g(T(x) ; \\theta)$. \n",
        "Notice that under maximum likelihood estimation, the $h$ term becomes irrelevant, leaving only $T(x)$, \n",
        "thereby providing an opportunity to reduce dimensionality of all data stored. \n",
        "\n",
        "$ \\hat \\theta = \\arg \\max_\\theta f(x; \\theta) = \\arg \\max_\\theta \\log f(x; \\theta) $\n",
        "$ = \\arg \\max_\\theta \\log(h(x)) + \\log(g(x; T(x))) = \\arg \\max_\\theta \\log(g(x; T(x)))$ \n",
        "\n",
        "For example, dataset $x$ may require $O(n)$ space to store, but for $\\theta in \\mathbb{R}^p$, \n",
        "it's common that $T$ only require $O(p)$ storage space. \n",
        "So, for a model with fixed parameter dimension, \n",
        "it is reasonably possible to have theoretically infinite data storage in a finite space. \n",
        "Of course, the amount of information truly stored will be practically bounded. \n",
        "\n",
        "For the purposes of this work, it is convenient to view sufficient statistics as regularizers. \n",
        "In situations where new data is added to old, like RL, this is particularly relevant. \n",
        "For example, with old data $X_A$ packed into $T_A = T(X_A)$ supplanting $\\hat \\theta_A$, \n",
        "we may add data $X_B$ ultimately producing $T_B = T(X_B)$ sufficiently estimating $\\theta_B$. \n",
        "\n",
        "$\\hat \\theta_B = \\arg\\max_\\theta f_X(X; \\theta) = \\arg \\max_\\theta \\log f_X(X_A;\\theta) + \\log f_X(X_B;\\theta) $\n",
        "$ = \\arg\\max_\\theta \\log f_X(X_A;\\theta) + \\log g(T(X_B; \\theta))$\n",
        "\n",
        "Here, $X$ is the vector containing both $X_A$ and $X_B$, concatenated. \n",
        "\n",
        "If we then insert a scalar multiple $\\lambda$, we recover the familiar regularizer form. \n",
        "\n",
        "$\\hat \\theta_B = \\arg\\max_\\theta \\log f_X(X_A;\\theta) + \\lambda \\log g(T(X_B; \\theta)) $\n",
        "\n",
        "Let this form of regularizer be the SSR. \n",
        "Like other regularizers, $\\lambda \\log g(T(X_B); \\theta)$ causes $\\theta$ to \n",
        "stay near some point $\\theta_A$. \n",
        "For example and without loss of generality (WLOG), $\\lambda \\| \\theta \\|^2$ centers $\\theta$ on zero. \n",
        "\n",
        "**Universal SSRs via approximation** \n",
        "\n",
        "Data reduction via sufficient statistics is like compression. \n",
        "A breadth of theory has been developed for losses compression. \n",
        "Here, we'll highlight the benefits of lossy compression \n",
        "by relaxing our sufficient statistic definition to accommodate an approximation. \n",
        "Define $X_n \\approx_{a.s.} Y$ to mean $\\lim_{n \\to \\infty} X_n = Y \\; a.s.$, \n",
        "and $X_n \\approx_{\\mathbb{P}} Y$ to mean $\\lim_{n \\to \\infty} X_n = Y$ in $\\mathbb{P}$.\n",
        "Then, instead of defining $T$ sufficient if $f_X(x; \\theta) = h(x) g(T(x); \\theta)$, \n",
        "we'll accept $f_X(x; \\theta) \\approx_{\\mathbb{P}} h(x) g(T(x); \\theta)$ for large sample size $n$. \n",
        "\n",
        "With this relaxation, we are free to create approximate universal sufficient statistics \n",
        "only requiring sufficient regularity assumptions apply that the central limit theorem (CLT) \n",
        "apply to make the log likelihood normally distributed. \n",
        "\n",
        "First, recognize that the following Taylor expansion is accurate for $\\theta$ near $\\theta_A$. \n",
        "\n",
        "$n^{-1} \\log f_X(X;\\theta) \\approx n^{-1}\\log f_X(X; \\theta_A) $\n",
        "$ + (\\theta - \\theta_A) n^{-1}\\frac{\\partial}{\\partial \\theta} \\log f_X(X; \\theta_A)$\n",
        "$ + 2^{-1}(\\theta - \\theta_A)^2 n^{-1} \\frac{\\partial^2}{\\partial \\theta^2} \\log f_X(X; \\theta_A) $  \n",
        "\n",
        "$\\approx_{a.s.} \\mathbb{E} \\log f_X(X_1; \\theta_A) + (\\theta - \\theta_A) \\mathbb{E} \\frac{\\partial}{\\partial \\theta} \\log f_X(X_1; \\theta_A)$ \n",
        "$ + 2^{-1} (\\theta - \\theta_A)^2 \\mathbb{E} \\frac{\\partial^2}{\\partial \\theta^2} \\log f_X(X_1;\\theta_A) $ (apply the strong law of large numbers (SLLN))\n",
        "\n",
        "$ = \\mathbb{E} \\log f_X(X_1; \\theta_A) + 0 - 2^{-1} (\\theta - \\theta_A)^2 \\mathcal{I}_{\\theta_A} $\n",
        "\n",
        "Here, we utilize further approximations:\n",
        "- $\\hat{\\mathcal{I}}(X_A) = \\hat{\\mathcal{I}} = n^{-1}\\sum_{i=1}^n G_i G_i^T \\approx_{a.s.} \\mathcal{I}$ \n",
        "where $G_i \\nabla_\\theta \\log f_X(X_i; \\theta_A) $ \n",
        "- $\\hat \\theta_A(X_A) = \\hat \\theta_A \\arg \\max_\\theta f_X(X_A;\\theta) \\approx_{\\mathbb{P}} \\theta_A$\n",
        "\n",
        "Then we realize the following. \n",
        "\n",
        "$\\mathbb{E} \\log f_X(X_1; \\theta_A) - 2^{-1} (\\theta - \\theta_A)^2 \\mathcal{I}_{\\theta_A}$\n",
        "\n",
        "$\\approx_{\\mathbb{P}} \\mathbb{E} \\log f_X(X_1; \\theta_A) - n_A 2^{-1} (\\theta - \\hat \\theta_A(X_A))^2 \\hat{\\mathcal{I}}(X_A)$\n",
        "\n",
        "Here, $n_A$ is the sample size of $X_A$. \n",
        "\n",
        "This approximation is not-yet useful, still depending on $\\theta_A$. \n",
        "So, we must apply it in maximum likelihood estimation to drop the $\\mathbb{E} \\log f_X(X_1; \\theta_A)$ term. \n",
        "Here, we add new data $X_B$ to the sample, while approximately retaining all information of $X_A$ \n",
        "in $T(X_A) = \\left(\\hat \\theta_A(X_A), \\hat{\\mathcal{I}}(X_A) \\right)$.\n",
        "\n",
        "$\\hat \\theta_B = \\arg\\max_\\theta f_X(X; \\theta) = \\arg\\max_\\theta \\log f_X(X_B; \\theta) + \\log f_X(X_A;\\theta) $\n",
        "\n",
        "$ \\approx_{\\mathbb{P}} \\arg\\max_\\theta \\log f_X(X_B; \\theta) $\n",
        "$+ \\mathbb{E} \\log f_X(X_1; \\theta_A) - n_A 2^{-1} (\\theta - \\hat \\theta_A(X_A))^2 \\hat{\\mathcal{I}}(X_A) $\n",
        "\n",
        "$ = \\arg\\max_\\theta \\log f_X(X_B; \\theta) - n_A 2^{-1} (\\theta - \\hat \\theta_A(X_A))^2 \\hat{\\mathcal{I}}(X_A) $\n",
        "\n",
        "We thus identify the universal SSR $2^{-1} (\\theta - \\hat \\theta_A(X_A))^2 \\hat{\\mathcal{I}}(X_A)$ \n",
        "with its natural regularization parameter value $\\lambda = n_A$. \n",
        "\n",
        "**Miniaturization** \n",
        "\n",
        "Storing $\\hat \\theta(X_A)$ takes $O(p)$ space and $\\hat{\\mathcal{I}}(X_A)$ takes $O(p^2)$. \n",
        "So, while technically finite relative to $O(n)$, the sheer size of modern deep learning models \n",
        "makes any $O(p^2)$ storage requirement infeasible. \n",
        "So, this work will leverage a series of approximations that keep practically effective approximations \n",
        "to $\\mathcal{I}$ in $O(p)$. \n",
        "- The simplest approximation is only storing diagonal terms, and zeroing all others. \n",
        "- A better (but usually unnecessary) approach is leveraging a Krylov estimate to \n",
        "approximate the major eigenvectors of $\\mathcal{I}$. \n",
        "These are calculated via a modified version of the Lanczos algorithm. \n",
        "\n",
        "For experimental validation showing these approximations are effective, see Appendix B."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimal MLE efficiency under deforming distributions \n",
        "\n",
        "As an RL agent learns from its environment, it tries new things and produces new kinds of data. \n",
        "Accepting that RL doesn't sample from identical distributions invites us to find an optimal transition paradigm. \n",
        "Interpreting _optimality_ as _efficiency_, this means working with the Cramer-Rao lower bound (CRB). \n",
        "Particularly, since maximum likelihood estimates (MLEs) from different distributions appear biased to another, \n",
        "we'll work with the mean squared error (MSE) form of the CRB. \n",
        "The strategy is simple: calculate the MSE under biased MLE deformations, and minimize it with respect to $\\lambda$. \n",
        "\n",
        "To recover a optimal $\\lambda$, we use the following definitions. \n",
        "- $X_A$ is the vector (or matrix) of observations having density $f_X(x; \\theta_A)$. \n",
        "- $X_B$ similarly has density $f_X(x; \\theta_B)$. \n",
        "- $\\hat \\theta_A = \\arg\\max_\\theta f_X(X_A; \\theta)$ is a simple MLE. \n",
        "- $\\hat \\theta_B = \\arg\\max_\\theta f_X(X_B; \\theta)$\n",
        "- $\\hat \\theta_{AB} = \\arg\\max_\\theta \\log f_X(X_A ; \\theta) + \\log f_X(X_B; \\theta) $ \n",
        "$ = \\arg\\max_\\theta \\sum_{i \\in A} \\log f_X(X_i ; \\theta) + \\sum_{i \\in B} \\log f_X(X_i; \\theta) $ \n",
        "- $n_A = \\#A, n_B = \\#B, n = n_A + n_B$\n",
        "- $p = \\lim_{n \\to \\infty} n_B/n$. WLOG, we assume this series converges so we may have $p \\approx n_B/n$. \n",
        "\n",
        "With these definitions in-place, we can realize this $\\theta_{AB}$ form which can optimize MSE in $p$, determining optimal $\\lambda$. \n",
        "\n",
        "$\\hat \\theta_{AB} = \\arg\\max_\\theta \\log f_X(X_A ; \\theta) + \\log f_X(X_B; \\theta) $\n",
        "\n",
        "$ \\approx_{\\mathbb{P}} \\arg\\max_\\theta \\log f_X(X_B; \\theta) - n_A 2^{-1} (\\theta - \\hat \\theta_A)^T \\hat{\\mathcal{I}} (\\theta - \\hat \\theta_A)$\n",
        "\n",
        "$ \\approx n^{-1}\\arg\\max_\\theta \\log f_X(X_B; \\theta) - (1-p) 2^{-1} (\\theta - \\hat \\theta_A)^T \\hat{\\mathcal{I}} (\\theta - \\hat \\theta_A)$\n",
        "\n",
        "**CRB for mixed data** \n",
        "\n",
        "The CRB was designed with independent and identically distributed (iid) observations in-mind. \n",
        "Given our core observation that RL deforms distributions during sampling, \n",
        "we'll apply some regularity assumptions on our distributions to keep the CRB relevant. \n",
        "Assume that: \n",
        "- $f_X(x; \\theta)$ is smooth in $\\theta$ near $\\theta_B$. \n",
        "- $\\theta_A$ is sufficiently near to $\\theta_B$ that $\\log f_X(x; \\theta_A) \\approx \\log f_X(x; \\theta_B) + (\\theta_A - \\theta_B)^T \\nabla_\\theta \\log f_X(x; \\theta_B) $\n",
        "$+ 2^{-1} (\\theta_A - \\theta_B)^T \\left( \\nabla^2_\\theta \\log f_X(x; \\theta_B) \\right) (\\theta_A - \\theta_B) $ holds.\n",
        "\n",
        "This implies both $\\theta_A$ and $\\theta_B$ have approximately the same Hessians, $-\\mathcal{I}$. \n",
        "Leveraging equivalent Hessians, the CRB can be applied to minimize the MSE. \n",
        "\n",
        "For bias term $b = \\mathbb{E} \\hat \\theta_{AB} - \\theta_B$ and $b' = \\nabla_{\\theta_B}b$, the CRB clearly states this effect of a biased estimate.\n",
        "\n",
        "$\\mathbb{E}(\\hat \\theta_{AB} - \\theta_B) \\geq (1 + b')^2 \\mathcal{I}^{-1} + b^2$\n",
        "\n",
        "Recognizing $\\mathbb{E}\\hat \\theta_{AB} \\approx_{\\mathbb{P}} = p \\theta_B + (1-p) \\theta_A$, we recover this inequality. \n",
        "\n",
        "$\\mathbb{E}(\\hat \\theta_{AB} - \\theta_B) \\geq (1 - 1 + p)^2/(p\\mathcal{I} + (1-p) \\mathcal{I}) + (1-p)^2(\\theta_A - \\theta_B) $\n",
        "$ = p^2 \\mathcal{I}^{-1} + (1-p)^2 (\\theta_A - \\theta_B)$\n",
        "\n",
        "Differentiating by $p$ and setting to zero, we recover an optimal approximate value for $\\lambda$. \n",
        "\n",
        "$\\frac{n_A}{n_B} \\approx \\frac{p}{p-1} = (\\theta_A - \\theta_B)^2 \\mathcal{I} $\n",
        "\n",
        "Using the natural value for $\\lambda = n_A$, we recover $\\hat \\lambda = n_B (\\theta_A - \\theta_B)^2 \\mathcal{I} $.\n",
        "\n",
        "Unfortunately, $\\hat \\lambda$ is used to estimate $\\theta_B$, so we do not yet know $(\\theta_A - \\theta_B)^2$. \n",
        "However, in RL, $(\\theta_A - \\theta_B)^2$ is observed recursively, so may be projected via moving average. \n",
        "So, $\\hat \\lambda$ is practically estimatable. \n",
        "\n",
        "Under this paradigm, $n_A$ is still useful when arbitrarily large. \n",
        "However, setting it to $n_A = \\hat \\lambda$ virtually up-samples $X_A$ or $X_B$. \n",
        "\n",
        "**$\\theta_t$ as diffusion $W_t$**\n",
        "\n",
        "In applying the above theory, estimating $(\\theta_A - \\theta_B)^2$ via moving average invites inevitably viewing \n",
        "the path between $\\theta_A$ and $\\theta_B$ as a diffusion. \n",
        "Here, we'll formalize this intuition. \n",
        "For all $s \\in [0, t]$, let $\\theta_s = \\sigma W_s $, a standard Brownian motion. \n",
        "WLOG, assume $\\theta_0 = W_0 = 0$. \n",
        "\n",
        "We assume that large samples may be taken at any individual $\\theta_s$, \n",
        "enabling $\\hat \\theta_s \\approx_{\\mathbb{P}} \\theta_s$ for each $s$, \n",
        "via MLE consistency. \n",
        "Notice that we do not sample continuously and we are indeed discretizing our sampling points. \n",
        "So, for each $\\theta_s$, we'll have observations $X_s$ with sample size $n$. \n",
        "For simplicity's sake, continuous sampling, instead of batches at $\\theta_s$ points, will not be considered here.\n",
        "\n",
        "Let our estimate be $\\hat \\theta = p \\hat \\theta_t + (p-1) (m-1)^{-1} \\sum_{j=1}^{m-1} \\hat \\theta_{tj/m}$ and $p \\in (0, 1)$. \n",
        "So, we explicitly up-sample $\\theta_t$. \n",
        "In application, it is essential to keep $m$ somewhat small \n",
        "because computational resources must be spent keeping $n$ for all of the $m$ sample batches. \n",
        "Sufficiently large $n$ allows $\\hat \\theta_s \\approx_{\\mathbb{P}} \\theta_s$, \n",
        "which our following derivation relies on. \n",
        "\n",
        "Fascinatingly, if $m$ could meaningfully be allowed to become large while maintaining $\\hat \\theta_s \\approx_{\\mathbb{P}} \\theta_s$, \n",
        "our estimate approaches an Ito integral $\\hat \\theta \\approx p \\hat \\theta_t + (1-p) \\int_0^t W_s ds$. \n",
        "This is an interesting pure exploration we'll leave out-of-scope for this work. \n",
        "\n",
        "To study efficiency in this context, we must now reconstruct the CRB from its Cauchy-Schwarz (CS) foundations.\n",
        "For notational brevity, \n",
        "- let $T(X_j) = \\hat \\theta_{tj/m}$, and \n",
        "- $V_j = \\frac{\\partial}{\\partial \\theta_{tj/m}} \\log f_X(X_j; \\theta_{tj/m})$.  \n",
        "\n",
        "Also, assume all $\\theta_{tj/m}$ are near $\\theta_t$, and that $\\log f_X(x; \\theta)$ is smooth, \n",
        "so that each batch has approximately the same Hessian, so $\\mathcal{I} = \\mathcal{I}_{tj/m}$ for each $j$. \n",
        "These definitions and assumptions give us the following CS components. \n",
        "\n",
        "$\\text{Var}V = p \\mathcal{I} + (1-p) (m-1)^{-1} \\sum_{j=1}^{m-1} \\mathcal{I} = \\mathcal{I} $ by independence of samples $X_j$. \n",
        "\n",
        "$\\forall j \\in \\{0, 1, 2, \\ldots, m\\}, \\mathbb{E}V_j = 0$ \n",
        "$ \\Rightarrow \\forall j, \\text{Cov}(T(X_j), V_j) = \\mathbb{E}T(X_j) V_j - 0$ \n",
        "$ = \\mathbb{E} T(X_j) \\frac{\\partial}{\\partial \\theta_j} \\log f_X(X_j; \\theta_{tj/m}) $\n",
        "$ = \\frac{\\partial}{\\partial \\theta_j} \\mathbb{E} T(X_j) $ \n",
        "$ = \\frac{\\partial}{\\partial \\theta_j} \\psi_j $. \n",
        "Usual CRB depends on this integral-derivative exchange. \n",
        "\n",
        "In our case bias $b = \\mathbb{E}\\hat \\theta - \\theta \\approx_{\\mathbb{P}} \\mathbb{E}[p \\theta_t - (1-p)(m-1)^{-1}\\sum_{j=1}^{m-1}\\theta_{tj/m}] - \\theta_t $\n",
        "\n",
        "TODO: Experimental results illustrating findings "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Alternative perspective: submanifolds versus data \n",
        "\n",
        "TODO: Data effectively reduces parameter dimension \n",
        "\n",
        "TODO: Reducing MSE with submanifolds "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion \n",
        "\n",
        "TODO: SSRs provide a universal miniaturization paradigm \n",
        "\n",
        "TODO: RL lift should be expected by accounting for deforming distributions \n",
        "\n",
        "TODO: Submanifolds should be utilized wherever possible to reduce parameter dimension "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References \n",
        "\n",
        "TODO: add content here, as needed "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Appendix A: modified Lanczos algorithm \n",
        "\n",
        "TODO: my limited-memory Lanczos algorithm "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Appendix B: Effectiveness of $O(p)$ Hessian approximations \n",
        "\n",
        "TODO: Show results "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK V2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}