{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pygame"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting pygame\n  Downloading pygame-2.5.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n\u001b[K     |████████████████████████████████| 14.0 MB 5.5 MB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: pygame\nSuccessfully installed pygame-2.5.2\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Google's Generative AI search provided the initial draft of this code. \n",
        "## I have modified it since. \n",
        "## It provided these sources: \n",
        "## https://github.com/jk96491/RL_Algorithms?tab=readme-ov-file \n",
        "## https://github.com/abyaadrafid/Deep-Reinforcement-Learning \n",
        "## I couldn't find a draft of this code in the sources, \n",
        "## so Generative AI search may have generated the code. \n",
        "\n",
        "import random \n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim \n",
        "\n",
        "class Object(object):\n",
        "    pass\n",
        "\n",
        "class ReplayBuffer(): \n",
        "    def __init__(self, capacity=10000): \n",
        "        self.n = 0 \n",
        "        self.capacity = capacity \n",
        "        self.state_list = [] \n",
        "        self.action_list = [] \n",
        "        self.reward_list = [] \n",
        "        self.next_state_list = [] \n",
        "        self.done_list = [] \n",
        "        pass \n",
        "    def add(self, state, action, reward, next_state, done): \n",
        "        if self.n >= self.capacity: \n",
        "            ## discard earliest observation \n",
        "            self.state_list = self.state_list[1:] \n",
        "            self.action_list = self.action_list[1:] \n",
        "            self.reward_list = self.reward_list[1:] \n",
        "            self.next_state_list = self.next_state_list[1:] \n",
        "            self.done_list = self.done_list[1:] \n",
        "            self.n -= 1 \n",
        "        pass \n",
        "        ## cast to torch  \n",
        "        state = torch.tensor(state) \n",
        "        action = torch.tensor(action) \n",
        "        reward = torch.tensor(reward) \n",
        "        next_state = torch.tensor(next_state) \n",
        "        done = torch.tensor(done) \n",
        "        ## append to buffer \n",
        "        self.state_list.append(state) \n",
        "        self.action_list.append(action) \n",
        "        self.reward_list.append(reward) \n",
        "        self.next_state_list.append(next_state) \n",
        "        self.done_list.append(done) \n",
        "        self.n += 1 \n",
        "        pass \n",
        "    def sample(self, batch_size=32): \n",
        "        ## sample lists \n",
        "        out = Object() ## transitions \n",
        "        out.sate = [] \n",
        "        out.action = [] \n",
        "        out.reward = [] \n",
        "        out.next_state = [] \n",
        "        out.done = [] \n",
        "        for _ in range(batch_size): \n",
        "            idx = random.randint(0, self.n) \n",
        "            out.state.append(self.state_list[idx]) \n",
        "            out.action.append(self.action_list[idx]) \n",
        "            out.reward.append(self.reward_list[idx]) \n",
        "            out.next_state.append(self.next_state_list[idx]) \n",
        "            out.done.append(self.done_list[idx]) \n",
        "            pass \n",
        "        ## stack  \n",
        "        out.state = torch.stack(out.state) \n",
        "        out.action = torch.stack(out.action) \n",
        "        out.reward = torch.stack(out.reward) \n",
        "        out.next_state = torch.stack(out.next_state) \n",
        "        out.done = torch.stack(out.done) \n",
        "        return out \n",
        "    pass \n",
        "\n",
        "# Define the actor and critic networks\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(state_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        action = torch.tanh(self.fc3(x))\n",
        "\n",
        "        return action\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(state_dim + action_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = torch.cat([state, action], dim=1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        value = self.fc3(x)\n",
        "\n",
        "        return value\n",
        "\n",
        "# Create the actor and critic networks\n",
        "actor = Actor(state_dim=4, action_dim=1)\n",
        "critic = Critic(state_dim=4, action_dim=1)\n",
        "\n",
        "# Define the optimizers\n",
        "actor_optimizer = optim.Adam(actor.parameters(), lr=1e-3)\n",
        "critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)\n",
        "\n",
        "# Create the replay buffer\n",
        "replay_buffer = ReplayBuffer(capacity=100000) \n",
        "\n",
        "# Define the environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# Train the agent\n",
        "for episode in range(1000):\n",
        "    state = env.reset()\n",
        "\n",
        "    for t in range(1000):\n",
        "        action = actor(torch.tensor(state)) \n",
        "        \n",
        "        action_p = action.item() * .5 + .5 \n",
        "        action_int = ## must be 0 or 1 \n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        replay_buffer.add(state, action, reward, next_state, done)\n",
        "\n",
        "        if len(replay_buffer) > 1000:\n",
        "            # Sample a batch of transitions from the replay buffer\n",
        "            transitions = replay_buffer.sample(batch_size=32)\n",
        "\n",
        "            # Calculate the target Q-values\n",
        "            target_Q = critic(transitions.next_state, actor(transitions.next_state))\n",
        "            target_Q = target_Q.detach()\n",
        "\n",
        "            # Calculate the current Q-values\n",
        "            current_Q = critic(transitions.state, transitions.action)\n",
        "\n",
        "            # Calculate the critic loss\n",
        "            critic_loss = torch.mean((target_Q - current_Q).pow(2))\n",
        "\n",
        "            # Calculate the actor loss\n",
        "            actor_loss = -torch.mean(critic(transitions.state, actor(transitions.state)))\n",
        "\n",
        "            # Update the critic network\n",
        "            critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            critic_optimizer.step()\n",
        "\n",
        "            # Update the actor network\n",
        "            actor_optimizer.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            actor_optimizer.step()\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Evaluate the agent\n",
        "    episode_reward = 0\n",
        "\n",
        "    for t in range(1000):\n",
        "        state = env.reset()\n",
        "\n",
        "        action = actor(state)\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        episode_reward += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    print('Episode {}: {}'.format(episode, episode_reward))"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "tensor([-0.0676], grad_fn=<TanhBackward0>) (<class 'torch.Tensor'>) invalid",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [4], line 130\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m    128\u001b[0m     action \u001b[38;5;241m=\u001b[39m actor(torch\u001b[38;5;241m.\u001b[39mtensor(state)) \n\u001b[0;32m--> 130\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     replay_buffer\u001b[38;5;241m.\u001b[39madd(state, action, reward, next_state, done)\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(replay_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1000\u001b[39m:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;66;03m# Sample a batch of transitions from the replay buffer\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/gym/wrappers/time_limit.py:17\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m---> 17\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py:13\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 13\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observation, reward, done, info\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py:118\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m    117\u001b[0m     err_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(action)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) invalid\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mcontains(action), err_msg\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCall reset before using step method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m     x, x_dot, theta, theta_dot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\n",
            "\u001b[0;31mAssertionError\u001b[0m: tensor([-0.0676], grad_fn=<TanhBackward0>) (<class 'torch.Tensor'>) invalid"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1710771405024
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "np.random.binomial(1, .5) "
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "tensor([-0.0676], grad_fn=<TanhBackward0>)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1710776651887
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}