{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer learning compared to meta learning \n",
        "\n",
        "Meta learning prepares a model to interpolate generally over a submanifold in the parameter space, \n",
        "but the submanifold dimension is practically small relative the parameter space dimension, \n",
        "because each one must be engineered. \n",
        "Alternatively, transfer learning effectively contributes significant samples sizes to some dimensions \n",
        "and usually arrives with a little bias. \n",
        "Here, we show (HYPOTHESIZE) that meta learning is competitive near its submanifold, \n",
        "but sufficiently-abstracted transfer learning ultimately produces greater generality beyond the submanifold, \n",
        "at least when bias is sufficiently low. \n",
        "Ultimately, it's a trade-off between meta learning's effective submanifold, \n",
        "and the relevance of a data abstraction produced by transfer learning. \n",
        "If the abstraction is coherent and small, it should be more general. \n",
        "When done correctly, packing greater volumes of data into smaller dimensional spaces lends to greater generality."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn \n",
        "\n",
        "MNIST_DIM = 28 \n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "class BaseLayer(nn.Module): \n",
        "    def __init__(self,\n",
        "            abstraction_dimension=20):\n",
        "        self.abstraction_dimension = abstraction_dimension \n",
        "        self.fc1 = nn.Linear(MNIST_DIM*MNIST_DIM, 256) \n",
        "        self.relu1 = nn.LeakyReLU()\n",
        "        self.fc2 = nn.Linear(256, self.abstraction_dimension) \n",
        "        pass \n",
        "    def forward(self, \n",
        "            x): \n",
        "        x = self.fc1(x) \n",
        "        x = self.relu1(x) \n",
        "        x = self.fc2(x) \n",
        "        return x \n",
        "    pass \n",
        "\n",
        "class AutoEncoder(nn.Module): \n",
        "    def __init__(self,\n",
        "            abstraction_dimension=20): \n",
        "        self.abstraction_dimension = abstraction_dimension \n",
        "        self.base_layer = BaseLayer(abstraction_dimension=self.abstraction_dimension) \n",
        "        self.relu1 = nn.LeakyReLU()\n",
        "        self.fc1 = nn.Linear(self.abstraction_dimension, 256) \n",
        "        self.relu2 = nn.LeakyReLU() \n",
        "        self.fc2 = nn.Linear(256, MNIST_DIM*MNIST_DIM) \n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=LEARNING_RATE) \n",
        "        pass \n",
        "        def forward(self,\n",
        "                x):\n",
        "            x = self.base_layer(x) \n",
        "            x = self.relu1(x) \n",
        "            x = self.fc1(x) \n",
        "            x = self.relu2(x) \n",
        "            x = self.fc2(x) \n",
        "            x = torch.sigmoid(x) \n",
        "            return x \n",
        "    pass \n",
        "\n",
        "    class Classifier(nn.Module): \n",
        "        def __init__(self,\n",
        "                abstraction_dimension=20, \n",
        "                base_layer_transfer=None, \n",
        "                n_labels=10): \n",
        "            self.abstraction_dimension = abstraction_dimension \n",
        "            self.n_labels=10 \n",
        "            self.base_layer = BaseLayer(abstraction_dimension=self.abstraction_dimension) \n",
        "            self.relu1 = nn.LeakyReLU() \n",
        "            self.fc1 = nn.Linear(self.abstraction_dimension, self.n_labels) \n",
        "            if base_layer_transfer is not None: \n",
        "                ## TODO copy params \n",
        "                pass \n",
        "            pass \n",
        "        def forward(self, \n",
        "                x): \n",
        "            x = self.base_layer(x) \n",
        "            x = self.relu1(x) \n",
        "            x = self.fc1(x) \n",
        "            x = torch.softmax(x) \n",
        "            return x \n",
        "        pass \n",
        "\n",
        "## TODO experimental cases: \n",
        "## Meta learning: fit model to linear interpolations of {0,1,2,3,4,5,6,7,8} making a 9-dim sub-manifold. \n",
        "## Illustrate effectiveness on fake, new, within-submanifold digits like p*2 + (1-p)*5 but ineffectiveness with 9. \n",
        "## Transfer learning: show how optimal abstraction dim on {0,1,2,3,4,5,6,7,8} results in greater effectiveness with 9. "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK V2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}