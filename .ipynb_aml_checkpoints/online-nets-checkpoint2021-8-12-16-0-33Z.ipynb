{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# online nets\r\n",
        "\r\n",
        "Deep learning is powerful but computationally expensive, frequently requiring massive compute budgets. In persuit of cost-effective-yet-powerful AI, this work explores and evaluates a heuristic which should lend to more-efficient use of data through online learning.\r\n",
        "\r\n",
        "Goal: evaluate a deep learning alternative capable of true online learning. Solution requirements:\r\n",
        "\r\n",
        "1. catastrophic forgetting should be impossible;\r\n",
        "2. all data is integrated into sufficient statistics of fixed dimension;\r\n",
        "3. and our solution should have predictive power comparable to deep learning.\r\n",
        "\r\n",
        "## alternative model\r\n",
        "\r\n",
        "Least squares regression (LSR) meets solution requirements 1 and 2. To achieve requirement 3, we'll structure our model similarly to deep learning by having LSR models depend on other LSR models, effectively producing a Gaussian Bayes net with nonlinear activations.\r\n",
        "Unfortunately, Bayes nets' latent variables result in computationally intractible integrals, so we'll use a heuristic to \"observe\" all variables.\r\n",
        "\r\n",
        "Our LSR model can be written as $X_n\\beta_n = Y_n$, where\r\n",
        "- $X_n \\in \\mathbb{R}^{n \\times p}$ is a matrix of regressor columns and $x_i \\in \\mathbb{R}^{1 \\times p}$ observation rows.\r\n",
        "- $Y_n \\in \\mathbb{R}^{n \\times q}$ is a matrix of dependent variable columns and $y_i \\in \\mathbb{R}^{1 \\times q}$ observation rows.\r\n",
        "- $\\beta_n \\in \\mathbb{R}^{p \\times q}$ is our matrix of regression weights.\r\n",
        "\r\n",
        "First, we'll rephrase our problems as into time series problems, so that we may assume our input and output dimensions are equivalent or $p = q$. While this category of problems clearly covers reinforcement learning, it can also cover more general problems.\r\n",
        "For example, given the right network topology and interpretting $X_{n+k}$ as the $n^{th}$ sample's output, we cover feed-forward classification problems as well. Our network will be used as a recurrent net, so we'll use this additional constraint:\r\n",
        "- $X_{n+1} := \\sigma(Y_n)$ where $\\sigma: \\mathbb{R}^p \\to \\mathbb{R}^p$ is an invertible activation function.\r\n",
        "\r\n",
        "To accomodate both observed and latent variables, let\r\n",
        "- $O_n \\in \\mathbb{R}^{p - l}$ be our observed varaibles\r\n",
        "- $L_n \\in \\mathbb{R}^l$ be our latent variables\r\n",
        "- $X_n = [O_n \\; | \\; L_n]$ be a column-concatentation of both observed and latent variables.\r\n",
        "\r\n",
        "Given $(O_n, L_n, O_{n+1})$ observed but $L_{n+1}$ not observed, we now construct a heuristic to guess $L_{n+1}$. Rather simply, we'll run the LSR backwards: $\\hat{X}_n := Y_n \\beta_n^{-1}$. We'll the use the $\\hat{L}_n$ portion of $\\hat{X}_n$ as $L_{n+1}$.\r\n",
        "Then, we'll fit our model to $Y_n =  \\left[ \\sigma^{-1}(O_{n+1}) \\; \\big| \\; \\hat{L}_n \\right]$.\r\n",
        "\r\n",
        "## motivating the heuristic\r\n",
        "\r\n",
        "The advantage of this heuristic over backpropagation is that it supports mathematically guaranteed online learning. However, we must motivate using $\\hat{L}^n$ as $L_{n+1}$ in $Y_n$.\r\n",
        "The essential idea is that we produce an approximate backpropagation equivalent by allowing $O_{n+1}$ to inform what $L_n$ should have been to produce $O_{n+1}$.\r\n",
        "Of course, having $L_{n+1}$ updated requires another iteration to inform $O_{n+2}$, so solutions may require several iterations to converge.\r\n",
        "\r\n",
        "This heuristic is indeed approximate and lacks the mathematical rigour guaranteeing deep learning's success, but it is worth trying because deep learning's computational cost is approaching infeasibility.\r\n",
        "\r\n",
        "## numerical considarations\r\n",
        "\r\n",
        "We'll use the Sherman-Morrison formula (SMF) to derive $\\hat{\\beta}_n^{-1}$ in a computationally tractible way.\r\n",
        "\r\n",
        "$$(A + uv^T)^{-1} = A^{-1} - \\frac{A^{-1} av^T A^{-1}}{1 + v^tA^{-1}u}$$\r\n",
        "\r\n",
        "We will later define our problems in terms of recurrent nets and time series. Input and output dimensions thus equate, so let $p = q$.\r\n",
        "\r\n",
        "With regularization term $\\lambda >0$, the L2-regularized estimate of $\\beta$ is $\\left(\\sum_{i=1}^nx_i^tx_i + \\lambda \\right)^{-1}\\sum_{i=1}^n x_i^Ty_i$. However, we'll need an additional inverse, so we must add further regularization.\r\n",
        "Take our $\\beta$ estimate to be $\\hat{\\beta}_n = \\left(\\sum_{i=1}^nx_i^tx_i + \\lambda \\right)^{-1}\\left(\\sum_{i=1}^n x_i^Ty_i + \\lambda \\right)$. We'll derive our SMF-inverse updates with these definitions:\r\n",
        "- $A_n := \\sum_{i=1}^n x_i^Tx_i + \\lambda$\r\n",
        "- $A_0 := \\lambda I_{p \\times p}$\r\n",
        "- $B_n := \\sum_{i=1}^n x_i^T y_i + \\lambda$\r\n",
        "- $B_0 := \\lambda I_{p \\times p}$\r\n",
        "\r\n",
        "With these definitions, we have that $\\hat{\\beta}_n = A_n^{-1} B_n$ and also that $\\hat{\\beta}_{n+1} = \\left(A_n + x_i^Tx_i \\right)^{-1} \\left( B_n + x_i^Ty_i \\right)$. Applying SMF, we get these identities\r\n",
        "$$A_{n+1}^{-1} = A_n^{-1} - \\frac{A_n^{-1} x_i^Tx_i A_n^{-1}}{1+x_iA_n^{-1}x_i^T}, \\; \\; \\; B_{n+1}^{-1} = B_n^{-1} - \\frac{B_n^{-1} x_i^Ty_i B_n^{-1}}{1+y_iB_n^{-1}x_i^T}$$\r\n",
        "\r\n",
        "So, we have derived our computationally-tractible inverse updates for $A_n$ and $B_n$. Now, since $\\hat{\\beta}_n = A_n^{-1}B_n$ it is trivial to calculate $\\hat{\\beta}_n^{-1} = B_n^{-1} A_n$.\r\n",
        "\r\n",
        "## the heuristic algorithm for supervised learning\r\n",
        "\r\n",
        "1. Choose $\\lambda > 0$, $p \\in \\mathbb{N}$, $l \\in \\{0, 1, \\ldots, p-1\\}$, sampling distribution $(O_{i+1} \\; | \\; O_i) \\sim F(o_{i+1} \\; | \\; o_i)$ & $X_0 \\sim F(x_0)$, and activation function $\\sigma: \\mathbb{R}^p \\to \\mathbb{R}^p$.\r\n",
        "2. Set $n \\gets 0$, $A_0 \\gets \\lambda I_{p \\times p}$, $A_0^{-1} \\gets \\lambda^{-1}I_{p \\times p}$, $B_0 \\gets \\lambda I_{p \\times p}$, and $B_0^{-1} \\gets \\lambda^{-1} I_{p \\times p}$.\r\n",
        "3. Sample $O_n, L_n, O_{n+1}$.\r\n",
        "4. Calculate $\\hat Y_n = \\left[ \\sigma^{-1}(O_{n+1}) \\; \\big|\\; \\sigma^{-1}( \\hat L_{n+1} ) \\right] = X_n \\hat \\beta_n = X_n A_n^{-1} B_n$. \r\n",
        "5. Set $Y_n \\gets \\left[ \\sigma^{-1}(O_{n+1}) \\; \\big| \\; \\sigma^{-1}( \\hat L_{n+1} ) \\right]$\r\n",
        "6. Calculate $\\hat X_n = \\left[ \\hat O_{n+1} \\; \\big|\\; \\hat L_n \\right] \\gets Y_n \\hat \\beta_n^{-1} = Y_n B_n^{-1} A_n$.\r\n",
        "7. Set $\\hat Y_n \\gets \\left[ \\sigma^{-1}(O_{n+1}) \\; \\big|\\; \\hat L_n \\right]$.  \r\n",
        "8. Set $A_{n+1} \\gets A_n + x_i^T x_i$, where $x_i$ is the latest row addition to $X_n$.\r\n",
        "9. Set $B_{n+1} \\gets B_n + x_i^T y_i$, where $y_i$ is the latest row addition to $Y_n$.\r\n",
        "10. Set $A_{n+1}^{-1} \\gets A_n^{-1} - \\left(A_n^{-1} x_i^T x_i A_n^{-1}\\right)/\\left(1 + x_i A_n^{-1} x_i^T\\right)$. \r\n",
        "11. Set $B_{n+1}^{-1} \\gets B_n^{-1} - \\left(B_n^{-1} x_i^T y_i B_n^{-1}\\right)/\\left(1 + y_i B_n^{-1} x_i^T\\right)$.\r\n",
        "12. Increment $n \\gets n + 1$.\r\n",
        "13. Return to step 3.\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model code definitions"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class OnlineNetBase:\n",
        "    '''\n",
        "    A low-level class defining a recursive neural net employing\n",
        "    a hueristic to enable true online learning. No activation \n",
        "    functions are used. Latent variables are not abstracted-away. \n",
        "    Only the essential math is defined as `y = x a^{-1} b`.\n",
        "    '''\n",
        "    def __init__(self, p, regularizer=0.001):\n",
        "        '''\n",
        "        Initialize an Online Net\n",
        "        inputs:\n",
        "         - p: input and output vector dimension\n",
        "         - regularizer: L2 regualarization value, `>0`\n",
        "        '''\n",
        "        ## verify inputs \n",
        "        if type(p) != int:\n",
        "            raise ValueError('`p` must be an int!')\n",
        "        if type(regularizer) not in [int, float]:\n",
        "            raise ValueError('`regularizer` must be an int or float!') \n",
        "        if type(regularizer) == int:\n",
        "            regularizer = float(regularizer)\n",
        "        if p <= 0:\n",
        "            raise ValueError('`p` must be `>0`!') \n",
        "        if regularizer <= .0:\n",
        "            raise ValueError('`regularizer` must be `>0`!')\n",
        "        ## store init variables \n",
        "        self.p = p\n",
        "        self.regularizer = regularizer\n",
        "        ## construct initial estimates \n",
        "        regularizer_vec = torch.tensor([regularizer]*p) \n",
        "        inv_regularizer_vec = torch.tensor([1./regularizer]*p) \n",
        "        self.a = torch.diag(regularizer_vec)\n",
        "        self.b = torch.diag(regularizer_vec)\n",
        "        self.inv_a = torch.diag(inv_regularizer_vec)\n",
        "        self.inv_b = torch.diag(inv_regularizer_vec)\n",
        "        pass \n",
        "    \n",
        "    def predict(self, x):\n",
        "        '''\n",
        "        Predict y vector from an x vector\n",
        "        inputs:\n",
        "         - x: torch.Tensor of shape `[p]`\n",
        "        outputs:\n",
        "         - y: torch.Tensor of shape `[p]`\n",
        "        '''\n",
        "        ## verify inputs \n",
        "        if type(x) != torch.Tensor:\n",
        "            raise ValueError('`x` must be of type `torch.Tensor`!') \n",
        "        if len(x.shape) != 1:\n",
        "            raise ValueError('`x` must satisfy `len(x.shape) == 1`!') \n",
        "        if x.shape[0] != self.p:\n",
        "            raise ValueError('`x` must satisfy `x.shape[0] == p`!') \n",
        "        ## calculate output\n",
        "        y = x.reshape([1, self.p])\n",
        "        y = torch.matmul(y, self.inv_a) \n",
        "        y = torch.matmul(y, self.b) \n",
        "        return y.reshape([self.p]) \n",
        "    \n",
        "    def fit(self, x, y):\n",
        "        '''\n",
        "        Update the model to a new observation `y`.\n",
        "        inputs:\n",
        "         - x: torch.Tensor of shape `[p]` \n",
        "         - y: torch.Tensor of shape `[p]`\n",
        "        '''\n",
        "        ## verify inputs \n",
        "        if type(x) != torch.Tensor:\n",
        "            raise ValueError('`x` must be of type `torch.Tensor`!') \n",
        "        if len(x.shape) != 1:\n",
        "            raise ValueError('`x` must satisfy `len(x.shape) == 1`!') \n",
        "        if x.shape[0] != self.p:\n",
        "            raise ValueError('`x` must satisfy `x.shape[0] == p`!') \n",
        "        if type(y) != torch.Tensor:\n",
        "            raise ValueError('`y` must satisfy `type(y) == torch.Tensor`!') \n",
        "        if len(y.shape) != 1:\n",
        "            raise ValueError('`y` must satisfy `len(y.shape) == 1`!') \n",
        "        if y.shape[0] != self.p:\n",
        "            raise ValueError('`y` must satisfy `y.shape[0] == self.p`!') \n",
        "        ## update model \n",
        "        x_row = x.reshape([1, self.p]) \n",
        "        y_row = y.reshape([1, self.p]) \n",
        "        xTx = x_row.transpose(0,1).matmul(x_row) \n",
        "        xTy = x_row.transpose(0,1).matmul(y_row) \n",
        "        self.a = self.a + xTx \n",
        "        self.b = self.b + xTy \n",
        "        self.inv_a = self.inv_a - (self.inv_a.matmul(xTx).matmul(self.inv_a))/(1. + x_row.matmul(self.inv_a).matmul(x_row.transpose(0,1)))\n",
        "        self.inv_b = self.inv_b - (self.inv_b.matmul(xTy).matmul(self.inv_b))/(1. + y_row.matmul(self.inv_b).matmul(x_row.transpose(0,1)))\n",
        "        pass \n",
        "    pass \n"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1631458498117
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# first experiment: mnist classification"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Scratch space ## TODO: DELETE ME\r\n",
        "\r\n",
        "import torch\r\n",
        "\r\n",
        "x = torch.tensor([.01, .001, .0001])\r\n",
        "l = list(x.shape)\r\n",
        "print(len(l))\r\n",
        "print(l[0])\r\n",
        "\r\n",
        "\r\n",
        "y = x.reshape([1,3])\r\n",
        "y.transpose(0,1).matmul(y)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "1\n3\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 32,
          "data": {
            "text/plain": "tensor([[1.0000e-04, 1.0000e-05, 1.0000e-06],\n        [1.0000e-05, 1.0000e-06, 1.0000e-07],\n        [1.0000e-06, 1.0000e-07, 1.0000e-08]])"
          },
          "metadata": {}
        }
      ],
      "execution_count": 32,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1631462019744
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}