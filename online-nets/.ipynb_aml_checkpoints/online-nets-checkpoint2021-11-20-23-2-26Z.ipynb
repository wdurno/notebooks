{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# online nets\r\n",
        "\r\n",
        "Deep learning is powerful but computationally expensive, frequently requiring massive compute budgets. In persuit of cost-effective-yet-powerful AI, this work explores and evaluates a heuristic which should lend to more-efficient use of data through online learning.\r\n",
        "\r\n",
        "Goal: evaluate a deep learning alternative capable of true online learning. Solution requirements:\r\n",
        "\r\n",
        "1. catastrophic forgetting should be impossible;\r\n",
        "2. all data is integrated into sufficient statistics of fixed dimension;\r\n",
        "3. and our solution should have predictive power comparable to deep learning.\r\n",
        "\r\n",
        "## modeling strategy\r\n",
        "\r\n",
        "We will not attempt to derive sufficient statistics for an entire deep net, but instead leverage well-known sufficient statistics for least squares models, \r\n",
        "so will have sufficient statistics per deep net layer. If this can be empirically shown effective, we'll build-out the theory afterwards. \r\n",
        "\r\n",
        "Recognizing a deep net as a series of compositions, as follows.\r\n",
        "\r\n",
        "$ Y + \\varepsilon \\approx \\mathbb{E}Y = \\sigma_3 \\circ \\beta_3^T \\circ \\sigma_2 \\circ \\beta_2^T \\circ \\sigma_1 \\circ \\beta_1^T X $\r\n",
        "\r\n",
        "So, we can isolate invidivdual $\\beta_j$ matrices using (psuedo-)inverses $\\beta_j^{-1}$ like so.\r\n",
        "\r\n",
        "$ \\sigma_2^{-1} \\circ \\beta_3^{-1} \\circ \\sigma_3^{-1} (Y) \\approx  \\beta_2^T \\circ \\sigma_1 \\circ \\beta_1^T X $\r\n",
        "\r\n",
        "In this example, if we freeze all $\\beta_j$'s except $\\beta_2$, we are free to update $\\hat \\beta_2$ using $\\tilde Y = \\sigma_2^{-1} \\circ \\beta_3^{-1} \\circ \\sigma_3^{-1} (Y) $\r\n",
        "and $\\tilde X = \\sigma_1 \\circ \\beta_1^T X $.\r\n",
        "\r\n",
        "Using a least squares formulation for fitting to $\\left( \\tilde X, \\tilde Y \\right)$, we get sufficient statistics per layer."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model code definitions"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "TORCH_TENSOR_TYPE = type(torch.tensor(1)) \n",
        "\n",
        "class OnlineDenseLayer:\n",
        "    '''\n",
        "    A single dense net, formulated as a least squares model. \n",
        "    '''\n",
        "    def __init__(self, p, q, activation=lambda x:x, activation_inverse=lambda x:x, lam=1.): \n",
        "        '''\n",
        "        inputs:\n",
        "        - p: input dimension\n",
        "        - q: output dimension\n",
        "        - activation: non-linear function, from R^p to R^q. Default is identity\n",
        "        - activation_inverse: inverse of the activation function. Default is identity. \n",
        "        - lam: regularization term \n",
        "        '''\n",
        "        self.__validate_inputs(p=p, q=q, lam=lam) \n",
        "        self.p = p\n",
        "        self.q = q\n",
        "        self.activation = activation \n",
        "        self.activation_inverse = activation_inverse \n",
        "        self.lam = lam \n",
        "        self.xTy = torch.zeros(p+1,q) # +1 for intercept \n",
        "        self.yTx = torch.zeros(q+1,p) \n",
        "        self.xTx_inv = torch.diag(torch.tensor([lam]*(p+1))) \n",
        "        self.yTy_inv = torch.diag(torch.tensor([lam]*(q+1))) \n",
        "        self.betaT_forward = torch.matmul(self.xTx_inv, self.xTy) \n",
        "        self.betaT_forward = torch.transpose(self.betaT_forward, 0, 1) \n",
        "        self.betaT_backward = torch.matmul(self.yTy_inv, self.yTx) \n",
        "        self.betaT_backward = torch.transpose(self.betaT_backward, 0, 1) \n",
        "        self.x_forward = None \n",
        "        self.y_forward = None \n",
        "        self.x_backward = None \n",
        "        self.y_backward = None \n",
        "        pass \n",
        "    def forward(self, x): \n",
        "        'creates and stores x_forward and y_forward, then returns activation(y_forward)' \n",
        "        self.__validate_inputs(x=x, p=self.p) \n",
        "        self.x_forward = x \n",
        "        x = torch.cat((torch.tensor([[1.]]), x), dim=0) # intercept \n",
        "        self.y_forward = torch.matmul(self.betaT_forward, x) # predict \n",
        "        return self.activation(self.y_forward) \n",
        "    def backward(self, y): \n",
        "        'creates and stores x_backward and y_backward, then returns y_backward'\n",
        "        y = self.activation_inverse(y) \n",
        "        self.__validate_inputs(y=y, q=self.q) \n",
        "        self.y_backward = y \n",
        "        y = torch.cat((torch.tensor([[1.]]), y), dim=0) \n",
        "        self.x_backward = torch.matmul(self.betaT_backward, y) \n",
        "        return self.x_backward \n",
        "    def forward_fit(self): \n",
        "        'uses x_forward and y_backward to update forward model, then returns Sherman Morrison denominator' \n",
        "        self.__validate_inputs(x=self.x_forward, y=self.y_backward, p=self.p, q=self.q) \n",
        "        x = torch.cat((torch.tensor([[1.]]), self.x_forward), dim=0) \n",
        "        self.xTx_inv, sm_denom = self.__sherman_morrison(self.xTx_inv, x, x) \n",
        "        self.xTy += torch.matmul(x, torch.transpose(self.y_backward, 0, 1)) \n",
        "        self.betaT_forward = torch.matmul(self.xTx_inv, self.xTy) \n",
        "        self.betaT_forward = torch.transpose(self.betaT_forward, 0, 1) \n",
        "        return sm_denom \n",
        "    def backward_fit(self):\n",
        "        'uses x_backward and y_forward to update backward model, then returns Sherman Morrison denominator'\n",
        "        self.__validate_inputs(x=self.x_forward, y=self.y_backward, p=self.p, q=self.q) \n",
        "        y = torch.cat((torch.tensor([[1.]]), self.y_backward), dim=0)\n",
        "        self.yTy_inv, sm_denom = self.__sherman_morrison(self.yTy_inv, y, y) \n",
        "        self.yTx += torch.matmul(y, torch.transpose(self.x_backward, 0, 1)) \n",
        "        self.betaT_backward = torch.matmul(self.yTy_inv, self.yTx)\n",
        "        self.betaT_backward = torch.transpose(self.betaT_backward, 0, 1) \n",
        "        return sm_denom \n",
        "    def __sherman_morrison(self, inv_mat, vec1, vec2):\n",
        "        '''\n",
        "        applies Sherman Morrison updates, (mat + vec1 vec2^T)^{-1}\n",
        "        inputs:\n",
        "        - inv_mat: an inverted matrix \n",
        "        - vec1: a column vector \n",
        "        - vec2: a column vector \n",
        "        returns:\n",
        "        - updated matrix\n",
        "        - the Sherman Morrison denominator, for tracking numerical stability \n",
        "        '''\n",
        "        v2t = torch.transpose(vec2, 0, 1)\n",
        "        denominator = 1. + torch.matmul(torch.matmul(v2t, inv_mat), vec1) \n",
        "        numerator = torch.matmul(torch.matmul(inv_mat, vec1), torch.matmul(v2t, inv_mat)) \n",
        "        updated_inv_mat = inv_mat - numerator / denominator \n",
        "        return updated_inv_mat, float(denominator) \n",
        "    def __validate_inputs(self, p=None, q=None, lam=None, x=None, y=None): \n",
        "        'raises value exceptions if provided parameters are invalid'\n",
        "        if q is not None:\n",
        "            if not isinstance(q, int):\n",
        "                raise ValueError('`q` must be int!')\n",
        "            if q <= 0:\n",
        "                raise ValueError('`q` must be greater than zero!')\n",
        "        if p is not None:\n",
        "            if not isinstance(p, int): \n",
        "                raise ValueError('`p` must be int!')\n",
        "            if p <= 0: \n",
        "                raise ValueError('`p` must be greater than zero!')\n",
        "        if lam is not None:\n",
        "            if not (isinstance(lam, float) or isinstance(lam, int)):\n",
        "                raise ValueError('`lam` must be float or int!')\n",
        "            if lam < 0:\n",
        "                raise ValueError('`lam` must be non-negative!')\n",
        "        if x is not None and p is not None: \n",
        "            if type(x) != TORCH_TENSOR_TYPE:\n",
        "                raise ValueError('`x` must be of type `torch.tensor`!') \n",
        "            if list(x.shape) != [p,1]: \n",
        "                raise ValueError('`x.shape` must be `[p,1]`!') \n",
        "            if torch.isnan(x).any():\n",
        "                raise ValueError('`x` contains `nan`!')\n",
        "            pass \n",
        "        if y is not None and q is not None: \n",
        "            if type(y) != TORCH_TENSOR_TYPE:\n",
        "                raise ValueError('`y` must be of type `torch.tensor`!') \n",
        "            if list(y.shape) != [q,1]: \n",
        "                raise ValueError('`y.shape` must be `[q,1]`') \n",
        "            if torch.isnan(y).any():\n",
        "                raise ValueError('`y` contains `nan`!')\n",
        "            pass \n",
        "        pass \n",
        "    pass\n",
        "\n",
        "class OnlineNet: \n",
        "    'online, sequential dense net' \n",
        "    def __init__(self, layer_list): \n",
        "        ## validate inputs \n",
        "        if type(layer_list) != list: \n",
        "            raise ValueError('`layer_list` must be of type list!') \n",
        "        for layer in layer_list: \n",
        "            if not issubclass(type(layer), OnlineDenseLayer):\n",
        "                raise ValueError('each item in `layer_list` must be an instance of a subclass of `OnlineDenseLayer`!') \n",
        "        ## assign \n",
        "        self.layer_list = layer_list \n",
        "        pass \n",
        "    def forward(self, x): \n",
        "        'predict forward'\n",
        "        for layer in self.layer_list:\n",
        "            x = layer.forward(x) \n",
        "        return x \n",
        "    def backward(self, y):\n",
        "        'predict backward'\n",
        "        for layer in reversed(self.layer_list): \n",
        "            y = layer.backward(y) \n",
        "        return y \n",
        "    def fit(self): \n",
        "        'assumes layers x & y targets have already been set. Returns Sherman Morrison denominators per layer in (forward, backward) pairs in a list'\n",
        "        sherman_morrison_denominator_list = [] \n",
        "        for layer in self.layer_list:\n",
        "            forward_smd = layer.forward_fit() \n",
        "            backward_smd = layer.backward_fit() \n",
        "            sherman_morrison_denominator_list.append((forward_smd, backward_smd))\n",
        "        return sherman_morrison_denominator_list \n",
        "    def __reduce_sherman_morrison_denominator_list(self, smd_pair_list):\n",
        "        'returns the value closest to zero'\n",
        "        if type(smd_pair_list) != list: \n",
        "            raise ValueError('`smd_pair_list` must be of type `list`!')\n",
        "        if len(smd_pair_list) == 0:\n",
        "            return None \n",
        "        smallest_smd = None \n",
        "        for smd_pair in smd_pair_list:\n",
        "            if type(smd_pair) != tuple:\n",
        "                raise ValueError('`smd_pair_list` must be list of tuples!')\n",
        "            if smallest_smd is None: \n",
        "                smallest_smd = smd_pair[0] \n",
        "            if abs(smallest_smd) > abs(smd_pair[0]): \n",
        "                smallest_smd = smd_pair[0] \n",
        "            if abs(smallest_smd) > abs(smd_pair[1]):\n",
        "                smallest_smd = smd_pair[1] \n",
        "        return float(smallest_smd) \n",
        "    def __call__(self, x, y=None): \n",
        "        '''\n",
        "        If only x is given, a prediction is made and returned.\n",
        "        If x and y are given, then the model is updated, and returns\n",
        "        - the prediction\n",
        "        - the sherman morrison denominator closest to zero, for tracking numerical stability\n",
        "        '''\n",
        "        y_hat = self.forward(x) \n",
        "        if y is None: \n",
        "            return y_hat \n",
        "        self.backward(y) \n",
        "        self.layer_list[0].x_forward = x \n",
        "        self.layer_list[0].x_backward = x \n",
        "        self.layer_list[-1].y_forward = y \n",
        "        self.layer_list[-1].y_backward = y \n",
        "        smd_pair_list = self.fit() \n",
        "        smallest_smd = self.__reduce_sherman_morrison_denominator_list(smd_pair_list) \n",
        "        return y_hat, smallest_smd "
      ],
      "outputs": [],
      "execution_count": 59,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1637943226909
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# first experiment: mnist classification"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\r\n",
        "from torchvision import datasets, transforms\r\n",
        "\r\n",
        "transform=transforms.Compose([\r\n",
        "    transforms.ToTensor(),\r\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\r\n",
        "    ])\r\n",
        "\r\n",
        "dataset1 = datasets.MNIST('../../data', train=True, download=True, transform=transform)\r\n",
        "dataset2 = datasets.MNIST('../../data', train=False, transform=transform)\r\n",
        "\r\n",
        "train_loader = torch.utils.data.DataLoader(dataset1)\r\n",
        "test_loader = torch.utils.data.DataLoader(dataset2)\r\n",
        "\r\n",
        "n_labels = 10 \r\n",
        "\r\n",
        "## activation functions \r\n",
        "## torch.sigmoid \r\n",
        "inv_sigmoid = lambda x: -torch.log((1/(x+1e-8))-1) \r\n",
        "leaky_relu_alpha = .1 \r\n",
        "leaky_relu = lambda x: (x > 0)*x + (x <= 0)*x*leaky_relu_alpha \r\n",
        "inv_leaky_relu = lambda x: (x > 0)*x + (x <= 0)*x/leaky_relu_alpha \r\n",
        "\r\n",
        "model = OnlineNet(\r\n",
        "    [\r\n",
        "        #OnlineDenseLayer(p=1*1*28*28, q=1000, activation=torch.sigmoid, activation_inverse=inv_sigmoid),  \r\n",
        "        #OnlineDenseLayer(p=1000, q=5000, activation=torch.sigmoid, activation_inverse=inv_sigmoid), \r\n",
        "        #OnlineDenseLayer(p=5000, q=100, activation=torch.sigmoid, activation_inverse=inv_sigmoid), \r\n",
        "        #OnlineDenseLayer(p=100, q=n_labels, activation=torch.sigmoid, activation_inverse=inv_sigmoid)\r\n",
        "        OnlineDenseLayer(p=3, q=2, activation=leaky_relu, activation_inverse=inv_leaky_relu),\r\n",
        "        OnlineDenseLayer(p=2, q=1)\r\n",
        "    ] \r\n",
        ")\r\n",
        "\r\n",
        "def build_data(image, label): \r\n",
        "    'format data from iterator for model' \r\n",
        "    y = torch.tensor([1. if int(label[0]) == idx else 0. for idx in range(n_labels)]) ## one-hot representation \r\n",
        "    x = image.reshape([-1]) ## flatten \r\n",
        "    ## shrink so sigmoid inverse is well-defined \r\n",
        "    y = y*.90 + .05 \r\n",
        "    ## reshape to column vectors \r\n",
        "    x = x.reshape([-1,1])\r\n",
        "    y = y.reshape([-1,1])\r\n",
        "    return x, y \r\n",
        "\r\n",
        "def build_test_data():\r\n",
        "    x = torch.normal(mean=torch.zeros([3,1]))\r\n",
        "    y = torch.sigmoid(3. + 5.*x[0] - 10.*x[1])\r\n",
        "    y = y + 3*x[2]\r\n",
        "    y = y.reshape([-1,1])\r\n",
        "    return x, y\r\n",
        "\r\n",
        "errs = [] \r\n",
        "pbar = tqdm(train_loader) \r\n",
        "for [image, label] in pbar: \r\n",
        "    #x, y = build_data(image, label) \r\n",
        "    x, y = build_test_data() \r\n",
        "    ## fit \r\n",
        "    y_hat, stability = model(x, y) \r\n",
        "    err = float((y - y_hat).abs().sum()) \r\n",
        "    errs.append(err) \r\n",
        "    pbar.set_description(f'err: {err}, stab: {stability}') \r\n",
        "    ## train error \r\n",
        "    ## TODO "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "err: 3.0371553897857666, stab: 1.0003470182418823:   5%|â–         | 2882/60000 [00:05<01:42, 559.73it/s]  \n"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-c817a6b086b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m## fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0merrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-c23e089ca9b8>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_forward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_backward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0msmd_pair_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0msmallest_smd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__reduce_sherman_morrison_denominator_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmd_pair_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmallest_smd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-c23e089ca9b8>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0msherman_morrison_denominator_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0mforward_smd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m             \u001b[0mbackward_smd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0msherman_morrison_denominator_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward_smd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward_smd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-c23e089ca9b8>\u001b[0m in \u001b[0;36mforward_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__validate_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_backward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_forward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxTx_inv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msm_denom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__sherman_morrison\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxTx_inv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxTy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_backward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbetaT_forward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxTx_inv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxTy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 75,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cs = torch.cumsum(torch.tensor(errs), dim=0)\r\n",
        "ma = (cs[100:] - cs[:-100])/100.\r\n",
        "list(ma)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 89,
          "data": {
            "text/plain": "[tensor(2.4054),\n tensor(2.3698),\n tensor(2.3624),\n tensor(2.3778),\n tensor(2.3792),\n tensor(2.4078),\n tensor(2.3771),\n tensor(2.3687),\n tensor(2.3677),\n tensor(2.4234),\n tensor(2.4001),\n tensor(2.4140),\n tensor(2.3619),\n tensor(2.3363),\n tensor(2.2973),\n tensor(2.2712),\n tensor(2.2751),\n tensor(2.2557),\n tensor(2.2674),\n tensor(2.2304),\n tensor(2.2235),\n tensor(2.2234),\n tensor(2.1952),\n tensor(2.1795),\n tensor(2.1764),\n tensor(2.1533),\n tensor(2.1417),\n tensor(2.1041),\n tensor(2.0624),\n tensor(2.0774),\n tensor(2.1180),\n tensor(2.1029),\n tensor(2.1038),\n tensor(2.1178),\n tensor(2.1345),\n tensor(2.1305),\n tensor(2.1291),\n tensor(2.1316),\n tensor(2.1430),\n tensor(2.1136),\n tensor(2.0851),\n tensor(2.1003),\n tensor(2.1138),\n tensor(2.1065),\n tensor(2.1218),\n tensor(2.1055),\n tensor(2.1398),\n tensor(2.1007),\n tensor(2.1166),\n tensor(2.1072),\n tensor(2.1168),\n tensor(2.1163),\n tensor(2.0727),\n tensor(2.0722),\n tensor(2.0782),\n tensor(2.1035),\n tensor(2.1093),\n tensor(2.1182),\n tensor(2.1383),\n tensor(2.1187),\n tensor(2.0899),\n tensor(2.1160),\n tensor(2.1160),\n tensor(2.0891),\n tensor(2.0591),\n tensor(2.0914),\n tensor(2.0897),\n tensor(2.1296),\n tensor(2.0983),\n tensor(2.1045),\n tensor(2.1037),\n tensor(2.1053),\n tensor(2.0778),\n tensor(2.0597),\n tensor(2.0854),\n tensor(2.1043),\n tensor(2.1069),\n tensor(2.1253),\n tensor(2.0873),\n tensor(2.0504),\n tensor(2.0724),\n tensor(2.0871),\n tensor(2.1108),\n tensor(2.0935),\n tensor(2.0739),\n tensor(2.0897),\n tensor(2.0953),\n tensor(2.0743),\n tensor(2.0727),\n tensor(2.0773),\n tensor(2.0543),\n tensor(2.0364),\n tensor(2.0284),\n tensor(2.0145),\n tensor(2.0283),\n tensor(2.0280),\n tensor(2.0395),\n tensor(2.0587),\n tensor(2.0284),\n tensor(2.0177),\n tensor(2.0315),\n tensor(2.0528),\n tensor(2.0379),\n tensor(2.0277),\n tensor(2.0117),\n tensor(2.0106),\n tensor(2.0116),\n tensor(2.0124),\n tensor(2.0579),\n tensor(1.9989),\n tensor(2.0217),\n tensor(2.0311),\n tensor(2.0505),\n tensor(2.0513),\n tensor(2.0555),\n tensor(2.0766),\n tensor(2.0644),\n tensor(2.0603),\n tensor(2.0363),\n tensor(2.0358),\n tensor(2.0392),\n tensor(2.0598),\n tensor(2.0809),\n tensor(2.0843),\n tensor(2.0971),\n tensor(2.1108),\n tensor(2.1230),\n tensor(2.1302),\n tensor(2.1311),\n tensor(2.1581),\n tensor(2.1205),\n tensor(2.1187),\n tensor(2.1029),\n tensor(2.1219),\n tensor(2.1313),\n tensor(2.1500),\n tensor(2.1404),\n tensor(2.1773),\n tensor(2.1598),\n tensor(2.1801),\n tensor(2.1833),\n tensor(2.1770),\n tensor(2.1796),\n tensor(2.1670),\n tensor(2.1621),\n tensor(2.1727),\n tensor(2.1494),\n tensor(2.1457),\n tensor(2.1315),\n tensor(2.2009),\n tensor(2.1897),\n tensor(2.1987),\n tensor(2.2185),\n tensor(2.2150),\n tensor(2.1761),\n tensor(2.1541),\n tensor(2.1116),\n tensor(2.1225),\n tensor(2.0914),\n tensor(2.0861),\n tensor(2.0851),\n tensor(2.0582),\n tensor(2.0779),\n tensor(2.0664),\n tensor(2.0786),\n tensor(2.0515),\n tensor(2.0557),\n tensor(2.0104),\n tensor(2.0304),\n tensor(2.0084),\n tensor(2.0454),\n tensor(2.0126),\n tensor(2.0229),\n tensor(2.0343),\n tensor(2.0061),\n tensor(1.9786),\n tensor(1.9858),\n tensor(1.9921),\n tensor(1.9858),\n tensor(2.0087),\n tensor(2.0012),\n tensor(1.9854),\n tensor(1.9668),\n tensor(1.9673),\n tensor(1.9754),\n tensor(1.9867),\n tensor(1.9900),\n tensor(1.9741),\n tensor(1.9958),\n tensor(1.9724),\n tensor(1.9799),\n tensor(1.9797),\n tensor(1.9896),\n tensor(1.9835),\n tensor(1.9751),\n tensor(1.9996),\n tensor(2.0394),\n tensor(2.0477),\n tensor(2.0496),\n tensor(2.0504),\n tensor(2.0697),\n tensor(2.0667),\n tensor(2.0585),\n tensor(2.0565),\n tensor(2.0720),\n tensor(2.0611),\n tensor(2.0833),\n tensor(2.0667),\n tensor(1.9996),\n tensor(2.0815),\n tensor(2.1133),\n tensor(2.0927),\n tensor(2.1129),\n tensor(2.1304),\n tensor(2.1554),\n tensor(2.1827),\n tensor(2.2074),\n tensor(2.2248),\n tensor(2.2391),\n tensor(2.2451),\n tensor(2.2337),\n tensor(2.2393),\n tensor(2.2289),\n tensor(2.2225),\n tensor(2.2233),\n tensor(2.1979),\n tensor(2.1822),\n tensor(2.2130),\n tensor(2.2412),\n tensor(2.2374),\n tensor(2.2420),\n tensor(2.2608),\n tensor(2.2641),\n tensor(2.2527),\n tensor(2.2388),\n tensor(2.2181),\n tensor(2.3009),\n tensor(2.2634),\n tensor(2.2651),\n tensor(2.2686),\n tensor(2.2677),\n tensor(2.2891),\n tensor(2.3019),\n tensor(2.3501),\n tensor(2.3391),\n tensor(2.3423),\n tensor(2.3848),\n tensor(2.3621),\n tensor(2.3973),\n tensor(2.3402),\n tensor(2.3982),\n tensor(2.3878),\n tensor(2.4068),\n tensor(2.4653),\n tensor(2.4578),\n tensor(2.4446),\n tensor(2.4833),\n tensor(2.4775),\n tensor(2.5178),\n tensor(2.5024),\n tensor(2.5075),\n tensor(2.5126),\n tensor(2.5214),\n tensor(2.5463),\n tensor(2.5523),\n tensor(2.5243),\n tensor(2.5304),\n tensor(2.5560),\n tensor(2.5290),\n tensor(2.5310),\n tensor(2.4850),\n tensor(2.5219),\n tensor(2.5165),\n tensor(2.5293),\n tensor(2.5264),\n tensor(2.5416),\n tensor(2.5212),\n tensor(2.4952),\n tensor(2.4881),\n tensor(2.4701),\n tensor(2.4541),\n tensor(2.4881),\n tensor(2.4825),\n tensor(2.5408),\n tensor(2.5883),\n tensor(2.6031),\n tensor(2.6650),\n tensor(2.6645),\n tensor(2.6891),\n tensor(2.6838),\n tensor(2.7025),\n tensor(2.6870),\n tensor(2.6760),\n tensor(2.7376),\n tensor(2.7339),\n tensor(2.7214),\n tensor(2.6683),\n tensor(2.6603),\n tensor(2.6704),\n tensor(2.6494),\n tensor(2.6497),\n tensor(2.6789),\n tensor(2.7136),\n tensor(2.7467),\n tensor(2.7647),\n tensor(2.7955),\n tensor(2.7690),\n tensor(2.7782),\n tensor(2.7735),\n tensor(2.7179),\n tensor(2.6649),\n tensor(2.7138),\n tensor(2.6958),\n tensor(2.7114),\n tensor(2.7472),\n tensor(2.6863),\n tensor(2.6862),\n tensor(2.6746),\n tensor(2.6364),\n tensor(2.6273),\n tensor(2.6543),\n tensor(2.6617),\n tensor(2.6845),\n tensor(2.7067),\n tensor(2.7156),\n tensor(2.7172),\n tensor(2.7200),\n tensor(2.6785),\n tensor(2.6486),\n tensor(2.6110),\n tensor(2.6183),\n tensor(2.6130),\n tensor(2.6106),\n tensor(2.5897),\n tensor(2.6228),\n tensor(2.6780),\n tensor(2.6197),\n tensor(2.6368),\n tensor(2.6429),\n tensor(2.6484),\n tensor(2.6448),\n tensor(2.6219),\n tensor(2.6200),\n tensor(2.5605),\n tensor(2.5931),\n tensor(2.5754),\n tensor(2.5640),\n tensor(2.5817),\n tensor(2.5929),\n tensor(2.5864),\n tensor(2.5565),\n tensor(2.5671),\n tensor(2.5160),\n tensor(2.4776),\n tensor(2.5373),\n tensor(2.5657),\n tensor(2.5483),\n tensor(2.5578),\n tensor(2.5535),\n tensor(2.5731),\n tensor(2.5524),\n tensor(2.5715),\n tensor(2.5549),\n tensor(2.5541),\n tensor(2.5636),\n tensor(2.5726),\n tensor(2.5615),\n tensor(2.5318),\n tensor(2.5337),\n tensor(2.5355),\n tensor(2.5335),\n tensor(2.5007),\n tensor(2.5317),\n tensor(2.5092),\n tensor(2.5016),\n tensor(2.4926),\n tensor(2.5178),\n tensor(2.5378),\n tensor(2.5458),\n tensor(2.5577),\n tensor(2.5743),\n tensor(2.5513),\n tensor(2.5464),\n tensor(2.4842),\n tensor(2.4812),\n tensor(2.4782),\n tensor(2.4352),\n tensor(2.4385),\n tensor(2.3991),\n tensor(2.4095),\n tensor(2.3946),\n tensor(2.3875),\n tensor(2.3881),\n tensor(2.3354),\n tensor(2.3564),\n tensor(2.3520),\n tensor(2.3655),\n tensor(2.3695),\n tensor(2.3682),\n tensor(2.3800),\n tensor(2.3615),\n tensor(2.3610),\n tensor(2.3298),\n tensor(2.2970),\n tensor(2.2701),\n tensor(2.2606),\n tensor(2.2585),\n tensor(2.3053),\n tensor(2.3254),\n tensor(2.2994),\n tensor(2.3030),\n tensor(2.2866),\n tensor(2.2736),\n tensor(2.2344),\n tensor(2.2389),\n tensor(2.2437),\n tensor(2.2208),\n tensor(2.2305),\n tensor(2.2344),\n tensor(2.2675),\n tensor(2.2656),\n tensor(2.2436),\n tensor(2.2423),\n tensor(2.2210),\n tensor(2.2647),\n tensor(2.2843),\n tensor(2.2992),\n tensor(2.3164),\n tensor(2.3089),\n tensor(2.3392),\n tensor(2.3326),\n tensor(2.3416),\n tensor(2.3290),\n tensor(2.3533),\n tensor(2.3444),\n tensor(2.3020),\n tensor(2.3059),\n tensor(2.2909),\n tensor(2.3373),\n tensor(2.2997),\n tensor(2.3091),\n tensor(2.3007),\n tensor(2.2705),\n tensor(2.2577),\n tensor(2.2278),\n tensor(2.2509),\n tensor(2.2075),\n tensor(2.1926),\n tensor(2.1670),\n tensor(2.2006),\n tensor(2.1935),\n tensor(2.2000),\n tensor(2.2087),\n tensor(2.1957),\n tensor(2.1324),\n tensor(2.1484),\n tensor(2.1430),\n tensor(2.1817),\n tensor(2.1803),\n tensor(2.1705),\n tensor(2.1690),\n tensor(2.1786),\n tensor(2.2269),\n tensor(2.2133),\n tensor(2.1915),\n tensor(2.2107),\n tensor(2.2374),\n tensor(2.2500),\n tensor(2.2598),\n tensor(2.3111),\n tensor(2.3652),\n tensor(2.3961),\n tensor(2.3748),\n tensor(2.3755),\n tensor(2.4186),\n tensor(2.4126),\n tensor(2.3977),\n tensor(2.4031),\n tensor(2.4097),\n tensor(2.4306),\n tensor(2.4260),\n tensor(2.4349),\n tensor(2.4713),\n tensor(2.4663),\n tensor(2.4262),\n tensor(2.4263),\n tensor(2.4222),\n tensor(2.4226),\n tensor(2.4257),\n tensor(2.4056),\n tensor(2.4135),\n tensor(2.4344),\n tensor(2.4664),\n tensor(2.4588),\n tensor(2.4571),\n tensor(2.4663),\n tensor(2.4653),\n tensor(2.4827),\n tensor(2.4684),\n tensor(2.4770),\n tensor(2.4592),\n tensor(2.4406),\n tensor(2.5011),\n tensor(2.5038),\n tensor(2.5140),\n tensor(2.4990),\n tensor(2.5111),\n tensor(2.4592),\n tensor(2.4727),\n tensor(2.4792),\n tensor(2.5217),\n tensor(2.5816),\n tensor(2.5877),\n tensor(2.5889),\n tensor(2.5561),\n tensor(2.6057),\n tensor(2.6152),\n tensor(2.5978),\n tensor(2.6484),\n tensor(2.6345),\n tensor(2.6476),\n tensor(2.6385),\n tensor(2.6328),\n tensor(2.6426),\n tensor(2.5961),\n tensor(2.5784),\n tensor(2.5887),\n tensor(2.5864),\n tensor(2.5952),\n tensor(2.5944),\n tensor(2.6075),\n tensor(2.6002),\n tensor(2.6115),\n tensor(2.6197),\n tensor(2.6100),\n tensor(2.6098),\n tensor(2.6020),\n tensor(2.6233),\n tensor(2.6251),\n tensor(2.6140),\n tensor(2.6000),\n tensor(2.5880),\n tensor(2.6381),\n tensor(2.6348),\n tensor(2.6553),\n tensor(2.6484),\n tensor(2.6733),\n tensor(2.6724),\n tensor(2.6500),\n tensor(2.6398),\n tensor(2.6368),\n tensor(2.6396),\n tensor(2.6580),\n tensor(2.6688),\n tensor(2.7045),\n tensor(2.6788),\n tensor(2.6831),\n tensor(2.6180),\n tensor(2.5939),\n tensor(2.5838),\n tensor(2.6516),\n tensor(2.6448),\n tensor(2.6015),\n tensor(2.5947),\n tensor(2.6219),\n tensor(2.6144),\n tensor(2.5963),\n tensor(2.5994),\n tensor(2.6363),\n tensor(2.6116),\n tensor(2.5786),\n tensor(2.5529),\n tensor(2.5427),\n tensor(2.5541),\n tensor(2.5079),\n tensor(2.5582),\n tensor(2.5478),\n tensor(2.5562),\n tensor(2.5490),\n tensor(2.5030),\n tensor(2.5171),\n tensor(2.4963),\n tensor(2.4694),\n tensor(2.5124),\n tensor(2.5075),\n tensor(2.5092),\n tensor(2.4887),\n tensor(2.4975),\n tensor(2.5107),\n tensor(2.5146),\n tensor(2.4949),\n tensor(2.5188),\n tensor(2.5092),\n tensor(2.5203),\n tensor(2.5063),\n tensor(2.5341),\n tensor(2.5258),\n tensor(2.5185),\n tensor(2.5214),\n tensor(2.5471),\n tensor(2.6069),\n tensor(2.5963),\n tensor(2.5304),\n tensor(2.5505),\n tensor(2.5493),\n tensor(2.5756),\n tensor(2.5684),\n tensor(2.5768),\n tensor(2.5656),\n tensor(2.5998),\n tensor(2.5411),\n tensor(2.4768),\n tensor(2.4855),\n tensor(2.5015),\n tensor(2.5052),\n tensor(2.4662),\n tensor(2.4777),\n tensor(2.4836),\n tensor(2.4912),\n tensor(2.4643),\n tensor(2.4289),\n tensor(2.4434),\n tensor(2.4406),\n tensor(2.5043),\n tensor(2.4795),\n tensor(2.4997),\n tensor(2.4696),\n tensor(2.4710),\n tensor(2.4961),\n tensor(2.4642),\n tensor(2.4725),\n tensor(2.4945),\n tensor(2.5181),\n tensor(2.4866),\n tensor(2.5487),\n tensor(2.5787),\n tensor(2.5685),\n tensor(2.5600),\n tensor(2.5077),\n tensor(2.5280),\n tensor(2.5514),\n tensor(2.5702),\n tensor(2.5558),\n tensor(2.5547),\n tensor(2.5443),\n tensor(2.5453),\n tensor(2.5603),\n tensor(2.5770),\n tensor(2.6273),\n tensor(2.6182),\n tensor(2.6090),\n tensor(2.5824),\n tensor(2.5562),\n tensor(2.5905),\n tensor(2.6054),\n tensor(2.6090),\n tensor(2.6009),\n tensor(2.6222),\n tensor(2.6384),\n tensor(2.6455),\n tensor(2.6058),\n tensor(2.6453),\n tensor(2.6495),\n tensor(2.6958),\n tensor(2.6888),\n tensor(2.7100),\n tensor(2.7084),\n tensor(2.7344),\n tensor(2.7334),\n tensor(2.7142),\n tensor(2.7204),\n tensor(2.7341),\n tensor(2.7524),\n tensor(2.7295),\n tensor(2.7331),\n tensor(2.7252),\n tensor(2.7400),\n tensor(2.7066),\n tensor(2.7008),\n tensor(2.7194),\n tensor(2.7048),\n tensor(2.6962),\n tensor(2.7170),\n tensor(2.6874),\n tensor(2.7162),\n tensor(2.7298),\n tensor(2.7418),\n tensor(2.7607),\n tensor(2.7422),\n tensor(2.7702),\n tensor(2.7886),\n tensor(2.7857),\n tensor(2.7664),\n tensor(2.7781),\n tensor(2.7856),\n tensor(2.7758),\n tensor(2.8061),\n tensor(2.7876),\n tensor(2.7865),\n tensor(2.7831),\n tensor(2.7182),\n tensor(2.7380),\n tensor(2.8005),\n tensor(2.8061),\n tensor(2.7907),\n tensor(2.7545),\n tensor(2.7590),\n tensor(2.7402),\n tensor(2.7366),\n tensor(2.7011),\n tensor(2.7032),\n tensor(2.6920),\n tensor(2.6708),\n tensor(2.6700),\n tensor(2.6954),\n tensor(2.6881),\n tensor(2.6649),\n tensor(2.6619),\n tensor(2.5925),\n tensor(2.6032),\n tensor(2.6053),\n tensor(2.6429),\n tensor(2.6127),\n tensor(2.5761),\n tensor(2.6080),\n tensor(2.6089),\n tensor(2.6207),\n tensor(2.6038),\n tensor(2.5829),\n tensor(2.5927),\n tensor(2.5654),\n tensor(2.5603),\n tensor(2.5467),\n tensor(2.5582),\n tensor(2.4892),\n tensor(2.4762),\n tensor(2.4633),\n tensor(2.4446),\n tensor(2.4585),\n tensor(2.4489),\n tensor(2.4685),\n tensor(2.4469),\n tensor(2.4517),\n tensor(2.4723),\n tensor(2.5219),\n tensor(2.4914),\n tensor(2.4712),\n tensor(2.5271),\n tensor(2.5203),\n tensor(2.5022),\n tensor(2.5482),\n tensor(2.5615),\n tensor(2.5773),\n tensor(2.6070),\n tensor(2.5724),\n tensor(2.5486),\n tensor(2.5277),\n tensor(2.5523),\n tensor(2.5205),\n tensor(2.5360),\n tensor(2.5089),\n tensor(2.4358),\n tensor(2.4764),\n tensor(2.4348),\n tensor(2.4628),\n tensor(2.4290),\n tensor(2.4324),\n tensor(2.4490),\n tensor(2.4197),\n tensor(2.4130),\n tensor(2.3857),\n tensor(2.3857),\n tensor(2.3874),\n tensor(2.4178),\n tensor(2.4170),\n tensor(2.4172),\n tensor(2.4144),\n tensor(2.4278),\n tensor(2.4329),\n tensor(2.4138),\n tensor(2.3965),\n tensor(2.4099),\n tensor(2.3949),\n tensor(2.4138),\n tensor(2.4076),\n tensor(2.3994),\n tensor(2.4062),\n tensor(2.4222),\n tensor(2.4164),\n tensor(2.3834),\n tensor(2.3730),\n tensor(2.3581),\n tensor(2.3590),\n tensor(2.3538),\n tensor(2.3608),\n tensor(2.3299),\n tensor(2.3376),\n tensor(2.3477),\n tensor(2.3897),\n tensor(2.3721),\n tensor(2.3709),\n tensor(2.3566),\n tensor(2.3297),\n tensor(2.4053),\n tensor(2.4093),\n tensor(2.4107),\n tensor(2.4132),\n tensor(2.4689),\n tensor(2.4528),\n tensor(2.4704),\n tensor(2.4719),\n tensor(2.4510),\n tensor(2.5153),\n tensor(2.4897),\n tensor(2.4408),\n tensor(2.4433),\n tensor(2.4906),\n tensor(2.5021),\n tensor(2.5477),\n tensor(2.5992),\n tensor(2.6400),\n tensor(2.6218),\n tensor(2.6287),\n tensor(2.6188),\n tensor(2.6120),\n tensor(2.5876),\n tensor(2.5957),\n tensor(2.6167),\n tensor(2.6306),\n tensor(2.6247),\n tensor(2.6375),\n tensor(2.6285),\n tensor(2.6181),\n tensor(2.6387),\n tensor(2.6186),\n tensor(2.5928),\n tensor(2.5951),\n tensor(2.6583),\n tensor(2.6391),\n tensor(2.6565),\n tensor(2.6404),\n tensor(2.6398),\n tensor(2.5990),\n tensor(2.6171),\n tensor(2.6178),\n tensor(2.6334),\n tensor(2.6260),\n tensor(2.6025),\n tensor(2.5902),\n tensor(2.5995),\n tensor(2.5570),\n tensor(2.5833),\n tensor(2.5999),\n tensor(2.5606),\n tensor(2.5656),\n tensor(2.5675),\n tensor(2.6057),\n tensor(2.5552),\n tensor(2.5756),\n tensor(2.5981),\n tensor(2.6460),\n tensor(2.6440),\n tensor(2.6262),\n tensor(2.6193),\n tensor(2.5895),\n tensor(2.6089),\n tensor(2.6077),\n tensor(2.5600),\n tensor(2.5530),\n tensor(2.5716),\n tensor(2.5946),\n tensor(2.5780),\n tensor(2.5924),\n tensor(2.5738),\n tensor(2.5964),\n tensor(2.5698),\n tensor(2.5679),\n tensor(2.5426),\n tensor(2.5421),\n tensor(2.5893),\n tensor(2.6357),\n tensor(2.6506),\n tensor(2.6512),\n tensor(2.6531),\n tensor(2.6330),\n tensor(2.6066),\n tensor(2.6146),\n tensor(2.5953),\n tensor(2.6386),\n tensor(2.6405),\n tensor(2.6473),\n tensor(2.6408),\n tensor(2.6429),\n tensor(2.6222),\n tensor(2.6111),\n tensor(2.6239),\n tensor(2.6305),\n tensor(2.6239),\n tensor(2.6084),\n tensor(2.5885),\n tensor(2.6554),\n tensor(2.6364),\n tensor(2.6135),\n tensor(2.5547),\n tensor(2.5757),\n tensor(2.5947),\n tensor(2.6130),\n tensor(2.5646),\n tensor(2.5628),\n tensor(2.5754),\n tensor(2.6159),\n tensor(2.6451),\n tensor(2.5811),\n tensor(2.5871),\n tensor(2.6142),\n tensor(2.6052),\n tensor(2.5546),\n tensor(2.5422),\n tensor(2.5202),\n tensor(2.5150),\n tensor(2.4991),\n tensor(2.4637),\n tensor(2.4969),\n tensor(2.4909),\n tensor(2.4696),\n tensor(2.4741),\n tensor(2.4716),\n tensor(2.4706),\n tensor(2.4538),\n tensor(2.4505),\n tensor(2.4320),\n tensor(2.4421),\n tensor(2.4743),\n tensor(2.4427),\n tensor(2.4561),\n tensor(2.4741),\n tensor(2.4904),\n tensor(2.4445),\n tensor(2.4830),\n tensor(2.5210),\n tensor(2.5324),\n tensor(2.5371),\n tensor(2.5578),\n tensor(2.5314),\n tensor(2.5103),\n tensor(2.4961),\n tensor(2.4856),\n tensor(2.4644),\n tensor(2.4681),\n tensor(2.4574),\n tensor(2.4479),\n tensor(2.4424),\n tensor(2.4018),\n tensor(2.3901),\n tensor(2.4142),\n tensor(2.4319),\n tensor(2.4202),\n tensor(2.4382),\n tensor(2.4310),\n tensor(2.4115),\n tensor(2.3730),\n tensor(2.3761),\n tensor(2.3343),\n tensor(2.3968),\n tensor(2.3913),\n tensor(2.3802),\n tensor(2.3944),\n tensor(2.4193),\n tensor(2.4180),\n tensor(2.4169),\n tensor(2.4122),\n tensor(2.4188),\n tensor(2.4060),\n tensor(2.4293),\n tensor(2.4142),\n tensor(2.4420),\n tensor(2.4458),\n tensor(2.4594),\n tensor(2.4625),\n tensor(2.4140),\n tensor(2.3766),\n tensor(2.3589),\n tensor(2.3776),\n tensor(2.3631),\n tensor(2.3588),\n tensor(2.3890),\n tensor(2.3785),\n tensor(2.4025),\n tensor(2.4255),\n tensor(2.4716),\n tensor(2.4607),\n tensor(2.4384),\n tensor(2.4645),\n tensor(2.4869),\n tensor(2.4712),\n tensor(2.4796),\n tensor(2.4444),\n tensor(2.4701),\n tensor(2.4493),\n tensor(2.4630),\n ...]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 89,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1637944405339
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## scratch space\r\n",
        "\r\n",
        "$x_1, X_2, X_3, \\ldots, X_{p-1}, x_p = y$\r\n",
        "\r\n",
        "$\\beta_1^F, \\beta_1^B, \\beta_2^F, \\beta_2^B, \\ldots, \\beta_{p-1}^F, \\beta_{p-1}^B$\r\n",
        "\r\n",
        "$\\hat x_{j+1} = \\sigma \\left( \\beta_j^{FT} x_j \\right)$\r\n",
        "\r\n",
        "forward series: $x_1, \\hat x_2, \\hat x_3, \\ldots, \\hat x_{p-1}, x_p = y$\r\n",
        "\r\n",
        "$ \\hat \\beta_{p-1}^{FT} = \\text{argmin}_\\beta \\| x_p - \\beta^{T} \\hat x_{p-1} \\|^2 $\r\n",
        "\r\n",
        "$ \\hat \\beta_{p-2}^{FT} = \\text{argmin}_\\beta \\| \\hat x_{p-1} - \\beta^{T} \\hat x_{p-2} \\|^2 $ We won't do this. \r\n",
        "\r\n",
        "$ \\tilde x_{j-1} = \\sigma^{-1}\\left( \\beta_j^{BT} x_j \\right)$\r\n",
        "\r\n",
        "backward series: $x_1, \\tilde x_2, \\tilde x_3, \\ldots, \\tilde x_{p-1}, x_p = y$\r\n",
        "\r\n",
        "$ \\hat \\beta_{p-2}^{F} = \\text{argmin}_\\beta \\| \\tilde x_{p-1} - \\beta^{T} \\hat x_{p-2} \\|^2 $\r\n",
        "\r\n",
        "$ \\hat \\beta_{p-2}^{B} = \\text{argmin}_\\beta \\| \\hat x_{p-2} - \\beta^{T} \\tilde x_{p-1} \\|^2 $"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ \\hat x_3 = \\sigma \\left( \\beta_2^{FT} \\hat x_2 \\right) $\r\n",
        "\r\n",
        "$ \\hat \\beta_2^F = \\text{argmin}_\\beta \\| \\sigma^{-1}\\left( \\hat x_3 \\right) - \\beta^T \\hat x_2 \\|^2 $ Useless without $\\tilde x$\r\n",
        "\r\n",
        "$ \\tilde x_2 = \\sigma^{-1}\\left( \\beta_3^{BT} \\tilde x_3 \\right) $\r\n",
        "\r\n",
        "$ \\hat \\beta_2^B = \\text{argmin}_\\beta \\| \\sigma\\left( \\hat x_2 \\right) - \\beta^T \\hat x_3 \\|^2 $ Useless without $\\hat x$\r\n",
        "\r\n",
        "So, use these estimates instead.\r\n",
        "\r\n",
        "$ \\hat \\beta_2^F = \\text{argmin}_\\beta \\| \\sigma^{-1}\\left( \\tilde x_3 \\right) - \\beta^T \\hat x_2 \\|^2 $\r\n",
        "\r\n",
        "$ \\hat \\beta_2^B = \\text{argmin}_\\beta \\| \\sigma\\left( \\hat x_2 \\right) - \\beta^T \\tilde x_3 \\|^2 $"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\mathbb{E} l(X;\\theta) \\approx l(X;\\theta_0) + \\left( \\theta - \\theta_0 \\right)^T \\mathbb{E} \\nabla_\\theta l(X;\\theta_0) + \\left( \\theta - \\theta_0 \\right)^T \\mathbb{E} \\nabla^2_\\theta l(X;\\theta_0) \\left( \\theta - \\theta_0 \\right)/2 $\r\n",
        "\r\n",
        "$ = \\mathbb{E}l(X;\\theta_0) + 0 - \\left( \\theta - \\theta_0 \\right)^T \\mathcal{I}_{\\theta_0} \\left( \\theta - \\theta_0 \\right)/2 $\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ X_{j+1} = \\beta^T X_j + \\sigma \\varepsilon, \\; \\varepsilon \\sim N(0, \\mathcal{I}) $\r\n",
        "\r\n",
        "$ X_{j+k} = \\beta^{kT} X_j \\Rightarrow \\mathbb{E}[X_j|X_0] = \\mathbb{E}[\\beta^{jT} X_0|X_0] = \\beta^{jT} X_0 $\r\n",
        "\r\n",
        "$ \\mathbb{E}\\left[ f(X_0) \\; | \\; X_0 \\right] = f(X_0) $"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}