{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# online nets\r\n",
        "\r\n",
        "Deep learning is powerful but computationally expensive, frequently requiring massive compute budgets. In persuit of cost-effective-yet-powerful AI, this work explores and evaluates a heuristic which should lend to more-efficient use of data through online learning.\r\n",
        "\r\n",
        "Goal: evaluate a deep learning alternative capable of true online learning. Solution requirements:\r\n",
        "\r\n",
        "1. catastrophic forgetting should be impossible;\r\n",
        "2. all data is integrated into sufficient statistics of fixed dimension;\r\n",
        "3. and our solution should have predictive power comparable to deep learning.\r\n",
        "\r\n",
        "## modeling strategy\r\n",
        "\r\n",
        "We will not attempt to derive sufficient statistics for an entire deep net, but instead leverage well-known sufficient statistics for least squares models, \r\n",
        "so will have sufficient statistics per deep net layer. If this can be empirically shown effective, we'll build-out the theory afterwards. \r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model code definitions"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class OnlineDenseLayer:\n",
        "    '''\n",
        "    A single dense net, formulated as a least squares model. \n",
        "    '''\n",
        "    def __init__(self, p, q, activation=lambda x:x, activation_inverse=lambda x:x, lam=1.): \n",
        "        '''\n",
        "        inputs:\n",
        "        - p: input dimension\n",
        "        - q: output dimension\n",
        "        - activation: non-linear function, from R^p to R^q. Default is identity\n",
        "        - activation_inverse: inverse of the activation function. Default is identity. \n",
        "        - lam: regularization term \n",
        "        '''\n",
        "        self.__validate_inputs(p=p, q=q, lam=lam) \n",
        "        self.p = p\n",
        "        self.q = q\n",
        "        self.activation = activation \n",
        "        self.activation_inverse = activation_inverse \n",
        "        self.lam = lam \n",
        "        self.xTy = torch.zeros(p,q) \n",
        "        self.yTx = torch.zeros(p,q) \n",
        "        self.xTx_inv = torch.diag(torch.tensor([lam]*p)) \n",
        "        self.yTy_inv = torch.diag(torch.tensor([lam]*q)) \n",
        "        self.betaT_forward = torch.matmul(self.xTx_inv, self.xTy)\n",
        "        self.betaT_forward = torch.transpose(self.betaT_forward, 0, 1) \n",
        "        self.betaT_backward = torch.matmul(self.yTy_inv, self.yTx) \n",
        "        self.betaT_backward = torch.transpose(self.betaT_backward, 0, 1) \n",
        "        self.x_forward = None \n",
        "        self.y_forward = None \n",
        "        self.x_backward = None \n",
        "        self.y_backward = None \n",
        "        pass \n",
        "    def forward_predict(self, x): \n",
        "        'creates and stores x_forward and y_forward, then returns activation(y_forward)'\n",
        "        self.__validate_inputs(x=x, p=self.p)\n",
        "        ## TODO prepend 1 for intercept \n",
        "        self.x_forward = x \n",
        "        self.y_forward = torch.matmul(self.betaT_forward, x) \n",
        "        return self.activation(self.y_forward) \n",
        "    def backward_predict(self, y):\n",
        "        'creates and stores x_backward and y_backward, then returns y_backward'\n",
        "        y = self.activation_inverse(y) \n",
        "        self.__validate_inputs(y=y, q=self.q)\n",
        "        self.y_backward = y\n",
        "        self.x_backward = torch.matmul(self.betaT_backward, y) \n",
        "        return self.x_backward \n",
        "    def forward_fit(self): \n",
        "        'uses x_forward and y_backward to update forward model, then returns Sherman Morrison denominator'\n",
        "        self.__validate_inputs(x=self.x_forward, y=self.y_backward) \n",
        "        self.xTx_inv, sm_denom = self.__sherman_morrison(self.xTx_inv, self.x_forward, self.x_forward) \n",
        "        self.xTy += torch.matmul(self.x_forward, torch.transpose(self.y_backward, 0, 1)) \n",
        "        self.betaT_forward = torch.matmul(self.xTx_inv, self.xTy) \n",
        "        self.betaT_forward = torch.transpose(self.betaT_forward, 0, 1) \n",
        "        return sm_denom \n",
        "    def backward_fit(self):\n",
        "        'uses x_backward and y_forward to update backward model, then returns Sherman Morrison denominator'\n",
        "        self.yTy_inv, sm_denom = self.__sherman_morrison(self.yTy_inv, self.y_forward, self.y_forward) \n",
        "        self.yTx += torch.matmul(self.y_forward, torch.transpose(self.x_backward, 0, 1)) \n",
        "        self.betaT_forward = torch.matmul(self.yTy_inv, self.yTx)\n",
        "        self.betaT_forward = torch.transpose(self.betaT_forward, 0, 1) \n",
        "        return sm_denom \n",
        "    def __sherman_morrison(self, inv_mat, vec1, vec2):\n",
        "        '''\n",
        "        applies Sherman Morrison updates, (mat + vec1 vec2^T)^{-1}\n",
        "        inputs:\n",
        "        - inv_mat: an inverted matrix \n",
        "        - vec1: a column vector \n",
        "        - vec2: a column vector \n",
        "        returns:\n",
        "        - updated matrix\n",
        "        - the Sherman Morrison denominator, for tracking numerical stability \n",
        "        '''\n",
        "        v2t = torch.transpose(vec2, 0, 1)\n",
        "        denominator = 1. + torch.matmul(torch.matmul(v2t, inv_mat), vec1) \n",
        "        numerator = torch.matmul(torch.matmul(inv_mat, vec1), torch.matmul(v2t, inv_mat)) \n",
        "        updated_inv_mat = inv_mat - numerator / denominator \n",
        "        return updated_inv_mat, float(denominator) \n",
        "    def __validate_inputs(self, p=None, q=None, lam=None, x=None, y=None):\n",
        "        'raises value exceptions if provided parameters are invalid'\n",
        "        if q is not None:\n",
        "            if not isinstance(q, int):\n",
        "                raise ValueError('`q` must be int!')\n",
        "            if q <= 0:\n",
        "                raise ValueError('`q` must be greater than zero!')\n",
        "        if p is not None:\n",
        "            if not isinstance(p, int): \n",
        "                raise ValueError('`p` must be int!')\n",
        "            if p <= 0: \n",
        "                raise ValueError('`p` must be greater than zero!')\n",
        "        if lam is not None:\n",
        "            if not (isinstance(lam, float) or isinstance(lam, int)):\n",
        "                raise ValueError('`lam` must be float or int!')\n",
        "            if lam < 0:\n",
        "                raise ValueError('`lam` must be non-negative!')\n",
        "        if x is not None and p is not None: \n",
        "            if type(x) != torch.tensor:\n",
        "                raise ValueError('`x` must be of type `torch.tensor`!') \n",
        "            if list(x.shape) != [p,1]: \n",
        "                raise ValueError('`x.shape` must be `[p,1]`') \n",
        "            pass \n",
        "        if y is not None and q is not None: \n",
        "            if type(y) != torch.tensor:\n",
        "                raise ValueError('`y` must be of type `torch.tensor`!') \n",
        "            if list(y.shape) != [q,1]: \n",
        "                raise ValueError('`y.shape` must be `[q,1]`') \n",
        "            pass \n",
        "        pass \n",
        "    pass\n",
        "\n",
        "class OnlineNet: \n",
        "    'online, sequential dense net' \n",
        "    def __init__(self, layer_list): \n",
        "        ## validate inputs \n",
        "        if type(layer_list) != list: \n",
        "            raise ValueError('`layer_list` must be of type list!') \n",
        "        for layer in layer_list: \n",
        "            if not issubclass(type(layer), OnlineDenseLayer):\n",
        "                raise ValueError('each item in `layer_list` must be an instance of a subclass of `OnlineDenseLayer`!') \n",
        "        ## assign \n",
        "        self.layer_list = layer_list \n",
        "        pass \n",
        "    def forward(self, x): \n",
        "        'predict forward'\n",
        "        for layer in self.layer_list:\n",
        "            x = layer.forward(x) \n",
        "        return x \n",
        "    def backward(self, y):\n",
        "        'predict backward'\n",
        "        for layer in reversed(self.layer_list): \n",
        "            y = layer.backward(y) \n",
        "        return y \n",
        "    def fit(self): \n",
        "        'assumes layers x & y targets have already been set. Returns Sherman Morrison denominators per layer in (forward, backward) pairs in a list'\n",
        "        sherman_morrison_denominator_list = [] \n",
        "        for layer in self.layer_list:\n",
        "            forward_smd = layer.forward_fit() \n",
        "            backward_smd = layer.backward_fit() \n",
        "            sherman_morrison_denominator_list.append((forward_smd, backward_smd))\n",
        "        return sherman_morrison_denominator_list \n",
        "    def __reduce_sherman_morrison_denominator_list(self, smd_pair_list):\n",
        "        'returns the value closest to zero'\n",
        "        if type(smd_pair_list) != list: \n",
        "            raise ValueError('`smd_pair_list` must be of type `list`!')\n",
        "        if len(smd_pair_list) == 0:\n",
        "            return None \n",
        "        smallest_smd = None \n",
        "        for smd_pair in smd_pair_list:\n",
        "            if type(smd_pair) != tuple:\n",
        "                raise ValueError('`smd_pair_list` must be list of tuples!')\n",
        "            if smallest_smd is None: \n",
        "                smallest_smd = smd_pair[0] \n",
        "            if abs(smallest_smd) > abs(smd_pair[0]): \n",
        "                smallest_smd = smd_pair[0] \n",
        "            if abs(smallest_smd) > abs(smd_pair[1]):\n",
        "                smallest_smd = smd_pair[1] \n",
        "        return smallest_smd \n",
        "    def __call__(self, x, y=None): \n",
        "        '''\n",
        "        If only x is given, a prediction is made and returned.\n",
        "        If x and y are given, then the model is updated, and returns\n",
        "        - the prediction\n",
        "        - the sherman morrison denominator closest to zero, for tracking numerical stability\n",
        "        '''\n",
        "        y_hat = self.forward(x) \n",
        "        if y is None: \n",
        "            return y_hat \n",
        "        self.backward(y) \n",
        "        self.x_forward = x \n",
        "        self.x_backward = x \n",
        "        self.y_forward = y \n",
        "        self.y_backward = y \n",
        "        smd_pair_list = self.fit() \n",
        "        smallest_smd = self.__reduce_sherman_morrison_denominator_list(smd_pair_list) \n",
        "        return y_hat, smallest_smd "
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1637875502258
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# first experiment: mnist classification"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\r\n",
        "from torchvision import datasets, transforms\r\n",
        "\r\n",
        "transform=transforms.Compose([\r\n",
        "    transforms.ToTensor(),\r\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\r\n",
        "    ])\r\n",
        "\r\n",
        "dataset1 = datasets.MNIST('../../data', train=True, download=True, transform=transform)\r\n",
        "dataset2 = datasets.MNIST('../../data', train=False, transform=transform)\r\n",
        "\r\n",
        "train_loader = torch.utils.data.DataLoader(dataset1)\r\n",
        "test_loader = torch.utils.data.DataLoader(dataset2)\r\n",
        "\r\n",
        "d = 1*1*28*28 + 1 + 10 ## data dim, +1 for intercept, +10 for one-hot encoding \r\n",
        "l = 100 \r\n",
        "p = d+l \r\n",
        "iters = 10 ## number of recurrent net iterations to run per classification \r\n",
        "n_labels = 10 \r\n",
        "regularizer = 1000\r\n",
        "\r\n",
        "model = OnlineNet(\r\n",
        "    [OnlineDenseLayer(p=1*1*28*28, q=1000), ## TODO add activation functions \r\n",
        "    OnlineDenseLayer(p=1000, q=5000), \r\n",
        "    OnlineDenseLayer(p=5000, q=100), \r\n",
        "    OnlineDenseLayer(p=100, q=n_labels)] \r\n",
        ")\r\n",
        "\r\n",
        "def build_data(image, label, latent_vec): \r\n",
        "    'format data from iterator for model' \r\n",
        "    y = torch.tensor([1. if int(label[0]) == idx else 0. for idx in range(n_labels)]) ## one-hot representation \r\n",
        "    x = image.reshape([-1]) ## flatten \r\n",
        "    ## shrink so sigmoid inverse is well-defined \r\n",
        "    y = y*.90 + .05 \r\n",
        "    return x, y \r\n",
        "\r\n",
        "errors = [] \r\n",
        "lat_sums = []\r\n",
        "pbar = tqdm(train_loader)\r\n",
        "for [image, label] in pbar:\r\n",
        "    x, y = build_data(image, label, latent_vec) \r\n",
        "    x0 = x \r\n",
        "    ## fit \r\n",
        "    for _ in range(iters): \r\n",
        "        y_pred = model.predict(x) \r\n",
        "        y_pred[d:] = torch.sigmoid(y_pred[d:])\r\n",
        "        #print(f'DEBUG 1 y_pred.sum(): {y_pred.sum()}') \r\n",
        "        y_target = model.build_y(x, y_pred, y[:d]) \r\n",
        "        ## update labels before fitting \r\n",
        "        y_target[:n_labels] = y[:n_labels] \r\n",
        "        model.fit(x, y_target) \r\n",
        "        ## recurse \r\n",
        "        x = y_pred \r\n",
        "    ## train error \r\n",
        "    for _ in range(iters): \r\n",
        "        x0 = model.predict(x0) \r\n",
        "    error = (x0[:n_labels] - y[:n_labels]).abs().sum() \r\n",
        "    errors.append(error) \r\n",
        "    lat_sum = latent_vec.abs().sum()\r\n",
        "    lat_sums.append(lat_sum)\r\n",
        "    pbar.set_description(f'error: {error}, {lat_sum}')\r\n",
        "    ## keep latent vec for next iteration \r\n",
        "    latent_vec = torch.sigmoid(x0[(p-l):])"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (1000x1000 and 784x1000)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-01f1b150cb79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m model = OnlineNet(\n\u001b[0;32m---> 23\u001b[0;31m     [OnlineDenseLayer(p=1*1*28*28, q=1000), ## TODO add activation functions \n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mOnlineDenseLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mOnlineDenseLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-42d38baf4705>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, p, q, activation, activation_inverse, lam)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbetaT_forward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxTx_inv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxTy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbetaT_forward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbetaT_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbetaT_backward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myTy_inv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myTx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbetaT_backward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbetaT_backward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_forward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1000x1000 and 784x1000)"
          ]
        }
      ],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([[1.,2.], [3., 5.], [6., 7.]])\r\n",
        "list(x.shape) == [3,2]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1637875351529
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## scratch space\r\n",
        "\r\n",
        "$x_1, X_2, X_3, \\ldots, X_{p-1}, x_p = y$\r\n",
        "\r\n",
        "$\\beta_1^F, \\beta_1^B, \\beta_2^F, \\beta_2^B, \\ldots, \\beta_{p-1}^F, \\beta_{p-1}^B$\r\n",
        "\r\n",
        "$\\hat x_{j+1} = \\sigma \\left( \\beta_j^{FT} x_j \\right)$\r\n",
        "\r\n",
        "forward series: $x_1, \\hat x_2, \\hat x_3, \\ldots, \\hat x_{p-1}, x_p = y$\r\n",
        "\r\n",
        "$ \\hat \\beta_{p-1}^{FT} = \\text{argmin}_\\beta \\| x_p - \\beta^{T} \\hat x_{p-1} \\|^2 $\r\n",
        "\r\n",
        "$ \\hat \\beta_{p-2}^{FT} = \\text{argmin}_\\beta \\| \\hat x_{p-1} - \\beta^{T} \\hat x_{p-2} \\|^2 $ We won't do this. \r\n",
        "\r\n",
        "$ \\tilde x_{j-1} = \\sigma^{-1}\\left( \\beta_j^{BT} x_j \\right)$\r\n",
        "\r\n",
        "backward series: $x_1, \\tilde x_2, \\tilde x_3, \\ldots, \\tilde x_{p-1}, x_p = y$\r\n",
        "\r\n",
        "$ \\hat \\beta_{p-2}^{F} = \\text{argmin}_\\beta \\| \\tilde x_{p-1} - \\beta^{T} \\hat x_{p-2} \\|^2 $\r\n",
        "\r\n",
        "$ \\hat \\beta_{p-2}^{B} = \\text{argmin}_\\beta \\| \\hat x_{p-2} - \\beta^{T} \\tilde x_{p-1} \\|^2 $"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ \\hat x_3 = \\sigma \\left( \\beta_2^{FT} \\hat x_2 \\right) $\r\n",
        "\r\n",
        "$ \\hat \\beta_2^F = \\text{argmin}_\\beta \\| \\sigma^{-1}\\left( \\hat x_3 \\right) - \\beta^T \\hat x_2 \\|^2 $ Useless without $\\tilde x$\r\n",
        "\r\n",
        "$ \\tilde x_2 = \\sigma^{-1}\\left( \\beta_3^{BT} \\tilde x_3 \\right) $\r\n",
        "\r\n",
        "$ \\hat \\beta_2^B = \\text{argmin}_\\beta \\| \\sigma\\left( \\hat x_2 \\right) - \\beta^T \\hat x_3 \\|^2 $ Useless without $\\hat x$\r\n",
        "\r\n",
        "So, use these estimates instead.\r\n",
        "\r\n",
        "$ \\hat \\beta_2^F = \\text{argmin}_\\beta \\| \\sigma^{-1}\\left( \\tilde x_3 \\right) - \\beta^T \\hat x_2 \\|^2 $\r\n",
        "\r\n",
        "$ \\hat \\beta_2^B = \\text{argmin}_\\beta \\| \\sigma\\left( \\hat x_2 \\right) - \\beta^T \\tilde x_3 \\|^2 $"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\pi$"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}