{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Regularizers as memory\n",
        "\n",
        "The greatest challenge in modern AI research is the limiting returns computational work. \n",
        "While the greatest advances can only be afforded by large technology firms, even they \n",
        "cannot afford to push results further. A clear plateau has developed. \n",
        "Insufficient computational efficiency motivates a return to theory, begs a question: \n",
        "_where is waste most significant?_ \n",
        "\n",
        "The advent of catestrophic forgetting shows raw, non-compressed information must be continually re-applied \n",
        "if it is not to be forgotten. If important information could be preserved--even partially--we would expect \n",
        "more efficient computation. In short, _this work targets the realization of memory_.\n",
        "\n",
        "Regularized likelihood equations have a Lagrangian form, so implicitly describe geometric constraints on estimates.\n",
        "For example, here's an estimate constrained to an L2-sphere in $\\Theta$-space.\n",
        "\n",
        "$$ \\hat \\theta_{L_2} = \\text{arg max}_{\\theta \\in \\Theta} n^{-1} \\sum_{i=1}^n \\log f_X(X_i;\\theta) - \\lambda \\| \\theta \\|_2^2 $$\n",
        "\n",
        "In this work, we'll generalize the regularizer $\\| \\cdot \\|_2^2$ to support alternative geometries, \n",
        "in attempting to construct numerically convenient memory approximations. \n",
        "Particulary, we'll seek to approximate the following equation.\n",
        "Note that it introduces quadratic geometric constraints on the estimate. \n",
        "\n",
        "$$ \\hat \\theta = \\text{arg max}_{\\theta \\in \\Theta} n^{-1} \\sum_{i=1}^n \\log f_X(X_i;\\theta) - \\frac{\\lambda}{2} (\\theta - \\theta_0)^T \\mathcal{I}_{\\theta_0} (\\theta - \\theta_0) $$"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RL-reweighted (RLR) estimates \n",
        "\n",
        "This first estimate is designed to improve efficiency of sampling in reinforcement learning (RL) by \n",
        "up-weighting more-important observations. We'll use this estimate:\n",
        "\n",
        "$$ \\hat \\theta_{RLR} = \\hat \\theta_{n_1+n_2} = \\text{arg max}_\\theta \\left( \\sum_{i=1}^{n_1+n_2} \\| \\hat r_i \\|_2^{2b} \\right)^{-1} \\left( \\sum_{i=n_1+1}^{n_1+n_2} \\| \\hat r_i \\|_2^{2b} \\log f_X(X_i; \\theta) - \\sum_{i=1}^{n_1} \\frac{\\lambda}{2} (\\theta - \\hat \\theta_{n_1+n_2-1})^T H_i (\\theta - \\hat \\theta_{n_1+n_2-1}) \\right) $$\n",
        "\n",
        "where \n",
        "- $n_1$ is the number of priviously observed sameples, \n",
        "- $n_2$ is the number of newly observed samples, \n",
        "- $\\hat r_i$ is the estimated reward for observation $i$, \n",
        "- $b > 0$ is a bias term,  \n",
        "- $H_i = \\| \\hat r_i \\|^{2b}_2 \\left( \\nabla_\\theta \\log f_X(X_j; \\theta)|_{\\theta = \\hat \\theta_{j-1}} \\right) \\left( \\nabla_\\theta \\log f_X(X_j; \\theta)|_{\\theta = \\hat \\theta_{j-1}} \\right)^T$ is a Hessian approximation, and\n",
        "- $\\hat \\theta_0$ is an initial estimate guess, commonly required in optimization routines. \n",
        "\n",
        "Define $ M:= \\sum_{i=1}^{n_1}(\\theta - \\theta_{n_1+n_2-1})^TH_i(\\theta - \\theta_{n_1+n_2-1})$, which we'll refer to as the \"memory term\". \n",
        "The key heuristic is that $M$, a quadratic regularizer, approximates large-sample likelihoods under maximization. \n",
        "This insight can be seen through a Taylor approximation near true $\\theta$, designated $\\theta_T$.\n",
        "\n",
        "$\\hat \\theta_{MLE} = \\text{arg max}_\\theta n^{-1} \\sum_{i=1}^n \\log f_X(X_i;\\theta) $\n",
        "\n",
        "$ \\approx \\text{arg max}_\\theta n^{-1}\\sum_i \\log f_X(X_i; \\theta_T) + n^{-1}\\sum_i (\\theta - \\theta_T)^T \\nabla_\\theta \\log f_X(X_i; \\theta_T) + n^{-1}2^{-1} \\sum_i (\\theta - \\theta_T)^T (\\nabla_\\theta^2 \\log f_X(X_i; \\theta_T))(\\theta - \\theta_T) $\n",
        "\n",
        "$ \\approx \\text{arg max}_\\theta n^{-1}\\sum_i \\log f_X(X_i; \\theta_T) + 0 + n^{-1}2^{-1} \\sum_i (\\theta - \\theta_T)^T (\\nabla_\\theta^2 \\log f_X(X_i; \\theta_T))(\\theta - \\theta_T) $\n",
        "\n",
        "$ \\approx \\text{arg max}_\\theta n^{-1}2^{-1} \\sum_i (\\theta - \\theta_T)^T (\\nabla_\\theta^2 \\log f_X(X_i; \\theta_T))(\\theta - \\theta_T) $\n",
        "\n",
        "This work basically tests the quality of our $M \\approx \\sum_i (\\theta - \\theta_T)^T (\\nabla_\\theta)^2 \\log f_X(X_i; \\theta_T)(\\theta - \\theta_T) $ heuristic.\n",
        "If true, then we should see that the quadratic regularizer, $M$, affects memory through geometric constraints on the estimator.\n",
        "\n",
        "Later, we'll experiment with recency bias and rank reductions for $M$.\n",
        "\n",
        "Notice that $\\hat \\theta_{RLR}$ is asymptotically equivalent to a redistributed sampling, \n",
        "with more observations when they are more important. \n",
        "Hence, the essence of this computational efficiency strategy is to \n",
        "- add frequency bias to important observations, \n",
        "- and preserve their information via geometric estimator constraints."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $M$ and almost-stationary processes\r\n",
        "\r\n",
        "$M$ is an asymptotic result, so only applies when observations are sufficiently many and produced by a stationary or i.i.d process.\r\n",
        "While most applied asymptotic theory assumes $\\theta_T$ is static, it could also be possible to work with a $\\theta_{T_n}$ on a continuous path in $\\Theta$ over $n$.\r\n",
        "Let us define an _almost-stationary process_ $X_n$ as satisfying the following.\r\n",
        "\r\n",
        "$$ \\forall \\tau \\in \\mathbb{R}, \\left(t_1, t_2, \\ldots, t_m\\right) \\in \\mathbb{R}_{\\geq 0}^m, \\mathbb{P}\\left[ X_{t_{1+\\tau}}, X_{t_{2+\\tau}}, \\ldots, X_{t_{m+\\tau}} ; \\theta_{t_1+\\tau} \\right] \\text{is continuous in } \\tau$$\r\n",
        "\r\n",
        "The question of whether $\\theta_{T_n}$ moves \"too quickly\" is dependent on an given process. \r\n",
        "If, for $n$ large, $M$ still approximates $\\sum_i (\\theta - \\theta_{T_n})^T (\\nabla_\\theta^2 \\log f_X(X_i; \\theta_{T_n}))(\\theta - \\theta_{T_n})$, \r\n",
        "then we'll say $\\theta_{T_n}$ is \"sufficiently slow\". Here are some examples relating to **transfer learning**.\r\n",
        "- If we withheld digits 1 & 2 from MNIST for the first 10000 observations, then started including them afterward, \r\n",
        "we should expect $\\theta_{T_n}$ to move too quickly. \r\n",
        "- As reinforcement learning (RL) agent explores its world, it's sampling process indeed deforms, albeit slowly. \r\n",
        "This context should have good opportunities for sufficiently slow changes in $\\theta_{T_n}$. \r\n",
        "So, our experiments will focus on RL applications.\r\n",
        "\r\n",
        "Developing transfer learning mechanisms for RL is particularly valuable, \r\n",
        "since it is frequently imagined to be a path to artificial general intelligence (AGI).\r\n",
        "\r\n",
        "Side node: almost-stationary process theory clearly needs development. \r\n",
        "My experience has taught me to test the value of theoretical hypotheses before developing them, \r\n",
        "since theoretical development is far more expensive than computational experimentation.\r\n",
        "I am confident in this hypothesis, because my previous theoretical developments are very similar. \r\n",
        "Any new proofs would not be very novel, and would be mere adaptations of prior work. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 1: Cartpole\r\n",
        "\r\n",
        "We start with a _very_ simple example, proving-out concepts."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Define model \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random \n",
        "\n",
        "INPUT_DIM = 4\n",
        "N_ACTIONS = 2\n",
        "MAX_SAMPLE = 5000\n",
        "DISCOUNT = .95 \n",
        "EPS = 1e-5\n",
        "EXPLORE_PROBABILITY_FUNC = lambda idx: 0.975**idx \n",
        "BATCH_SIZE = 100 \n",
        "LEARNING_RATE = 0.0001 \n",
        "GRAD_CLIP = 10.0 \n",
        "SHORT_TERM_MEMORY_LENGTH = 5 \n",
        "LBFGS = False  \n",
        "\n",
        "class Model(nn.Module): \n",
        "    def __init__(self, \n",
        "            input_dim=INPUT_DIM, \n",
        "            n_actions=N_ACTIONS, \n",
        "            discount = DISCOUNT, \n",
        "            max_sample=MAX_SAMPLE, \n",
        "            short_term_memory_length=SHORT_TERM_MEMORY_LENGTH, \n",
        "            eps=EPS): \n",
        "        super(Model, self).__init__() \n",
        "        ## store config \n",
        "        self.input_dim = input_dim \n",
        "        self.n_actions = n_actions \n",
        "        self.discount = discount \n",
        "        self.max_sample = max_sample \n",
        "        self.short_term_memory_length = short_term_memory_length \n",
        "        self.eps = eps \n",
        "        ## init feed forward net \n",
        "        self.fc1 = nn.Linear(input_dim * short_term_memory_length, 256) \n",
        "        self.fc1_bn = nn.BatchNorm1d(256) \n",
        "        self.fc2 = nn.Linear(256, 256) \n",
        "        self.fc2_bn = nn.BatchNorm1d(256) \n",
        "        self.fc3 = nn.Linear(256, n_actions) \n",
        "        ## init data structures \n",
        "        self.observations = [] \n",
        "        if LBFGS: \n",
        "            self.optimizer = optim.LBFGS(self.parameters(), history_size=5) ## RAM req = O(history_size * model_dim) \n",
        "        else: \n",
        "            ## LBFGS was giving nan parameters \n",
        "            self.optimizer = optim.Adam(self.parameters(), lr=LEARNING_RATE) \n",
        "            pass \n",
        "        pass \n",
        "    \n",
        "    def copy(self): \n",
        "        out = Model(input_dim=self.input_dim, \\\n",
        "                n_actions=self.n_actions, \\\n",
        "                discount=self.discount, \\\n",
        "                max_sample=self.max_sample, \\\n",
        "                eps=self.eps) \n",
        "        out.load_state_dict(self.state_dict()) \n",
        "        return out \n",
        "\n",
        "    def forward(self, x): \n",
        "        x = self.fc1(x)\n",
        "        x = self.fc1_bn(x) \n",
        "        x = torch.relu(x) \n",
        "        x = self.fc2(x) \n",
        "        x = self.fc2_bn(x)  \n",
        "        x = torch.relu(x) \n",
        "        x = self.fc3(x) \n",
        "        #x = x*x \n",
        "        return x \n",
        "    \n",
        "    def get_action(self, env_state): \n",
        "        env_state = torch.tensor(env_state).float() \n",
        "        env_state = env_state.reshape([1, -1]) \n",
        "        predicted_reward_per_action_idx = self.forward(env_state) \n",
        "        return int(predicted_reward_per_action_idx.argmax()) \n",
        "    \n",
        "    def store_observation(self, observation): \n",
        "        if len(observation) > self.max_sample: \n",
        "            observation = observation[1:] \n",
        "        self.observations.append(observation) \n",
        "        pass \n",
        "    \n",
        "    def clear_observations(self): \n",
        "        self.observations = [] \n",
        "        pass \n",
        "    \n",
        "    def __memory_replay(self, target_model, batch_size=None): \n",
        "        ## random sample \n",
        "        obs = self.observations \n",
        "        if batch_size is not None: \n",
        "            if batch_size < len(self.observations): \n",
        "                obs = random.sample(self.observations, batch_size) \n",
        "        ## unpack samples \n",
        "        samples = [ (torch.tensor(env_state).float(), \\\n",
        "                torch.tensor(reward).float(), \\\n",
        "                torch.tensor(done).int(), \\\n",
        "                torch.tensor(prev_env_state).float(), \\\n",
        "                torch.tensor(action).int()) for \\\n",
        "                (env_state, reward, done, info, prev_env_state, action) in obs] \n",
        "        ## build matrices \n",
        "        env_state = torch.stack([obs[0] for obs in samples], dim=0) ## inserts dim 0 \n",
        "        observed_rewards = torch.stack([obs[1] for obs in samples], dim=0) \n",
        "        done = torch.stack([obs[2] for obs in samples], dim=0) \n",
        "        prev_env_state = torch.stack([obs[3] for obs in samples], dim=0) \n",
        "        action = torch.stack([obs[4] for obs in samples], dim=0).reshape([-1, 1]).type(torch.int64) \n",
        "        ## calculate target \n",
        "        with torch.no_grad(): \n",
        "            predicted_rewards = target_model.forward(env_state) \n",
        "            predicted_rewards = torch.max(predicted_rewards, dim=1, keepdim=True).values.reshape([-1]) \n",
        "            target = observed_rewards + (1 - done) * self.discount * predicted_rewards \n",
        "            target = target.reshape([-1, 1]).detach() \n",
        "            pass \n",
        "        ## calculate prediction \n",
        "        self.zero_grad() \n",
        "        self.train() \n",
        "        predicted_rewards = self.forward(prev_env_state) \n",
        "        prediction = predicted_rewards.gather(1, action) \n",
        "        return prediction, target \n",
        "    \n",
        "    def get_parameter_vector(self): \n",
        "        return nn.utils.parameters_to_vector(self.parameters()) \n",
        "    \n",
        "    def optimize(self, max_iter=None, batch_size=None): \n",
        "        iter_n = 0 \n",
        "        n_dels = 30 \n",
        "        dels = [None]*n_dels \n",
        "        continue_iterating = True \n",
        "        halt_method = None \n",
        "        loss_f = None \n",
        "        mean_reward = None \n",
        "        target_model = self.copy() \n",
        "        while continue_iterating: \n",
        "            prev_theta = self.get_parameter_vector() \n",
        "            predicted, target = self.__memory_replay(target_model=target_model, batch_size=batch_size) \n",
        "            mean_reward = predicted.mean() \n",
        "            #loss = F.mse_loss(predicted, target) \n",
        "            loss = F.smooth_l1_loss(predicted, target) \n",
        "            loss_f = float(loss) \n",
        "            loss.backward() \n",
        "            if not LBFGS: \n",
        "                ## lbfgs really doesn't like this \n",
        "                nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP) \n",
        "            ## lbfgs must re-evaluate target, hence lambda \n",
        "            if LBFGS:\n",
        "                self.optimizer.step(lambda: float(F.smooth_l1_loss(predicted, target))) \n",
        "            else:\n",
        "                self.optimizer.step() \n",
        "                pass \n",
        "            updated_theta = self.get_parameter_vector() \n",
        "            ## decide to continue iterating or not \n",
        "            if max_iter is not None: \n",
        "                if iter_n > max_iter: \n",
        "                    ## halt: iters have hit limit \n",
        "                    continue_iterating = False \n",
        "                    halt_method = 'max-iter' \n",
        "            if iter_n >= n_dels: \n",
        "                ## test convergence with chebyshev ineq \n",
        "                dels = dels[1:] + [(updated_theta - prev_theta).abs().sum()] \n",
        "                sigma = torch.tensor(dels).square().mean().sqrt() \n",
        "                if (sigma/self.eps)**2 < .95: \n",
        "                    ## halt: convergance \n",
        "                    continue_iterating = False \n",
        "                    halt_method = 'cauchy-convergence' \n",
        "            else: \n",
        "                ## collect data for variance estimate \n",
        "                dels[iter_n] = (updated_theta - prev_theta).abs().sum() \n",
        "                pass \n",
        "            iter_n += 1 \n",
        "            pass \n",
        "        return loss_f, halt_method, mean_reward  "
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1649942642645
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Define experiment \r\n",
        "import gym \r\n",
        "from tqdm import tqdm \r\n",
        "import numpy as np \r\n",
        "import matplotlib.pyplot as plt \r\n",
        "\r\n",
        "model = Model() \r\n",
        "\r\n",
        "env = gym.make('CartPole-v1') \r\n",
        "env_state = env.reset() \r\n",
        "env_state_list = [torch.tensor(env_state) for _ in range(SHORT_TERM_MEMORY_LENGTH)] \r\n",
        "env_state = torch.cat(env_state_list) \r\n",
        "last_start = 0 \r\n",
        "last_survival = 0 \r\n",
        "n_restarts = 0 \r\n",
        "total_reward = 0 \r\n",
        "survivals = [] \r\n",
        "total_iters = 10000 \r\n",
        "\r\n",
        "plt.plot([EXPLORE_PROBABILITY_FUNC(idx) for idx in range(total_iters)])\r\n",
        "plt.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAATQElEQVR4nO3da4ycZ3mH8eue2V0nscmJbJCxndoBA/UHAmEJSSk0lEIOrRq1QmpCaSAisqKSirZSSyLUVhUfKqBFKErAWDSl9EBoIQI3Mg0VtE0lCsRRIdgJhsVp4sWh3hASQgx21r774X13PTsz3h07s5l9xtdPWu28h5m5Hwf+evZ+T5GZSJLK1xh0AZKk/jDQJWlIGOiSNCQMdEkaEga6JA2JkUF98TnnnJPr168f1NdLUpHuu+++xzJzvNu2gQX6+vXr2bFjx6C+XpKKFBEPH2ubLRdJGhIGuiQNCQNdkoaEgS5JQ8JAl6QhsWigR8TtEbE/InYeY3tExC0RMRkR90fEhf0vU5K0mF5m6J8ALl9g+xXAxvpnM/DRZ1+WJOl4LRromXkP8PgCu1wFfDIrXwXOjIjV/Sqw3e4fPMWHvribx35ycKm+QpKK1I8e+hpgb8vyVL2uQ0RsjogdEbFjenr6hL5scv9PuOXLkzz+9KETer8kDat+BHp0Wdf1qRmZuTUzJzJzYny865Wri2rU33bEB3NI0jz9CPQpYF3L8lpgXx8+t6uIKtGPHFmqb5CkMvUj0LcB19Znu1wMPJmZj/bhc7tyhi5J3S16c66I+BRwKXBOREwBfwaMAmTmFmA7cCUwCRwArluqYgEa9QzdPJek+RYN9My8ZpHtCbyrbxUtolH/TeEMXZLmK+5K0bkeuoEuSfMUF+iNuUAfcCGStMwUGOjV73SGLknzFBjoztAlqZviAj08bVGSuiou0BseFJWkrooNdPNckuYrLtBtuUhSd8UF+tGzXAZbhyQtN8UFuhcWSVJ3xQW6PXRJ6q7AQK9+O0OXpPkKDHQvLJKkbooLdM9ykaTuigv0oz10A12SWhUb6LZcJGm+AgO9+m3LRZLmKy7Qwxm6JHVVXKB7P3RJ6q7AQPdKUUnqptxAPzLgQiRpmSku0D0PXZK6KzbQjXNJmq+4QPfCIknqrthA97RFSZqvwECvfttDl6T5igt0LyySpO6KC3QvLJKk7goM9Nnz0A10SWpVbqCb55I0T3GBHnXFHhSVpPl6CvSIuDwidkfEZETc1GX7GRHxLxHxzYjYFRHX9b/Uig+JlqTuFg30iGgCtwFXAJuAayJiU9tu7wIeyMwLgEuBv4qIsT7XCnjaoiQdSy8z9IuAyczck5mHgDuAq9r2SeB5UZ1TuAp4HJjpa6U1e+iS1F0vgb4G2NuyPFWva3Ur8PPAPuBbwLszs+N+iBGxOSJ2RMSO6enpEyrYm3NJUne9BHp0WdeeppcB3wBeCLwCuDUiTu94U+bWzJzIzInx8fHjLrYqxnu5SFI3vQT6FLCuZXkt1Uy81XXAnVmZBB4CXtafEuc72kNfik+XpHL1Euj3AhsjYkN9oPNqYFvbPo8AbwSIiBcALwX29LPQWZ7lIkndjSy2Q2bORMSNwN1AE7g9M3dFxA319i3A+4BPRMS3qFo078nMx5aiYHvoktTdooEOkJnbge1t67a0vN4HvLm/pXUXEUTYQ5ekdsVdKQpV28UeuiTNV2ig23KRpHZFBno4Q5ekDkUGesMeuiR1KDTQw5aLJLUpONAHXYUkLS9FBnp4UFSSOhQZ6I0IrxSVpDaFBrozdElqV2SghwdFJalDkYFezdAHXYUkLS9FBnrYQ5ekDkUGuhcWSVKnQgPdHroktSs40AddhSQtL0UGuhcWSVKnIgPdC4skqVOhge4MXZLaFRro9tAlqV2RgW4PXZI6FRnoVQ/dQJekVsUG+pEjg65CkpaXIgPdloskdSo00D0oKkntigx07+UiSZ0KDfTAOJek+QoNdDhsz0WS5ikz0BvebVGS2hUZ6M0IZ+iS1KbIQG80DHRJatdToEfE5RGxOyImI+KmY+xzaUR8IyJ2RcR/9rfM+Zo+4EKSOowstkNENIHbgDcBU8C9EbEtMx9o2edM4CPA5Zn5SEScu1QFAzQbwcEZA12SWvUyQ78ImMzMPZl5CLgDuKptn7cCd2bmIwCZub+/Zc7XaASHzXNJmqeXQF8D7G1ZnqrXtXoJcFZE/EdE3BcR13b7oIjYHBE7ImLH9PT0iVUMNAOO2EOXpHl6CfTosq49TUeAVwG/ClwG/ElEvKTjTZlbM3MiMyfGx8ePu9hZTQ+KSlKHRXvoVDPydS3La4F9XfZ5LDOfBp6OiHuAC4Dv9KXKNg0PikpSh15m6PcCGyNiQ0SMAVcD29r2+TzwuogYiYjTgNcAD/a31KOcoUtSp0Vn6Jk5ExE3AncDTeD2zNwVETfU27dk5oMR8a/A/cAR4OOZuXOpijbQJalTLy0XMnM7sL1t3Za25Q8CH+xfacfWbASHbblI0jxFXinqpf+S1KnIQG80wtMWJalNkYHeDFsuktSuyECvbs416CokaXkpMtCbDR8SLUntygx0D4pKUociA92DopLUqchA96CoJHUqM9C9UlSSOhQZ6D4kWpI6FRnoHhSVpE5FBno1Q4d0li5Jc4oM9JFG9cwNZ+mSdFSRgd6cDXRn6JI0p8hAb0QV6Ee8/F+S5hQZ6M26amfoknRUkYE+O0O3hy5JRxUZ6LM9dC//l6Sjig50Wy6SdFSRgX70oKiBLkmzigx0Z+iS1KnMQPegqCR1KDLQGw3PQ5ekdkUGuuehS1KnIgPd89AlqVORgT7SqMo20CXpqCIDfa7lYqBL0pwiA33uPHR76JI0p8hAb3o/dEnqUGSgN7ywSJI6FBnoTS/9l6QOPQV6RFweEbsjYjIiblpgv1dHxOGIeEv/Suxky0WSOi0a6BHRBG4DrgA2AddExKZj7Pd+4O5+F9lu7jx0Wy6SNKeXGfpFwGRm7snMQ8AdwFVd9vs94LPA/j7W19VI0xm6JLXrJdDXAHtblqfqdXMiYg3wG8CWhT4oIjZHxI6I2DE9PX28tc6ZbbnMGOiSNKeXQI8u69qT9MPAezLz8EIflJlbM3MiMyfGx8d7rbHDaH2l6MxhA12SZo30sM8UsK5leS2wr22fCeCOqHrb5wBXRsRMZn6uL1W2mW25zBz2douSNKuXQL8X2BgRG4DvA1cDb23dITM3zL6OiE8Ady1VmAOM2HKRpA6LBnpmzkTEjVRnrzSB2zNzV0TcUG9fsG++FEbqm7nMeEN0SZrTywydzNwObG9b1zXIM/Mdz76shc3O0J+xhy5Jc4q8UnS06UFRSWpXZKDPHRS15SJJc4oMdE9blKRORQZ60xm6JHUoMtA9KCpJnYoMdA+KSlKnIgO92QgibLlIUqsiAx2qA6O2XCTpqGIDvdkIDjtDl6Q5xQb6SDOcoUtSi2IDfbTZsIcuSS2KDfSRRniWiyS1KDbQR5seFJWkVsUGugdFJWm+YgN9pBk84wMuJGlOsYE+2mj4CDpJalFsoI80PSgqSa0KDvSGLRdJalFsoI82wpaLJLUoNtCbjWDGGbokzSk20EebHhSVpFbFBvpI0xm6JLUqN9C9fa4kzVNsoK8YaXBo5vCgy5CkZaPoQD84Yw9dkmaVG+ijDQ4Z6JI0p9hAH2s6Q5ekVsUG+orRJgftoUvSnGIDfaxZtVwyPdNFkqDgQF8x0uBI4rnoklTrKdAj4vKI2B0RkxFxU5ftvx0R99c/X4mIC/pf6nwrRqvSPTAqSZVFAz0imsBtwBXAJuCaiNjUtttDwC9l5suB9wFb+11ou7FmVboHRiWp0ssM/SJgMjP3ZOYh4A7gqtYdMvMrmfmjevGrwNr+ltlpxWgTcIYuSbN6CfQ1wN6W5al63bG8E/hCtw0RsTkidkTEjunp6d6r7OLoDN0zXSQJegv06LKu65HIiHgDVaC/p9v2zNyamROZOTE+Pt57lV3M9tBtuUhSZaSHfaaAdS3La4F97TtFxMuBjwNXZOYP+1Pesc3O0G25SFKllxn6vcDGiNgQEWPA1cC21h0i4jzgTuB3MvM7/S+z02wP3ZaLJFUWnaFn5kxE3AjcDTSB2zNzV0TcUG/fAvwp8HzgIxEBMJOZE0tXdnUeOthykaRZvbRcyMztwPa2dVtaXl8PXN/f0hY2ZqBL0jxFXykKcPAZA12SYBgC3R66JAEFB/opswdFnaFLElBwoK8cq9r/Tx+aGXAlkrQ8FBvop62oZugHDtlykSQoONBXjDQZbQZPH3SGLklQcKADnDY24gxdkmpFB/rKsaYzdEmqFR3op61whi5Js4oO9JVjTc9ykaRa0YF+2tgIBw46Q5ckKDzQV65whi5Js4oOdM9ykaSjig70lSua/MSzXCQJKDzQTz91lCcPPENm1yfiSdJJpehAP+u0MQ4dPmLbRZIoPtBHAfjRgUMDrkSSBq/oQD/j1DEAnjjwzIArkaTBKzrQZ2foBroklR7oK6sZui0XSSo80M+cm6Eb6JJUdKCffdoYjYD9Tx0cdCmSNHBFB/pIs8ELTj+FfU/8bNClSNLAFR3oAC8881T2PfHTQZchSQM3HIH+pIEuSUMQ6Kfw6BM/4/ARL/+XdHIrPtBfNL6KQ4eP8PAPnx50KZI0UMUH+qbVpwPw4KNPDbgSSRqs4gP9xeeuotkIdu57ctClSNJAFR/op4w2eeW6M/mv704PuhRJGqjiAx3gDS87l53f/zF7Hz8w6FIkaWB6CvSIuDwidkfEZETc1GV7RMQt9fb7I+LC/pd6bL954RpGm8GtX558Lr9WkpaVRQM9IprAbcAVwCbgmojY1LbbFcDG+mcz8NE+17mg1WecynWv3cCnd+zlL7Y/yNSPDvgUI0knnZEe9rkImMzMPQARcQdwFfBAyz5XAZ/MKkW/GhFnRsTqzHy07xUfwx9d9lKeOHCIj92zh4/ds4dTR5usOmWEU0ebREBUtc9//VwVJ0ktfuvV67j+def3/XN7CfQ1wN6W5SngNT3sswaYF+gRsZlqBs955513vLUuaLTZ4ANvuYDNr38R//29x3j4hwd4+tAMPz10mAQyqX9n/doZvKTBOGfViiX53F4CvdtEtj0Ne9mHzNwKbAWYmJhYkkR98bmrePG5q5bioyVpWevloOgUsK5leS2w7wT2kSQtoV4C/V5gY0RsiIgx4GpgW9s+24Br67NdLgaefC7755KkHloumTkTETcCdwNN4PbM3BURN9TbtwDbgSuBSeAAcN3SlSxJ6qaXHjqZuZ0qtFvXbWl5ncC7+luaJOl4DMWVopIkA12ShoaBLklDwkCXpCERg7rnSURMAw+f4NvPAR7rYzklcMwnB8d8cng2Y/65zBzvtmFggf5sRMSOzJwYdB3PJcd8cnDMJ4elGrMtF0kaEga6JA2JUgN966ALGADHfHJwzCeHJRlzkT10SVKnUmfokqQ2BrokDYniAn2xB1aXIiLWRcS/R8SDEbErIt5drz87Iv4tIr5b/z6r5T031+PeHRGXtax/VUR8q952S0Qs66frRUQzIv4nIu6ql4d6zPUjGT8TEd+u/3tfchKM+Q/q/13vjIhPRcQpwzbmiLg9IvZHxM6WdX0bY0SsiIhP1+u/FhHrFy2qeiRbGT9Ut+/9HnA+MAZ8E9g06LpOcCyrgQvr188DvkP1EO4PADfV628C3l+/3lSPdwWwof53aNbbvg5cQvXkqC8AVwx6fIuM/Q+BfwTuqpeHeszA3wLX16/HgDOHecxUj598CDi1Xv4n4B3DNmbg9cCFwM6WdX0bI/C7wJb69dXApxetadD/KMf5D3gJcHfL8s3AzYOuq09j+zzwJmA3sLpetxrY3W2sVPenv6Te59st668BPjbo8SwwzrXAl4Bfbgn0oR0zcHodbtG2fpjHPPuM4bOpbtF9F/DmYRwzsL4t0Ps2xtl96tcjVFeWxkL1lNZyOdbDqItW/yn1SuBrwAuyftpT/fvcerdjjX1N/bp9/XL1YeCPgSMt64Z5zOcD08Df1G2mj0fESoZ4zJn5feAvgUeoHhT/ZGZ+kSEec4t+jnHuPZk5AzwJPH+hLy8t0Ht6GHVJImIV8Fng9zPzxwvt2mVdLrB+2YmIXwP2Z+Z9vb6ly7qixkw1s7oQ+GhmvhJ4mupP8WMpfsx13/gqqtbCC4GVEfG2hd7SZV1RY+7BiYzxuMdfWqAP1cOoI2KUKsz/ITPvrFf/X0SsrrevBvbX64819qn6dfv65ei1wK9HxP8CdwC/HBF/z3CPeQqYysyv1cufoQr4YR7zrwAPZeZ0Zj4D3An8AsM95ln9HOPceyJiBDgDeHyhLy8t0Ht5YHUR6iPZfw08mJkfatm0DXh7/frtVL312fVX10e+NwAbga/Xf9Y9FREX1595bct7lpXMvDkz12bmeqr/dl/OzLcx3GP+AbA3Il5ar3oj8ABDPGaqVsvFEXFaXesbgQcZ7jHP6ucYWz/rLVT/f1n4L5RBH1Q4gYMQV1KdEfI94L2DrudZjOMXqf58uh/4Rv1zJVWP7EvAd+vfZ7e85731uHfTcrQfmAB21ttuZZEDJ8vhB7iUowdFh3rMwCuAHfV/688BZ50EY/5z4Nt1vX9HdXbHUI0Z+BTVMYJnqGbT7+znGIFTgH8GJqnOhDl/sZq89F+ShkRpLRdJ0jEY6JI0JAx0SRoSBrokDQkDXZKGhIEuSUPCQJekIfH/lvAx5x5pyDAAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1649944826295
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## run experiment \n",
        "iters = tqdm(range(total_iters), disable=False) \n",
        "for iter_idx in iters: \n",
        "  prev_env_state = env_state \n",
        "  if EXPLORE_PROBABILITY_FUNC(iter_idx) < np.random.uniform(): \n",
        "    ## explore \n",
        "    action = env.action_space.sample() \n",
        "  else: \n",
        "    ## exploit \n",
        "    model.eval() \n",
        "    action = model.get_action(env_state) \n",
        "    pass \n",
        "  env_state, reward, done, info = env.step(action) \n",
        "  env_state_list = env_state_list[1:] + [torch.tensor(env_state)] \n",
        "  env_state = torch.cat(env_state_list) \n",
        "  if done: \n",
        "    total_reward = -1 \n",
        "    reward = 0 \n",
        "  else: \n",
        "    total_reward += reward \n",
        "    pass \n",
        "  observation = env_state, reward, done, info, prev_env_state, action \n",
        "  model.store_observation(observation) \n",
        "\n",
        "  if iter_idx > 30 and iter_idx % 1 == 0: \n",
        "    #loss, halt_method, mean_reward = model.optimize(max_iter=100, batch_size=BATCH_SIZE) \n",
        "    loss, halt_method, mean_reward = model.optimize(max_iter=10, batch_size=100) \n",
        "    loss = float(loss) \n",
        "    mean_reward = float(mean_reward) \n",
        "    param_nan = model.get_parameter_vector().isnan().sum() \n",
        "    iters.set_description(f'n_restarts: {n_restarts}, last_survival: {last_survival}, '+\\\n",
        "        f'loss: {round(loss,4)}, halt: {halt_method}, mean_reward: {round(mean_reward,2)}, action: {action}') \n",
        "    pass \n",
        "\n",
        "  if done: \n",
        "    env_state = env.reset() \n",
        "    env_state_list = env_state_list[1:] + [torch.tensor(env_state)] \n",
        "    env_state = torch.cat(env_state_list) \n",
        "    last_survival = iter_idx - last_start \n",
        "    survivals.append(last_survival) \n",
        "    last_start = iter_idx \n",
        "    n_restarts += 1 \n",
        "    pass \n",
        "  pass \n",
        "env.close() "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "DEBUG 1: torch.Size([20]) <class 'torch.Tensor'>\nDEBUG 2: <class 'torch.Tensor'>\nDEBUG 1: torch.Size([20]) <class 'torch.Tensor'>\nDEBUG 2: <class 'torch.Tensor'>\nDEBUG 1: torch.Size([20]) <class 'torch.Tensor'>\nDEBUG 2: <class 'torch.Tensor'>\nDEBUG 1: torch.Size([20]) <class 'torch.Tensor'>\nDEBUG 2: <class 'torch.Tensor'>\nDEBUG 1: torch.Size([20]) <class 'torch.Tensor'>\nDEBUG 2: <class 'torch.Tensor'>\nDEBUG 1: torch.Size([20]) <class 'torch.Tensor'>\nDEBUG 2: <class 'torch.Tensor'>\nDEBUG 1: torch.Size([20]) <class 'torch.Tensor'>\nDEBUG 2: <class 'torch.Tensor'>\nDEBUG 1: torch.Size([20]) <class 'torch.Tensor'>\nDEBUG 2: <class 'torch.Tensor'>\nDEBUG 1: torch.Size([20]) <class 'torch.Tensor'>\nDEBUG 2: <class 'torch.Tensor'>\nDEBUG 1: torch.Size([20]) <class 'torch.Tensor'>\nDEBUG 2: <class 'torch.Tensor'>\nDEBUG 1: torch.Size([20]) <class 'torch.Tensor'>\nDEBUG 2: <class 'torch.Tensor'>\nDEBUG 2: <class 'torch.Tensor'>\nDEBUG 1: torch.Size([20]) <class 'torch.Tensor'>\nDEBUG 2: <class 'torch.Tensor'>\nDEBUG 1: torch.Size([20]) <class 'torch.Tensor'>\nDEBUG 2: <class 'torch.Tensor'>\nDEBUG 1: torch.Size([20]) <class 'torch.Tensor'>\nDEBUG 2: <class 'torch.Tensor'>\nDEBUG 1: (4,) <class 'numpy.ndarray'>\n"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (1x4 and 20x256)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-ccd5544a9203>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'DEBUG 1: {env_state.shape} {type(env_state)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0menv_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-9f9d469384e8>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, env_state)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0menv_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0menv_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mpredicted_reward_per_action_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_reward_per_action_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-9f9d469384e8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x4 and 20x256)"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\r  0%|          | 0/10000 [00:00<?, ?it/s]/anaconda/envs/azureml_py36/lib/python3.6/site-packages/ipykernel_launcher.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\r  0%|          | 15/10000 [00:00<00:18, 551.92it/s]\n"
        }
      ],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1649943985562
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "plt.plot(survivals)\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1649942643521
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1,2,3]) \r\n",
        "torch.cat([x,x,x]).shape "
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "torch.Size([9])"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1649942826168
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}