{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Regularizers as memory\n",
        "\n",
        "The greatest challenge in modern AI research is the limiting returns computational work. \n",
        "While the greatest advances can only be afforded by large technology firms, even they \n",
        "cannot afford to push results further. A clear plateau has developed. \n",
        "Insufficient computational efficiency motivates a return to theory, begs a question: \n",
        "_where is waste most significant?_ \n",
        "\n",
        "The advent of catestrophic forgetting shows raw, non-compressed information must be continually re-applied \n",
        "if it is not to be forgotten. If important information could be preserved--even partially--we would expect \n",
        "more efficient computation. In short, _this work targets the realization of memory_.\n",
        "\n",
        "Regularized likelihood equations have a Lagrangian form, so implicitly describe geometric constraints on estimates.\n",
        "For example, here's an estimate constrained to an L2-sphere in $\\Theta$-space.\n",
        "\n",
        "$$ \\hat \\theta = \\text{arg max}_{\\theta \\in \\Theta} n^{-1} \\sum_{i=1}^n \\log f_X(X_i;\\theta) - \\lambda \\| \\theta \\|_2^2 $$\n",
        "\n",
        "In this work, we'll generalize the regularizer $\\| \\cdot \\|_2^2$ to support alternative geometries, \n",
        "in attempting to construct numerically convenient memory approximations. \n",
        "Particulary, we'll seek to approximate the following equation.\n",
        "Note that it introduces parabolic geometric constraints on the estimate. \n",
        "\n",
        "$$ \\hat \\theta = \\text{arg max}_{\\theta \\in \\Theta} n^{-1} \\sum_{i=1}^n \\log f_X(X_i;\\theta) - \\frac{\\lambda}{2} (\\theta - \\theta_0)^T \\mathcal{I}_{\\theta_0} (\\theta - \\theta_0) $$"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RL-reweighted estimates \n",
        "\n",
        "This first estimate is designed to improve efficiency of sampling in reinforcement learning (RL) by \n",
        "up-weighting more-important observations. We'll use this estimate:\n",
        "\n",
        "$$ \\hat \\theta_n = \\text{arg max}_\\theta \\left( \\sum_{i=1}^n \\| \\hat r_i \\|_2^{2b} \\right)^{-1} \\sum_{i=1}^n \\| \\hat r_i \\|_2^{2b} \\log f_X(X_i; \\theta) - \\frac{\\lambda}{2} (\\theta - \\hat \\theta_{n-1})^T H_i (\\theta - \\hat \\theta_{n-1})  $$\n",
        "\n",
        "where \n",
        "- $\\hat r_i$ is the estimated reward for observation $i$, \n",
        "- $b > 0$ is a bias term,  \n",
        "- $H_i = \\sum_{j=1}^i \\left( \\nabla_\\theta \\log f_X(X_j; \\theta)|_{\\theta = \\hat \\theta_{j-1}} \\right) \\left( \\nabla_\\theta \\log f_X(X_j; \\theta)|_{\\theta = \\hat \\theta_{j-1}} \\right)^T$ is a Hessian approximation, and\n",
        "- $\\hat \\theta_0$ is an initial estimate guess, commonly required in optimization routines. \n",
        "\n",
        "Later, we'll experiment with recency bias and rank reductions for $H_i$."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.6 - AzureML",
      "language": "python",
      "name": "python3-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}