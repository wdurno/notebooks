{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# memory as geometry \r\n",
        "\r\n",
        "Under asymptotic analysis and mild regularity assumptions, all statistical models have a Lagrangian-form regularizer equivalent to memory \\[1\\]. \r\n",
        "We will mathematically generalize this concept to show a breadth of geometries equivalent to memory. \r\n",
        "As a corollary, we'll construct universal sufficient statistics. \r\n",
        "Finally, given large models' recent immense productivity, it is natural to expect hardware limitations to soon motivate miniaturization (TODO CITE). \r\n",
        "We'll leverage memory's geometric properties to construct miniaturization techniques for deep learning, including \r\n",
        "memory merges, regularizers as memory, and a _frontal lobe_ concept. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## result: Langrangian-regularized estimates are MLEs \n",
        "\n",
        "The MLE's powerful theoretical guarantees provide an important foundation for Deep Learning's success. \n",
        "So, we should be curious as to when our new estimation paradigms are equivalent to MLEs. \n",
        "Theory is a guide to the statistical researcher. \n",
        "\n",
        "**LEMMA 1:** \n",
        "\n",
        "Let the following solution exist uniquely. \n",
        "\n",
        "$$ \\hat \\theta_L = \\arg \\max_{\\theta \\in \\Theta} n^{-1} \\log f_X(X;\\theta) - \\lambda g(\\theta) $$\n",
        "\n",
        "Then then there exists linear subspace $H \\subset \\Theta$ \n",
        "such that $\\hat \\theta_L$ is the solution to the following optimization program. \n",
        "\n",
        "$$ \\hat \\theta_L = \\arg \\max_{\\theta \\in H} \\log f_X(X; \\theta) $$\n",
        "\n",
        "**PROOF:** \n",
        "\n",
        "Define $\\hat{\\mathcal{L}} := n^{-1} \\log f_X(X;\\theta) - \\lambda g(\\theta)$. \n",
        "Since $\\hat \\theta_L = \\arg \\max_{\\theta \\in \\Theta} \\hat{\\mathcal{L}}$ exists uniquely, \n",
        "and $\\log f_X, g$ are differentiable, so we are guaranteed that \n",
        "\n",
        "1. $\\nabla_\\theta \\hat{\\mathcal{L}} = 0$ and \n",
        "2. $\\nabla_\\lambda \\hat{\\mathcal{L}} = 0$. \n",
        "\n",
        "There exists $c = g(\\hat \\theta_L)$ \n",
        "and $\\mathcal{L} := n^{-1} \\log f_X(X;\\theta) - \\lambda( g(\\theta) - c )$ such that \n",
        "\n",
        "3. $\\nabla_\\theta \\hat{\\mathcal{L}} = \\nabla_\\theta \\mathcal{L} = 0$ and \n",
        "4. $\\nabla_\\lambda \\mathcal{L} = c$. \n",
        "\n",
        "By (4), $\\theta \\in g^{-1}(c)$. \n",
        "For $\\theta$ sufficiently near $\\hat \\theta_L$, $g^{-1}(\\theta) \\approx \\left( J_{g, \\hat \\theta_L} \\right)^{-1} \\left(\\theta - \\hat \\theta_L \\right)$ by the inverse function theorem (TODO: check calcs, and use Constant Rank Theorem instead). \n",
        "Define this local linear subspace as $H := \\left\\{ \\theta \\; : \\; \\eta \\; s.t. \\; \\exists \\theta \\; \\& \\; \\eta = \\left( J_{g, \\hat \\theta_L} \\right)^{-1} \\left(\\theta - \\hat \\theta_L \\right) \\right\\}$ or $H$-space. \n",
        "\n",
        "Since $\\nabla_\\theta \\hat{\\mathcal{L}} = 0$ also holds, \n",
        "$\\mathcal{L}$ is a Langrangian constraining $\\theta$ to linear subspace $H \\subset \\Theta$. \n",
        "\n",
        "$\\square$"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RESULT 1:** \n",
        "\n",
        "If $\\hat \\theta_{MLE}:= \\arg\\max_{\\theta \\in \\Theta} \\log f_X(X;\\theta) $, \n",
        "then $\\hat \\theta_L$ is an MLE. \n",
        "\n",
        "**PROOF:** \n",
        "\n",
        "Since $\\hat \\theta_{MLE}$ is an MLE, $f_X$ is a likelihood function. \n",
        "By LEMMA 1, the estimation program is constrained to well-defined linear subspace $H \\subset \\Theta$. \n",
        "Hence $\\hat \\theta_L$ is indeed an MLE. \n",
        "\n",
        "$\\square$ "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Clarify $J$. \n",
        "\n",
        "**COROLLARY 1:** \n",
        "\n",
        "Define $J := \\left[ \\partial \\theta_0 / \\partial \\eta_j \\right]$, \n",
        "$J^+$ to be the pseudo-inverse matrix, \n",
        "and $\\mathcal{I}_H :=  J^+ \\mathcal{I}_{\\theta_0} J^{+T}$\n",
        "\n",
        "If $X_i \\sim  f_X(x; \\theta_0)$ and $\\hat \\theta_L$ is sufficiently near $\\hat \\theta_{MLE}$ and $n$ sufficiently large, \n",
        "then $\\hat \\theta_L$ has a corresponding estimate $\\hat \\eta$ in the $H$-space basis,  \n",
        "and $\\sqrt{n} \\left( \\hat \\eta - J^+ \\theta_0 \\right) \\sim N(0, \\mathcal{I}_H )$. \n",
        "\n",
        "**PROOF:** \n",
        "\n",
        "$0 = \\nabla_\\theta \\log f_X(X; \\theta) $\n",
        "\n",
        "$\\approx \\nabla_\\theta \\log f_X(X; \\theta_0) - \\left( \\theta - \\theta_0 \\right)^T \\nabla_\\theta^T \\nabla_\\theta \\log f_X(X; \\theta_0) $ by Taylor expansion \n",
        "\n",
        "$ \\Rightarrow \\sqrt{n}^{-1} \\nabla_\\theta \\log f_X(X; \\theta_0) \\approx \\frac{\\sqrt{n}}{n} \\left( \\theta - \\theta_0 \\right)^T \\nabla_\\theta^T \\nabla_\\theta \\log f_X(X; \\theta_0) $\n",
        "\n",
        "$ \\Rightarrow N \\mathcal{I}_{\\theta_0}^{1/2} =_d \\sqrt{n} \\left( \\theta - \\theta_0 \\right)^T \\mathcal{I}_{\\theta_0} $ \n",
        "where $=_d$ is equivalence in distribution and $N \\sim N\\left(0, I_{\\dim(\\Theta)} \\right)$ \n",
        "\n",
        "Apply lemma 1 by parameterizing $H$-space via $\\theta \\gets J \\eta$.\n",
        "\n",
        "$ \\Rightarrow \\sqrt{n} \\left( J \\eta - \\theta_0 \\right) \\sim N(0, \\mathcal{I}_{\\theta_0}) $\n",
        "\n",
        "$ \\Rightarrow \\sqrt{n} \\left( \\eta - J^+ \\theta_0 \\right) \\sim N(0, J^+ \\mathcal{I}_{\\theta_0} J^{+T} ) $ \n",
        "\n",
        "$\\square$ \n",
        "\n",
        "Since $J^+$ is also a projection matrix, the distribution of $\\hat \\eta$ is equivalent to the $\\hat \\theta_{MLE}$ projection, \n",
        "$\\sqrt{n} J^+ \\left( \\hat \\theta_{MLE} - \\theta_0 \\right) \\sim N(0, \\mathcal{I}_H )$. \n",
        "\n",
        "**COROLLARY 2:** \n",
        "\n",
        "$\\text{rank}(J^+) > 0 \\Rightarrow \\mathcal{I}_H < \\mathcal{I}_{\\theta_0}$ \n",
        "\n",
        "**PROOF:** \n",
        "\n",
        "Since $\\mathcal{I}_{\\theta_0} > 0$ (positive semi-definite), there is matrix $A$ such that $\\mathcal{I}_{\\theta_0} = AA^T $. \n",
        "Since $J^+$ is a projection matrix, we have the following.\n",
        "\n",
        "$ \\mathcal{I}_{\\theta_0} = \\mathcal{I}_H + (I - J^+)AA^TJ^{+T} + J^+AA^T(I - J^+)^T + (I - J^+)AA^T(I - J^+)^T$ \n",
        "\n",
        "$ = \\mathcal{I}_H + 0 + 0 + (I - J^+)AA^T(I - J^+)^T $ by linear independence of complementrary projections \n",
        "\n",
        "Recognizing $(I - J^+)AA^T(I - J^+)^T > 0$ completes the proof. \n",
        "\n",
        "$\\square$ \n",
        "\n",
        "If $\\hat \\theta_L \\not= \\hat \\theta_{MLE}$ for all $n$, then $\\theta_0 \\not \\in H$, \n",
        "and we may interpret $H$ as the tangent space of a biased sub-manifold in $\\Theta$. \n",
        "However, since dimensional reductions cause $\\mathcal{I}_H < \\mathcal{I}_{\\theta_0}$, \n",
        "a sufficiently-unbiased manifold produces more-efficient estimators, \n",
        "particularly so if $\\theta_0 \\in \\lim_{n \\to \\infty} H$. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## result: elliptical sub-manifolds in $\\Theta$-space are memories\n",
        "\n",
        "Define a _memory_ as all information associated with samples $X_1, X_2, \\ldots, X_n$ under a model. \n",
        "Concretely, we'll use approximate MLE sufficient statistics $\\left( \\hat \\theta, \\hat{\\mathcal{I}} \\right)$.\n",
        "\n",
        "WLOG, assume $\\mathcal{I}_{\\theta_0} > 0$. \n",
        "If not, perfect correlations exist, so the model can be reparameterized to a lower dimension. \n",
        "$\\mathcal{I}_{\\theta_0} > 0 \\Rightarrow \\mathcal{I}_{\\theta_0} = AA^T \\; \\& \\; \\exists A^{-1}$. \n",
        "For all $B \\in \\mathbb{R}^{p \\times q}$ where $p = \\dim(\\Theta)$ and $q \\leq p$, \n",
        "there exists $J = A^{-1}B$ mapping from an unbiased, linear $H$-space to $\\Theta$-space, \n",
        "and $(\\theta - \\theta_0)^T AA^T (\\theta - \\theta_0) = (\\eta - \\eta_0)^T J^T AA^T J (\\eta - \\eta_0) = (\\eta - \\eta_0)^T BB^T (\\eta - \\eta_0)^T$.\n",
        "This is true for any choice of $B$, so covers all PSD matrices $BB^T \\geq 0$. \n",
        "\n",
        "Choosing $g(\\theta) = (\\theta - \\theta_0)^T BB^T (\\theta - \\theta_0)$ and applying lemma 1, \n",
        "we recognize the existence of a elliptical sub-manifold in $\\Theta$-space corresponding to Lagrangian-form regularizer $g(\\theta)$. \n",
        "Hence, there exists an MLE with $\\mathcal{I} = BB^T$ when estimation is constrained to a subspace with $J = A^{-1}B$. \n",
        "So, for every elliptical sub-manifold with Riemann metric $\\mathcal{I} = BB$, there exists another linear sub-manifold $H$ \n",
        "such that $\\arg\\max_{\\theta \\in H} \\log f_X(X;\\theta)$ has approximate sufficient statistics $(\\hat \\theta, BB^T)$. \n",
        "\n",
        "Since we interpret $(\\hat \\theta, BB^T)$ as a memory, \n",
        "every elliptical sub-manifold $(\\eta - \\eta_0)^T BB^T (\\eta - \\eta_0)^T$ \n",
        "has an associated memory. \n",
        "\n",
        "TODO: look for a 1-1 correspondence, tying each manifold to a unique memory. \n",
        "Example consideration: $(\\hat \\eta, BB^T)$ and $(\\eta - \\hat \\eta)^T BB^T (\\eta - \\hat \\eta)^T$. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## frontal lobe \n",
        "\n",
        "In humans, the frontal lobe operates on a longer-term timeline than Hippocampus-based short-term memory, \n",
        "but on a shorter horizon than remaining cortical regions, \n",
        "so functions as a mid-term horizon or _working space_ (TODO: CITE!). \n",
        "Observing miniaturization requirements on deep learning imposes parameter space restrictions, \n",
        "which deny infinite growth strategies like `net2net` (TODO: CITE!). \n",
        "Instead, we'll allocate dimensional parameter space to a flex capacity, \n",
        "so operating in similar capacity to a biological frontal lobe. \n",
        "Mathematically, we'll achieve this by all filling dimensional space with information, \n",
        "then periodically reducing information stored in the frontal lobe parameters. \n",
        "The net result is a temporary increase in dimensional space available to the model, \n",
        "which routinely moves information into long-term memory. \n",
        "This results in the following two-step optimization procedure.\n",
        "\n",
        "1. $ \\hat \\theta = \\arg \\max_\\theta n_B^{-1} \\log f_X(X; \\theta) $\n",
        "$ - n_A^{-1} (\\theta - \\hat \\theta_A)^T \\hat{\\mathcal{I}}_{\\hat \\theta_A} (\\theta - \\hat \\theta_A) $\n",
        "2. $ \\hat \\theta = \\arg \\max_\\theta  n_B^{-1} \\log f_X(X; \\theta) $ \n",
        "$ - n_A^{-1} (\\theta - \\hat \\theta_A)^T \\hat{\\mathcal{I}}_{\\hat \\theta_A} (\\theta - \\hat \\theta_A) $ \n",
        "$ - \\lambda \\| \\hat{\\mathcal{I}}_\\theta \\|_{FL} $\n",
        "(TODO: check sample sizes)\n",
        "\n",
        "As we'll illustrate (TODO!), a frontal lobe is practically motivated in deep learning, \n",
        "because freezing all parameters in memory effectively consumes all dimensional space. \n",
        "By regularly relieving dimensional space, a necessary working space can be retained. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## references \r\n",
        "\r\n",
        "\\[1\\] Kirkpatrick et al. (2017) \"Overcoming catastrophic forgetting in neural networks\", PNAS"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK V2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}