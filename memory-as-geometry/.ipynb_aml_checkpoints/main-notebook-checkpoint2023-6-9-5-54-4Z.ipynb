{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Example: Information geometry informs data size requirements \r\n",
        "\r\n",
        "We've made great progress in large models by increasing sample size $n$ via advances in data collection and computation. \r\n",
        "The goal is to get our estimate $\\hat \\theta$ as close to truth $\\theta_0$ as possible. \r\n",
        "Asymptotically, maximum likelihood estimates (MLEs) will yield variance of $\\mathcal{I}_{\\theta_0}/n$, \r\n",
        "which describes how close $\\hat \\theta$ is to $\\theta_0$. \r\n",
        "Thus, our journey toward powerful large models is essentially one of shrinking $\\mathcal{I}_{\\theta_0}/n$.\r\n",
        "We control $n$, so it makes sense that we started there. \r\n",
        "We don't control $\\mathcal{I}_{\\theta_0}$, but we should take advantage of its structure. \r\n",
        "\r\n",
        "$\\mathcal{I}_{\\theta_0}$ is a Riemann metric, so may have high-magnitude tangent-space dimensions thereby bestowing significant information or mutual information. \r\n",
        "In turn, this information makes our estimator variance $\\mathcal{I}_{\\theta_0}/n$ smaller. \r\n",
        "If we discover high-information dimensions, we may over-weight them to have the same result as increasing $n$, at least for target dimensions. \r\n",
        "So, we'll replace the usual regularizing scalar $\\lambda$ with a function to and from information matrices. \r\n",
        "\r\n",
        "The following example illustrates these mechanisms in play, aiding estimation.\r\n",
        "\r\n",
        "$\\hat \\theta_A := \\arg \\max_{\\theta \\in \\Theta} \\log f_X(X_B; \\theta) - n_A (\\theta - \\hat \\theta_A)^T \\lambda \\left( \\hat{\\mathcal{I}}_{X_A} \\right) (\\theta - \\hat \\theta_A)$.\r\n",
        "\r\n",
        "$\\hat \\theta_B := \\arg \\max_{\\theta \\in \\Theta} \\log f_X(X_A; \\theta) $.\r\n",
        "\r\n",
        "$\\hat{\\mathcal{I}}_{X_A} := n_A^{-1} \\sum_{i=1}^{n_A} \\left( \\nabla_\\theta \\log f_X(X_A; \\theta) \\right) \\left( \\nabla_\\theta \\log f_X(X_A; \\theta) \\right)^T $\r\n",
        "$ \\to_{a.s.} \\mathcal{I}_{\\theta_0} \\text{ as } n \\to \\infty$\r\n",
        "\r\n",
        "$\\theta = (\\mu_1, \\mu_2, \\rho, \\sigma, \\tau)^T \\in \\Theta \\subset \\mathbb{R}^p, \\; p = 5 $.\r\n",
        "\r\n",
        "$\\eta = (\\mu, \\mu, \\rho, \\sigma, \\tau)^T \\in H \\subset \\Theta$, where $H$ is the submanifold where $\\mu = \\mu_1 = \\mu_2$. \r\n",
        "\r\n",
        "$\\theta_0 = (\\mu_0, \\mu_0, \\rho_0, \\sigma_0, \\tau_0)^T \\in H \\text{ and } X \\sim N_2\\left( \\begin{bmatrix} \\mu_0 \\\\ \\mu_0 \\end{bmatrix}, \\begin{bmatrix} \\sigma_0^2 & \\sigma_0 \\tau_0 \\rho_0 \\\\ \\sigma_0 \\tau_0 \\rho_0 & \\tau_0^2 \\end{bmatrix} \\right)$\r\n",
        "\r\n",
        "$\\lambda : P \\to P$ where $P \\subset \\mathbb{R}^{p \\times p}$ is a set of positive semi-definite (PSD) matrices."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## foundational definitions \r\n",
        "import numpy as np \r\n",
        "from scipy.stats import multivariate_normal\r\n",
        "from scipy.special import expit, logit\r\n",
        "from scipy.optimize import minimize \r\n",
        "from numpy.linalg import inv \r\n",
        "\r\n",
        "def sample_data(n, mu0=0., rho0=0., sigma0=1., tau0=1.): \r\n",
        "    'generates sample matrix x' \r\n",
        "    mean = np.array([mu0, mu0]) \r\n",
        "    cov = np.array([[sigma0*sigma0, sigma0*tau0*rho0], [sigma0*tau0*rho0, tau0*tau0]]) \r\n",
        "    return multivariate_normal.rvs(mean=mean, cov=cov, size=n) \r\n",
        "\r\n",
        "def unpack_theta(theta): \r\n",
        "    'unpack from (mu1, mu2, logit rho, log sigma, log tau)'\r\n",
        "    mu1 = theta[0] \r\n",
        "    mu2 = theta[1] \r\n",
        "    rho = expit(theta[2]) \r\n",
        "    sigma = np.exp(theta[3]) \r\n",
        "    tau = np.exp(theta[4]) \r\n",
        "    theta_vec = np.array([mu1, mu2, rho, sigma, tau]) \r\n",
        "    mean = np.array([mu1, mu2]) \r\n",
        "    cov = np.array([[sigma*sigma, sigma*tau*rho], [sigma*tau*rho, tau*tau]]) \r\n",
        "    return theta_vec, mean, cov \r\n",
        "\r\n",
        "def mle(x): \r\n",
        "    'x is a sampled matrix'\r\n",
        "    def loss(theta): \r\n",
        "        _, mean, cov = unpack_theta(theta) \r\n",
        "        l = multivariate_normal.logpdf(x=x, mean=mean, cov=cov) \r\n",
        "        l = sum(l) \r\n",
        "        return -l  \r\n",
        "    theta0 = [0., 0., 0., 0., 0.] \r\n",
        "    result = minimize(loss, theta0)  \r\n",
        "    theta_A_estimate = result.x \r\n",
        "    info_estimate = inv(result.hess_inv) \r\n",
        "    return theta_A_estimate, info_estimate  \r\n",
        "\r\n",
        "def memory_mle(x, info, theta_A): \r\n",
        "    '''\r\n",
        "    info is an information matrix, already adjusted by regularizer and sample size \r\n",
        "    theta_A is the memorized estimate\r\n",
        "    '''\r\n",
        "    def loss(theta): \r\n",
        "        theta_vec, mean, cov = unpack_theta(theta) \r\n",
        "        memory_term = np.transpose(theta_vec - theta_A) \r\n",
        "        memory_term = np.matmul(memory_term, info) \r\n",
        "        memory_term = np.matmul(memory_term, theta_vec - theta_A) \r\n",
        "        l = multivariate_normal.logpdf(x=x, mean=mean, cov=cov) \r\n",
        "        l = sum(l) \r\n",
        "        l = l - memory_term \r\n",
        "        return -l \r\n",
        "    theta0 = [0., 0., 0., 0., 0.] \r\n",
        "    return minimize(loss, theta0).x \r\n",
        "\r\n",
        "print('Examples...') \r\n",
        "n_A = 10 \r\n",
        "n_B = 10 \r\n",
        "x = sample_data(n_A) \r\n",
        "y = sample_data(n_B) \r\n",
        "print(f'sample_data({n_A})[:3,]: {x[:3,]}') \r\n",
        "theta_A_estimate, info_estimate = mle(x) \r\n",
        "print(f'initial estimate: {unpack_theta(theta_A_estimate)[0]}') \r\n",
        "print(f'info estimate: {info_estimate}')\r\n",
        "theta_B_estimate = memory_mle(y, info_estimate, theta_A_estimate) \r\n",
        "print(f'updated estimate: {theta_B_estimate}') "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Examples...\nsample_data(10)[:3,]: [[-0.63383035 -1.31419678]\n [ 0.04025527  1.01620383]\n [-0.26725872  0.02448196]]\ninitial estimate: [-0.48245104 -0.06806565  0.29299423  0.42164847  0.75460873]\ninfo estimate: [[ 6.17050422e+01 -1.17109593e+01  1.86733519e-01 -2.47740511e-01\n  -5.57971422e-01]\n [-1.17109593e+01  1.87966585e+01  1.68530502e-02 -3.75785191e-01\n  -1.44981030e-02]\n [ 1.86733519e-01  1.68530502e-02  5.52158324e-01 -5.99420353e-01\n  -6.34738564e-01]\n [-2.47740511e-01 -3.75785191e-01 -5.99420353e-01  2.05791116e+01\n  -9.54750912e-01]\n [-5.57971422e-01 -1.44981030e-02 -6.34738564e-01 -9.54750912e-01\n   2.06599707e+01]]\nupdated estimate: [-3.23832916e-01 -5.34646660e-04 -1.67218057e+01 -5.25297735e-01\n -4.37809893e-01]\n"
        }
      ],
      "execution_count": 94,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688880470254
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When $\\sigma = \\tau = 1$, $\\mu$'s information is $2(1-\\rho)/(1 - \\rho^2)$. This illustrates the effect of high-information dimensions. \r\n",
        "- As $\\rho \\to -1$, information becomes infinite, so a single sample reveals $\\mu_0$ exactly via $\\hat \\mu_0 = (X_1 + X_2)/2$. \r\n",
        "- If $\\rho = 0$, information is 2, so we need only half the sample size. \r\n",
        "- As $\\rho \\to 1$, information becomes 1, so multidimensionality lends no benefit. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## plot of mu's info \r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "rho_val_list = np.linspace(-.75, .75).tolist() \r\n",
        "info = lambda rho : 2.*(1. - rho)/(1. - rho*rho) \r\n",
        "info_val_list = [info(rho) for rho in rho_val_list] \r\n",
        "\r\n",
        "plt.plot(rho_val_list, info_val_list)\r\n",
        "plt.ylabel('mu info')\r\n",
        "plt.xlabel('rho')\r\n",
        "plt.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEGCAYAAACEgjUUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAAsTAAALEwEAmpwYAAAkFklEQVR4nO3deXxddZ3/8dfnZt/3tGnSJmlpoaWlhcbSIptAFRhBFBdwcFCR+hPHdXzM6Di/H+hvfuM446A4ygjqoIyyKAICitoiBQp0SfeW7um+Jl2ytUmb5PP7497WtKTJbZu75N738/E4j9zcnHvPm0vzviffc+73mLsjIiKJLRDrACIiEnkqexGRJKCyFxFJAip7EZEkoLIXEUkCqbEO0FtpaanX1NTEOoaIyJCxePHiJncvG2i9uCr7mpoa6uvrYx1DRGTIMLOt4aynYRwRkSSgshcRSQIqexGRJKCyFxFJAip7EZEkENGyN7MvmdlqM1tlZo+bWWYktyciIn2LWNmbWSXweaDO3ScCKcBtkdqeiIicXqSHcVKBLDNLBbKBXYO9gc6ubn70yibmbWga7KcWEUkYESt7d98JfAfYBuwGmt39T6euZ2azzKzezOobGxvPeDvpKQF+/GoDzyzdec6ZRUQSVSSHcYqA9wG1wAggx8zuOHU9d3/Y3evcva6sbMBP/Pa1HabVFjO/Yf85ZxYRSVSRHMa5Dtjs7o3ufgx4GrgsEhu6tLaYnYeOsOPg4Ug8vYjIkBfJst8GTDezbDMz4FpgTSQ2NH1MCQALGg5E4ulFRIa8SI7ZLwCeApYAK0PbejgS2xpXnkdhdhoLNmsoR0SkLxGd9dLd7wXujeQ2AAIBY1pNMfO1Zy8i0qeE+QTtpaNL2HbgMLubj8Q6iohI3Emcsq8tBjRuLyLSl4Qp+/EV+eRlpmrcXkSkDwlT9ikB49LaYu3Zi4j0IWHKHuDS2hIamtrZ19IR6ygiInElscp+dHDcfv5m7d2LiPSWUGU/oSKf3IxUFmjqBBGRkyRU2aemBKirKWKB9uxFRE6SUGUPMH10CRv3tdHU1hnrKCIicSPhyl7n24uIvF3Clf3EygKy01N0vr2ISC8JV/ZpKQGmVhdpz15EpJeEK3sIjtuv29vKgfajsY4iIhIXErTsg+P2C3VWjogIkKBlP6mykMy0gC5VKCISkpBln54aGrfXnr2ICJCgZQ/BeXLW7mmh+fCxWEcREYm5BC77Ytxh4Rbt3YuIJGzZTx5ZSHpqQPPkiIiQwGWfmZbC1FFFzNvYFOsoIiIxF7GyN7PzzWxZr6XFzL4Yqe315drx5azd08r2A4ejuVkRkbgTsbJ393XuPsXdpwBTgcPAM5HaXl9mThgGwOy39kZzsyIicSdawzjXApvcfWuUtgdAdUkOY8tzmbNGZS8iyS1aZX8b8HhfPzCzWWZWb2b1jY2Ng77hmROGsWDzAZ2CKSJJLeJlb2bpwM3Ar/v6ubs/7O517l5XVlY26NufOWEY3T3Oy+v2Dfpzi4gMFdHYs78BWOLuMRlLmVxVSFleBrM1lCMiSSwaZX87pxnCiYZAwLhufDmvrGuks6s7VjFERGIqomVvZjnATODpSG5nIDMnDKOts4v5muNeRJJURMve3dvdvcTdmyO5nYFcNqaUrLQU5ugUTBFJUgn7CdreMtNSuHJcKXPW7MXdYx1HRCTqkqLsAWZOGM7u5g5W7WyJdRQRkahLmrK/5oJyAobOyhGRpJQ0ZV+ck05ddbGmThCRpJQ0ZQ/Bs3LW7G7RxGgiknSSruwBzZUjIkknqcq+plQTo4lIckqqsge4bsIwFjQcoPmIJkYTkeSRdGU/c8IwunqcuZoYTUSSSNKV/ZSqQkpzM3RWjogklaQr+0DAmDmhnLnrGuk4ponRRCQ5JF3ZA9w8uZK2zi7+uHpPrKOIiERFUpb9pbXFVBZm8dTiHbGOIiISFUlZ9oGAcevUKuZtbGJ385FYxxERibikLHuAWy+pxB2eWboz1lFERCIuacu+uiSHabXFPLV4h6Y9FpGEl7RlD/DBqVU0NLazdPuhWEcREYmopC77GydVkJWWogO1IpLwkrrsczNSuWHicJ5fvkvn3ItIQkvqsofgUE5rRxd/0idqRSSBRbTszazQzJ4ys7VmtsbMZkRye2dj+ugSnXMvIgkv0nv2DwB/cPcLgMnAmghv74wFAsatl1Qyb0Mje5o7Yh1HRCQiIlb2ZlYAXAn8FMDdj7r7oUht71zcOrWKHp1zLyIJLJJ79rVAI/CImS01s5+YWc6pK5nZLDOrN7P6xsbGCMY5veqSHKbVFPPU4u06515EElIkyz4VuAT4L3e/GGgHvnrqSu7+sLvXuXtdWVlZBOP079aplWxqbGeZzrkXkQQUybLfAexw9wWh758iWP5x6cZJFWSmBXSgVkQSUsTK3t33ANvN7PzQXdcCb0Vqe+cqLzONGyZW8JzOuReRBBTps3E+B/zSzFYAU4B/ifD2zsmH6oLn3D+3bFeso4iIDKqIlr27LwuNx1/k7re4+8FIbu9czRhdwviKfH4yr0EHakUkoST9J2h7MzM+dXkt6/e28eqGpljHEREZNCr7U9w0eQTD8jP4yWsNsY4iIjJoVPanSE8NcOdlNby2oYk1u1tiHUdEZFCo7Pvw0WmjyEpL4afzNsc6iojIoFDZ96EwO50P11Xx22U72dei+XJEZOhT2Z/GJy+vpavH+fmbW2IdRUTknKnsT6O6JIf3TBjOL+Zv4/DRrljHERE5Jyr7fnzqilqajxzjN5pCQUSGOJV9P6ZWFzFlZCE/nbeZ7h59yEpEhi6VfT/MjLuvGM2W/YeZs0aXLRSRoUtlP4D3XDiMysIsfvqaTsMUkaFLZT+A1JQAn7y8loVbDmiuexEZslT2YfjIO0ZSmJ3Gd2evj3UUEZGzorIPQ25GKvdcPYZX1jcyv2F/rOOIiJwxlX2Y/mZGDcPzM/m3P6zV9MciMuSo7MOUmZbCF64by5Jth3hpzb5YxxEROSMq+zPwoalV1Jbm8O9/XKfz7kVkSFHZn4HUlABfnjmOdXtbeW75zljHEREJW1hlb2aTzexvQ8vkSIeKZ381qYIJFfncP3s9R7t6Yh1HRCQsA5a9mX0B+CVQHlp+YWafi3SweBUIGH9//flsP3CEJxZti3UcEZGwpIaxzl3Ape7eDmBm3wbeBP5zoAea2RagFegGuty97uyjxo+rxpUxrbaY77+0kQ9OrSI7PZyXUUQkdsIZxjGCZX1cd+i+cL3L3ackStFDcM6cf7j+fJraOnnk9S2xjiMiMqBwyv4RYIGZ3Wdm9wHzgZ9GNNUQMLW6mGsvKOehVzbRfPhYrOOIiPTrtGVvZrUA7n4/8AngQGj5hLt/L8znd+BPZrbYzGadZjuzzKzezOobGxvPKHysfeU959Pa2cUP526MdRQRkX71t2f/FICZveTuS9z9+6Fl6Rk8/+XufglwA/BZM7vy1BXc/WF3r3P3urKysjNLH2PjK/L50NQq/nveZtbtaY11HBGR0+qv7ANm9o/AODP78qlLOE/u7jtDX/cBzwDTzj1yfPnqDePJzUzln55dSY8+aCUicaq/sr+N4MHYVCCvj6VfZpZjZnnHbwPvBlada+B4U5yTzj/eMJ5FWw7y1BJdvlBE4tNpzxl093XAt81shbu/eBbPPQx4xsyOb+cxd//D2cWMbx+cWsWv6rfzrd+vYeb4YRTlpMc6kojIScI5QfzPZvZRoKb3+u7+zf4e5O4NQFJ82jYQMP75/RN57/fn8a8vruXbH7wo1pFERE4SzqmXvwXeB3QB7b0W6eWC4fncdXktT9Zvp37LgVjHERE5STh79lXufn3EkySAL1w3lhdW7Obrz6zihc9fTlqK5pkTkfgQThu9YWaTIp4kAWSnp3LvTRNYt7eVR17XBcpFJH6EU/aXA4vNbJ2ZrTCzlWa2ItLBhqp3Xzic68YP47uzN7Dz0JFYxxERAcIr+xuAsQRPnbwJeG/oq5zGfTdPAODe367SJQxFJC70N11Cfuhm62kWOY2qomz+7t3jmLNmH08u2h7rOCIi/R6gfYzgXvxignPc9J7p0oHREcw15H3ynbXMXdfIN55/i2m1xYwuy411JBFJYqfds3f394a+1rr76NDX44uKfgCBgPGdD00mIy3AF59cxrFuXdVKRGJH5wZG0PCCTL71/kms2NHM9+asj3UcEUliKvsIu2FSBR+uq+LBuZtYuFkfthKR2FDZR8G9N11IdXE2X3pyGc1HdKETEYm+cC44PqqvJRrhEkVORirf/cgU9rR08H9+m3ATf4rIEBDOdAm/4y9n42QCtcA64MII5ko4F48q4gvXjuX+2et51/nl3HJxZawjiUgSGXDP3t0nuftFoa9jCV6A5M3IR0s891w9hrrqIv73s6vY1NgW6zgikkTOeMze3ZcAl0YgS8JLTQnwwO0Xk5Ya4O5H62np0Pi9iERHOGP2vS9H+BUzewzYFYVsCamyMIsH//oStu0/zJeeWKZLGYpIVISzZ9/7UoQZBMfw3xfJUIlu+ugS7r1pAi+t3cf9s3X+vYhE3oAHaN39G9EIkmzumF7N6l0t/ODljUwYkc+NkypiHUlEEtiAZW9mdcDXgWpOviyhrr13DsyMb7zvQtbvbeXvfrWc2tIcxlfkD/xAEZGzEM4wzi+BR4BbCU5tfHwJi5mlmNlSM3vh7CImrozUFH50x1Tys1KZ9T/1HGw/GutIIpKgwin7Rnd/zt03u/vW48sZbOMLwJqzzJfwyvMzeehjdext6eSzjy3RhGkiEhHhlP29ZvYTM7vdzD5wfAnnyc2sCvgr4CfnlDLBTRlZyL+8fxJvbNrPV3+zUhc8EZFBF84naD8BXACkAcd3Ox14OozHfg/4e4Jn8kg/Pji1ip0Hj/DdOespzU3nazeOj3UkEUkg4ZT9O9z9/DN9YjN7L7DP3Reb2dX9rDcLmAUwalRyT7nz+WvP40B7Jw+92kBxTjqfvmpMrCOJSIIIZxjnDTObcBbP/U7gZjPbAjwBXGNmvzh1JXd/2N3r3L2urKzsLDaTOMyMe2+6kJsmj+BbL67l1/W6pKGIDI5w9uynA8vMbDPQSXBCNB/o1Et3/xrwNYDQnv1X3P2Oc0qbBAIB4z8+NJlDh4/y1adXUpidzswJw2IdS0SGuHD27K8HxgLvJnjK5Xs5g1Mv5cylpwb40R1TmVhZwN8+tkQXPRGRcxbOrJdb+1rOZCPuPvf4NW0lPDkZqTzy8XdQVZTFXT9fxOpdzbGOJCJDmK5UFceKc9J59K5Lyc9M46M/XsCKHYdiHUlEhiiVfZyrLMziiVnTyc9K5a9/vIDFWw/GOpKIDEEq+yFgZHE2T86aQWleBn/z0wUsaNgf60giMsSo7IeIEYVZPDlrOsMLMrnzkYW8vrEp1pFEZAhR2Q8h5fmZPDFrBtXFOXziZ4t4ed2+WEcSkSFCZT/ElOVl8Pis6ZxXlsunH13MH1fviXUkERkCVPZDUHFOOo/fPZ3xI/L5zC8W8/M3tsQ6kojEOZX9EFWQncbjd1/KNReUc+9zq/nnF97S9WxF5LRU9kNYdnoqD32sjjtnVPOTeZv57GNL6DjWHetYIhKHVPZDXErAuO/mC/mnvxrPH1bv4aM/ns/+ts5YxxKROKOyTwBmxqeuGM2DH72E1bta+MB/vcHmpvZYxxKROKKyTyA3TKrgsbun09rRxS0/fJ25OjVTREJU9glmanURz97zTkYUZvGJny3i+y9t0IFbEVHZJ6JRJdk8/ZnLuGVKJffPXs/dj9bTfORYrGOJSAyp7BNUVnoK9394Mt9834W8sr6Rm38wjzW7W2IdS0RiRGWfwMyMv5lRw5Ofns6Ro928/8HXeXbpzljHEpEYUNknganVxbzw+cu5qKqQLz65jK/8ejltnV2xjiUiUaSyTxLleZn88lOX8vlrzuPpJTu48YHXWLJNc+OLJAuVfRJJSwnw5Xefz5OfnkF3j/OhH73JA3M20NXdE+toIhJhKvsk9I6aYl784hXcdFEF352zno88PJ/tBw7HOpaIRFDEyt7MMs1soZktN7PVZvaNSG1Lzlx+Zhrfu+1ivveRKazf08oND7zG4wu34a5z8kUSUST37DuBa9x9MjAFuN7Mpkdwe3IWbrm4kt9/4QomVubztadX8tEfL2CLploQSTgRK3sPagt9mxZatNsYh0YWZ/PYp6bzrQ9MYtXOZt7zvVd5+NVNGssXSSARHbM3sxQzWwbsA2a7+4I+1pllZvVmVt/Y2BjJONKPQMC4fdooZn/5Kq4YW8a//H4t73/wDd7apQ9iiSQCi8YYrZkVAs8An3P3Vadbr66uzuvr6yOeR/rn7vxu5W7ue241hw4f464ravncNWPJzUiNdTQROYWZLXb3uoHWi8rZOO5+CHgZuD4a25NzY2a896IRzP7SVbz/4koeeqWBa74zl2eX7tQBXJEhKpJn45SF9ugxsyxgJrA2UtuTwVeUk86/f2gyT99zGcMLMvnik8v48ENvsnpXc6yjicgZiuSefQXwspmtABYRHLN/IYLbkwi5ZFRw2uR//cAkNjW2c9N/zuOfnl3JwfajsY4mImGKyph9uDRmH/+aDx/ju3PW8+ibW8jNSOWed53Hxy+rITMtJdbRRJJSXI3ZS+IoyE7jvpsv5MUvXMnU6iL+9cW1vOs7c/lV/Xa6dZEUkbilspezcv7wPB75xDQev3s65XkZ/P1TK7jxgdf489q9OogrEodU9nJOZowp4dnPvpMffvQSOru6+eTP6vnIQ/N5Y1OTSl8kjmjMXgbNse4enli4jf/880b2tXYyrbaYL143lhmjSzCzWMcTSUjhjtmr7GXQdRzr5omF23hw7qZg6deESn+MSl9ksKnsJeY6jnXz5KLtPDh3I3tbOnlHTRGfuXoMV48rJxBQ6YsMBpW9xI2OY938qn47/zV3E7ubOxg3LJdZV47h5skjSE/VYSORc6Gyl7hzrLuH55fv4uFXG1i7p5Xh+ZncdXktt00bSV5mWqzjiQxJKnuJW+7OK+sbeeiVBt5s2E9eZiq3vWMkH5tew6iS7FjHExlSVPYyJCzffogfv9bAi6v20OPOtReUc+dlNVx+XqkO5oqEQWUvQ8qe5g5+uWArjy3Yxv72o4wpy+HOy2r4wCVVmlpZpB8qexmSOru6+d2K3fzsjS2s2NFMdnoKN08ewe3TRnFRVYH29kVOobKXIc3dWbb9EI8v3Mbzy3dz5Fg34yvyuX3aSN43pZKCLB3QFQGVvSSQlo5jPLdsF48v3MbqXS1kpgW4cWIFt06tYsboEp2zL0lNZS8JaeWOZh5ftI3nl+2itbOLEQWZ3HJxJR+4pIrzynNjHU8k6lT2ktA6jnUz+629/GbJDl5d30iPw+SRhdx6SSU3TqqgNDcj1hFFokJlL0ljX2sHv126i98s2cHaPa0EDN55Xik3XTSC90wcrvF9SWgqe0lK6/a08tzynTy/fDfbDhwmPSXAlePKuGlyBdeOH6bTOCXhqOwlqbk7K3Y08/zyXbywYjd7WjpITw1wxXmlXD9xONeNH0ZRTnqsY4qcM5W9SEhPj7N420FeXLmHP67ew85DR0gJGNNHF3P9xApmjh/G8ILMWMcUOSsxL3szGwk8CgwDHHjY3R/o7zEqe4k0d2flzmb+sGoPf1i1h4amdgAmVRZw7fhyrhs/jAtH5OvDWzJkxEPZVwAV7r7EzPKAxcAt7v7W6R6jspdocnc27Gtjzpq9zHlrL0u3H8IdKgoyueaCcq4dX8700SVkp2ucX+JXzMv+bRsy+y3wA3effbp1VPYSS01tnfx57T5eWrOXV9c3ceRYN+mpAS6tLeaqcWVcfX45Y8pytNcvcSWuyt7MaoBXgYnu3nLKz2YBswBGjRo1devWrRHPIzKQjmPdLNpygLnrGpm7bh+bGoPDPVVFWVw5rowrzitlxpgSCrN1kFdiK27K3sxygVeA/+fuT/e3rvbsJV5tP3CYV9Y3MnddI29uaqL9aDcBC471Xz62lHeeV8rU6iIyUlNiHVWSTFyUvZmlAS8Af3T3+wdaX2UvQ8Gx7h6WbT/EvA1NvL6xiaXbD9Hd42SmBZhaXcSM0SXMGFPCpMpCXXZRIi7mZW/Bgc2fAwfc/YvhPEZlL0NRa8cxFjQcYN7GJuY37GftnlYAstJSqKspYvroEi6tLWZSVYH2/GXQxUPZXw68BqwEekJ3/6O7//50j1HZSyI40H6UhZv38+am/bzZsJ/1e9sAyEgNMHlkIdNqinlHbTGXjCrUtXflnMW87M+Gyl4S0f62ThZtOciiLQeo33KAVbta6O5xAgYXDM9nanXRiaWqKEtn+8gZUdmLxKn2zi6WbjvEwi0HWLL1IEu3HaT9aDcAZXkZTB1VxMWjCpk8spBJlQXkaD4f6Ue4Za9/RSJRlpORyuVjS7l8bCkA3T3Ouj2tLNl2kCVbD1K/9SB/WL0HgIDBuGF5TBlZyJSRwTeAseW5pKbowK+cGe3Zi8Sh/W2dLN9xiGXbm1m2/RDLtx+i+cgxADLTAkyoyOeiquCe/0VVBYwuyyVFV+xKShrGEUkg7s7mpnZW7mxmxY5mVu5oZtWuZg6Hhn+y01MYX5HPhSOOLwWMG5anUz+TgMpeJMF19zgNjW3B8t/ZzOpdzby1q+XE+H9aijG2PI8JI/IZX5HP+Io8JlTk61O/CUZlL5KEenqcLfvbWb2rJbQ0s2Z3C01tR0+sU1GQyfiKfC4Ynsf5oWV0aa7+ChiidIBWJAkFAsboslxGl+Vy0+QRJ+7f19rBmt2trN3dwprdLazZ3cqr6xvp6gnu7KUGjNFlOZw/PJ/zh+UydlgeY8tzqS7J0bGABKGyF0kC5XmZlOdlctW4shP3He3qoaGpjXV7Wk8sS7Ye5Pnlu06sk54aYExZLmPLcxk3LJcxZbmcF3oT0F8CQ4vKXiRJpacGuGB4PhcMzz/p/rbOLjbua2PD3lY27Gtj/d5WFm89yHO93gRSAkZ1cTZjyoNvAKPLchhTlsPo0lxd7jFOqexF5CS5Gaknzuvvrb2zi4bGdjY2trJpXzsb97WxsbGNl9fuOzEcBFCUnRYcSirNobYsh9qSHGpKc6gpySErXXMDxYrKXkTCkpORyqSqAiZVFZx0f1d3D9sPHqGhsY2GxnYamtrY1NjOy+sa+fXiHSetW1GQSU1JDjWl2VSX5FBTks2o4hyqS7L1SeEI06srIuckNSVAbWkOtaU5XDv+5J+1dhxj6/7DbG5qZ0tTO5v3t7O5qZ0/rd7L/vajJ61bmptBdUk21cXZjCzOZlRxNqNKgl/LcjMI6EDxOVHZi0jE5GWmMbGygImVBW/7WUvHMbbtP8yW/e1s3X+YLU3tbDtwmPkN+3lm2U56nxWekRqgqiiLkcXZjCzKftvtwuw0TSA3AJW9iMREfj9vBJ1d3ew8eIRtBw6z/cDh0NcjbD94mCVbD9LS0XXS+tnpKVQVZVFZmEVlURZVRdlUFmYxojB4X1leRtKfQqqyF5G4k5GacuLzAn1pPnKMHQeDbwA7Dx1hx8HD7DwYvL1k21/mETouLcUYXpDJiILgG0BFQSYVhVmMKMikoiCLEYWZFGQl9l8HKnsRGXIKstIoyCrgwhFv/6sAgscKdjd3nHgD2BVadh46wsLNB9jb0nHSGUQQvLJYRUEmw0NLReiNYHh+8Pth+ZmU5KQP2WMHKnsRSTh5mWnkZaYxblhenz/v7nEaWzvZ1XyE3Yc62N18hF2HOtjbErw9f9N+9rZ20n3KG0JailGel8mw/AyG5Wf2WjJOfC3PzyQvIzXu/kpQ2YtI0kkJ2Ik9eEb1vU53j9PU1snu5g72NAffCPa0dLC3Ofh13d5W5m1oorWz622PzUwLhD61nEF5fgbleZmU5WVQnpdBWWgpz8ukOCc9ascSVPYiIn1ICdiJPXdGnn699s4u9rV2srel48Syr6WTfa2d7GvtYO2eVl5b3/ebQsCgJDeDmpJsfv2/Lovgf43KXkTknORkpFKbkUptaU6/6x0+2kVja+dflra/3I7GiE/Eyt7M/ht4L7DP3SdGajsiIkNBdnoq1SWpVJf0/6YQKZGctu5nwPURfH4REQlTxMre3V8FDkTq+UVEJHwxn5DazGaZWb2Z1Tc2NsY6johIQop52bv7w+5e5+51ZWVlAz9ARETOWMzLXkREIk9lLyKSBCJW9mb2OPAmcL6Z7TCzuyK1LRER6V/EzrN399sj9dwiInJmzN0HXitKzKwR2HrK3aVAUwzihCve80H8Z4z3fKCMgyHe80H8Z+wrX7W7D3h2S1yVfV/MrN7d62Kd43TiPR/Ef8Z4zwfKOBjiPR/Ef8ZzyacDtCIiSUBlLyKSBIZC2T8c6wADiPd8EP8Z4z0fKONgiPd8EP8Zzzpf3I/Zi4jIuRsKe/YiInKOVPYiIkkg7srezIrNbLaZbQh9LTrNev9mZqvNbI2Zfd+idHXfM8g3ysz+FMr3lpnVRCPfmWQMrZsf+oTzD+Ipn5lNMbM3Q/+PV5jZR6KU7XozW2dmG83sq338PMPMngz9fEE0/7+Gme/LoX9vK8zsJTOrjma+cDL2Wu9WM3Mzi+qpjuHkM7MPh17H1Wb2WDTzhZMx1C8vm9nS0P/rGwd8UnePqwX4N+CrodtfBb7dxzqXAa8DKaHlTeDqeMkX+tlcYGbodi6QHU+vYa91HwAeA34QT/mAccDY0O0RwG6gMMK5UoBNwGggHVgOTDhlnXuAH4Vu3wY8GcXXLZx87zr+bw34TDTzhZsxtF4e8CowH6iLp3zAWGApUBT6vjzeXkOCB2o/E7o9Adgy0PPG3Z498D7g56HbPwdu6WMdBzIJvhAZQBqwNxrhCCOfmU0AUt19NoC7t7n74Sjlg/BeQ8xsKjAM+FN0Yp0wYD53X+/uG0K3dwH7gEjPgT0N2OjuDe5+FHgilLW33tmfAq6N1l+V4eRz95d7/VubD1RFKVvYGUP+L/BtoCOa4Qgv393AD939IIC774vDjA7kh24XALsGetJ4LPth7r47dHsPwTI6ibu/CbxMcG9vN/BHd18TL/kI7pUeMrOnQ39m/buZpUQpH4SR0cwCwH8AX4liruPCeQ1PMLNpBN/YN0U4VyWwvdf3O0L39bmOu3cBzUBJhHO9bdshfeXr7S7gxYgmersBM5rZJcBId/9dNIOFhPMajgPGmdnrZjbfzKJ9edVwMt4H3GFmO4DfA58b6EkjNhFaf8xsDjC8jx99vfc37u5m9rZzQ83sPGA8f9lrmW1mV7j7a/GQj+DregVwMbANeBL4OPDTwcg3SBnvAX7v7jsisWM6CPmOP08F8D/Ane7eM7gpE5eZ3QHUAVfFOktvoZ2M+wn+PsSrVIJDOVcT7JhXzWySux+KZahT3A78zN3/w8xmAP9jZhP7+x2JSdm7+3Wn+5mZ7TWzCnffHfpF7+tPqPcD8929LfSYF4EZwKCU/SDk2wEsc/eG0GOeBaYziGU/CBlnAFeY2T0Ejymkm1mbu5/2gFqU82Fm+cDvgK+7+/zByDWAncDIXt9Xhe7ra50dZpZK8E/o/VHI1nvbx/WVDzO7juCb6lXu3hmlbMcNlDEPmAjMDe1kDAeeM7Ob3b0+DvJB8Pd3gbsfAzab2XqC5b8oCvkgvIx3AddDcKTDzDIJTpJ22iGneBzGeQ64M3T7TuC3fayzDbjKzFLNLI3g3ku0hnHCybcIKDSz42PM1wBvRSHbcQNmdPe/dvdR7l5DcCjn0cEq+sHIZ2bpwDOhXE9FKdciYKyZ1Ya2f1soa2+9s38Q+LOHjpLFQz4zuxh4CLg5BmPNA2Z092Z3L3X3mtC/vfmhrNEo+gHzhTxLcK8eMyslOKzTEKV84WbcBlwbyjie4DHM/i/iHc2jzGEeiS4BXgI2AHOA4tD9dcBPeh2tfohgwb8F3B9P+ULfzwRWACuBnwHp8Zax1/ofJ7pn44Tz//gO4BiwrNcyJQrZbgTWEzw+8PXQfd8kWEiEfql+DWwEFgKjo/W6hZlvDsGTFY6/Zs9FM184GU9Zdy5RPBsnzNfQCA41vRX6/b0t3l5DgmfgvE7wTJ1lwLsHek5NlyAikgTicRhHREQGmcpeRCQJqOxFRJKAyl5EJAmo7EVEkoDKXqQPZlZjZqtinUNksKjsRfpm6PdDEoj+MYuEhPbm15nZo8AqIMvMfhya0/xPZpYVWm9KaIKsFWb2jPVzvQCReKGyFznZWOBB4EKC85P80N0vBA4Bt4bWeRT4B3e/iOAnLO+NQU6RM6KyFznZVv/LpGub3X1Z6PZioMbMCgheROWV0P0/B66MckaRM6ayFzlZe6/bvWeM7CZGs8SKDAaVvcgZcPdm4KCZXRG662PAK/08RCQuaE9F5MzdCfzIzLIJTn37iRjnERmQZr0UEUkCGsYREUkCKnsRkSSgshcRSQIqexGRJKCyFxFJAip7EZEkoLIXEUkC/x/qHRGpfU1uPgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "execution_count": 96,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688881169914
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For $\\rho \\approx -1$, we'll benefit from overweighting our information matrix estimate. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_error(n_A, n_B, rho0, lmbda): \r\n",
        "    x = sample_data(n_A, rho0=rho0) \r\n",
        "    y = sample_data(n_B, rho0=rho0) \r\n",
        "    theta_A_estimate, info_estimate = mle(x) \r\n",
        "    theta_B_estimate = memory_mle(y, n_A*lmbda*info_estimate, theta_A_estimate) \r\n",
        "    return theta_B_estimate[0] \r\n",
        "\r\n",
        "get_error(10, 10, rho0=-.99, lmbda=1)\r\n",
        "lmbda_list = np.linspace(0.1)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 99,
          "data": {
            "text/plain": "-0.07446399707385237"
          },
          "metadata": {}
        }
      ],
      "execution_count": 99,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688881970709
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "THE FOLLOWING IS OLD CONTENT TO BE REVISED\r\n",
        "\r\n",
        "# memory as geometry \r\n",
        "\r\n",
        "Under asymptotic analysis and mild regularity assumptions, all statistical models have a Lagrangian-form regularizer equivalent to memory \\[1\\]. \r\n",
        "We will mathematically generalize this concept to show a breadth of geometries equivalent to memory. \r\n",
        "As a corollary, we'll construct universal sufficient statistics. \r\n",
        "Finally, given large models' recent immense productivity, it is natural to expect hardware limitations to soon motivate miniaturization (TODO CITE). \r\n",
        "We'll leverage memory's geometric properties to construct miniaturization techniques for deep learning, including \r\n",
        "memory merges, regularizers as memory, and a _frontal lobe_ concept. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## result: Langrangian-regularized estimates are MLEs \n",
        "\n",
        "The MLE's powerful theoretical guarantees provide an important foundation for Deep Learning's success. \n",
        "So, we should be curious as to when our new estimation paradigms are equivalent to MLEs. \n",
        "Theory is a guide to the statistical researcher. \n",
        "\n",
        "**LEMMA 1:** \n",
        "\n",
        "Let the following solution exist uniquely. \n",
        "\n",
        "$$ \\hat \\theta_L = \\arg \\max_{\\theta \\in \\Theta} n^{-1} \\log f_X(X;\\theta) - \\lambda g(\\theta) $$\n",
        "\n",
        "Then then there exists linear subspace $H \\subset \\Theta$ \n",
        "such that $\\hat \\theta_L$ is the solution to the following optimization program. \n",
        "\n",
        "$$ \\hat \\theta_L = \\arg \\max_{\\theta \\in H} \\log f_X(X; \\theta) $$\n",
        "\n",
        "**PROOF:** \n",
        "\n",
        "Define $\\hat{\\mathcal{L}} := n^{-1} \\log f_X(X;\\theta) - \\lambda g(\\theta)$. \n",
        "Since $\\hat \\theta_L = \\arg \\max_{\\theta \\in \\Theta} \\hat{\\mathcal{L}}$ exists uniquely, \n",
        "and $\\log f_X, g$ are differentiable, so we are guaranteed that \n",
        "\n",
        "1. $\\nabla_\\theta \\hat{\\mathcal{L}} = 0$ and \n",
        "2. $\\nabla_\\lambda \\hat{\\mathcal{L}} = 0$. \n",
        "\n",
        "There exists $c = g(\\hat \\theta_L)$ \n",
        "and $\\mathcal{L} := n^{-1} \\log f_X(X;\\theta) - \\lambda( g(\\theta) - c )$ such that \n",
        "\n",
        "3. $\\nabla_\\theta \\hat{\\mathcal{L}} = \\nabla_\\theta \\mathcal{L} = 0$ and \n",
        "4. $\\nabla_\\lambda \\mathcal{L} = c$. \n",
        "\n",
        "By (4), $\\theta \\in g^{-1}(c)$. \n",
        "For $\\theta$ sufficiently near $\\hat \\theta_L$, $g^{-1}(\\theta) \\approx \\left( J_{g, \\hat \\theta_L} \\right)^{-1} \\left(\\theta - \\hat \\theta_L \\right)$ by the inverse function theorem (TODO: check calcs, and use Constant Rank Theorem instead). \n",
        "Define this local linear subspace as $H := \\left\\{ \\theta \\; : \\; \\eta \\; s.t. \\; \\exists \\theta \\; \\& \\; \\eta = \\left( J_{g, \\hat \\theta_L} \\right)^{-1} \\left(\\theta - \\hat \\theta_L \\right) \\right\\}$ or $H$-space. \n",
        "\n",
        "Since $\\nabla_\\theta \\hat{\\mathcal{L}} = 0$ also holds, \n",
        "$\\mathcal{L}$ is a Langrangian constraining $\\theta$ to linear subspace $H \\subset \\Theta$. \n",
        "\n",
        "$\\square$"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RESULT 1:** \n",
        "\n",
        "If $\\hat \\theta_{MLE}:= \\arg\\max_{\\theta \\in \\Theta} \\log f_X(X;\\theta) $, \n",
        "then $\\hat \\theta_L$ is an MLE. \n",
        "\n",
        "**PROOF:** \n",
        "\n",
        "Since $\\hat \\theta_{MLE}$ is an MLE, $f_X$ is a likelihood function. \n",
        "By LEMMA 1, the estimation program is constrained to well-defined linear subspace $H \\subset \\Theta$. \n",
        "Hence $\\hat \\theta_L$ is indeed an MLE. \n",
        "\n",
        "$\\square$ "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Clarify $J$. \n",
        "\n",
        "**COROLLARY 1:** \n",
        "\n",
        "Define $J := \\left[ \\partial \\theta_0 / \\partial \\eta_j \\right]$, \n",
        "$J^+$ to be the pseudo-inverse matrix, \n",
        "and $\\mathcal{I}_H :=  J^+ \\mathcal{I}_{\\theta_0} J^{+T}$\n",
        "\n",
        "If $X_i \\sim  f_X(x; \\theta_0)$ and $\\hat \\theta_L$ is sufficiently near $\\hat \\theta_{MLE}$ and $n$ sufficiently large, \n",
        "then $\\hat \\theta_L$ has a corresponding estimate $\\hat \\eta$ in the $H$-space basis,  \n",
        "and $\\sqrt{n} \\left( \\hat \\eta - J^+ \\theta_0 \\right) \\sim N(0, \\mathcal{I}_H )$. \n",
        "\n",
        "**PROOF:** \n",
        "\n",
        "$0 = \\nabla_\\theta \\log f_X(X; \\theta) $\n",
        "\n",
        "$\\approx \\nabla_\\theta \\log f_X(X; \\theta_0) - \\left( \\theta - \\theta_0 \\right)^T \\nabla_\\theta^T \\nabla_\\theta \\log f_X(X; \\theta_0) $ by Taylor expansion \n",
        "\n",
        "$ \\Rightarrow \\sqrt{n}^{-1} \\nabla_\\theta \\log f_X(X; \\theta_0) \\approx \\frac{\\sqrt{n}}{n} \\left( \\theta - \\theta_0 \\right)^T \\nabla_\\theta^T \\nabla_\\theta \\log f_X(X; \\theta_0) $\n",
        "\n",
        "$ \\Rightarrow N \\mathcal{I}_{\\theta_0}^{1/2} =_d \\sqrt{n} \\left( \\theta - \\theta_0 \\right)^T \\mathcal{I}_{\\theta_0} $ \n",
        "where $=_d$ is equivalence in distribution and $N \\sim N\\left(0, I_{\\dim(\\Theta)} \\right)$ \n",
        "\n",
        "Apply lemma 1 by parameterizing $H$-space via $\\theta \\gets J \\eta$.\n",
        "\n",
        "$ \\Rightarrow \\sqrt{n} \\left( J \\eta - \\theta_0 \\right) \\sim N(0, \\mathcal{I}_{\\theta_0}) $\n",
        "\n",
        "$ \\Rightarrow \\sqrt{n} \\left( \\eta - J^+ \\theta_0 \\right) \\sim N(0, J^+ \\mathcal{I}_{\\theta_0} J^{+T} ) $ \n",
        "\n",
        "$\\square$ \n",
        "\n",
        "Since $J^+$ is also a projection matrix, the distribution of $\\hat \\eta$ is equivalent to the $\\hat \\theta_{MLE}$ projection, \n",
        "$\\sqrt{n} J^+ \\left( \\hat \\theta_{MLE} - \\theta_0 \\right) \\sim N(0, \\mathcal{I}_H )$. \n",
        "\n",
        "**COROLLARY 2:** \n",
        "\n",
        "$\\text{rank}(J^+) > 0 \\Rightarrow \\mathcal{I}_H < \\mathcal{I}_{\\theta_0}$ \n",
        "\n",
        "**PROOF:** \n",
        "\n",
        "Since $\\mathcal{I}_{\\theta_0} > 0$ (positive semi-definite), there is matrix $A$ such that $\\mathcal{I}_{\\theta_0} = AA^T $. \n",
        "Since $J^+$ is a projection matrix, we have the following.\n",
        "\n",
        "$ \\mathcal{I}_{\\theta_0} = \\mathcal{I}_H + (I - J^+)AA^TJ^{+T} + J^+AA^T(I - J^+)^T + (I - J^+)AA^T(I - J^+)^T$ \n",
        "\n",
        "$ = \\mathcal{I}_H + 0 + 0 + (I - J^+)AA^T(I - J^+)^T $ by linear independence of complementrary projections \n",
        "\n",
        "Recognizing $(I - J^+)AA^T(I - J^+)^T > 0$ completes the proof. \n",
        "\n",
        "$\\square$ \n",
        "\n",
        "If $\\hat \\theta_L \\not= \\hat \\theta_{MLE}$ for all $n$, then $\\theta_0 \\not \\in H$, \n",
        "and we may interpret $H$ as the tangent space of a biased sub-manifold in $\\Theta$. \n",
        "However, since dimensional reductions cause $\\mathcal{I}_H < \\mathcal{I}_{\\theta_0}$, \n",
        "a sufficiently-unbiased manifold produces more-efficient estimators, \n",
        "particularly so if $\\theta_0 \\in \\lim_{n \\to \\infty} H$. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## result: elliptical sub-manifolds in $\\Theta$-space are memories\n",
        "\n",
        "Define a _memory_ as all information associated with samples $X_1, X_2, \\ldots, X_n$ under a model. \n",
        "Concretely, we'll use approximate MLE sufficient statistics $\\left( \\hat \\theta, \\hat{\\mathcal{I}} \\right)$.\n",
        "\n",
        "WLOG, assume $\\mathcal{I}_{\\theta_0} > 0$. \n",
        "If not, perfect correlations exist, so the model can be reparameterized to a lower dimension. \n",
        "$\\mathcal{I}_{\\theta_0} > 0 \\Rightarrow \\mathcal{I}_{\\theta_0} = AA^T \\; \\& \\; \\exists A^{-1}$. \n",
        "For all $B \\in \\mathbb{R}^{p \\times q}$ where $p = \\dim(\\Theta)$ and $q \\leq p$, \n",
        "there exists $J = A^{-1}B$ mapping from an unbiased, linear $H$-space to $\\Theta$-space, \n",
        "and $(\\theta - \\theta_0)^T AA^T (\\theta - \\theta_0) = (\\eta - \\eta_0)^T J^T AA^T J (\\eta - \\eta_0) = (\\eta - \\eta_0)^T BB^T (\\eta - \\eta_0)^T$.\n",
        "This is true for any choice of $B$, so covers all PSD matrices $BB^T \\geq 0$. \n",
        "\n",
        "Choosing $g(\\theta) = (\\theta - \\theta_0)^T BB^T (\\theta - \\theta_0)$ and applying lemma 1, \n",
        "we recognize the existence of a elliptical sub-manifold in $\\Theta$-space corresponding to Lagrangian-form regularizer $g(\\theta)$. \n",
        "Hence, there exists an MLE with $\\mathcal{I} = BB^T$ when estimation is constrained to a subspace with $J = A^{-1}B$. \n",
        "So, for every elliptical sub-manifold with Riemann metric $\\mathcal{I} = BB$, there exists another linear sub-manifold $H$ \n",
        "such that $\\arg\\max_{\\theta \\in H} \\log f_X(X;\\theta)$ has approximate sufficient statistics $(\\hat \\theta, BB^T)$. \n",
        "\n",
        "Since we interpret $(\\hat \\theta, BB^T)$ as a memory, \n",
        "every elliptical sub-manifold $(\\eta - \\eta_0)^T BB^T (\\eta - \\eta_0)^T$ \n",
        "has an associated memory. \n",
        "\n",
        "TODO: look for a 1-1 correspondence, tying each manifold to a unique memory. \n",
        "Example consideration: $(\\hat \\eta, BB^T)$ and $(\\eta - \\hat \\eta)^T BB^T (\\eta - \\hat \\eta)^T$. \n",
        "\n",
        "## likely decent definition for memory \n",
        "\n",
        "$(\\theta_H, H)$ for $\\theta_H \\in \\Theta \\subset \\mathbb{R}^p$ and smooth, $q$-dimensional sub-manifold $H \\subset \\Theta$\n",
        " with local tangent space $T_{\\theta_H}$ and associated total derivative $D_{\\theta_H} \\in \\mathbb{R}^{p \\times q}$, \n",
        " so that $\\hat \\theta_H = \\arg \\max_{\\theta \\in H} \\log f(X; \\theta) \\; \\& \\; X \\sim f(X; \\theta_H)$, \n",
        " and $\\mathcal{I}_{\\theta_H} = D_{\\theta_H}D_{\\theta_H}^T$. \n",
        "\n",
        " This $(\\theta_H, H)$ definition works, because it allows me to sneak-in other kinds of subspaces via Lagrangian constraints like \"$-\\lambda \\|\\hat{\\mathcal{I}}_\\theta \\|$\"."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## frontal lobe \n",
        "\n",
        "In humans, the frontal lobe operates on a longer-term timeline than Hippocampus-based short-term memory, \n",
        "but on a shorter horizon than remaining cortical regions, \n",
        "so functions as a mid-term horizon or _working space_ (TODO: CITE!). \n",
        "Observing miniaturization requirements on deep learning imposes parameter space restrictions, \n",
        "which deny infinite growth strategies like `net2net` (TODO: CITE!). \n",
        "Instead, we'll allocate dimensional parameter space to a flex capacity, \n",
        "so operating in similar capacity to a biological frontal lobe. \n",
        "Mathematically, we'll achieve this by all filling dimensional space with information, \n",
        "then periodically reducing information stored in the frontal lobe parameters. \n",
        "The net result is a temporary increase in dimensional space available to the model, \n",
        "which routinely moves information into long-term memory. \n",
        "This results in the following two-step optimization procedure.\n",
        "\n",
        "1. $ \\hat \\theta = \\arg \\max_\\theta \\log f_X(X; \\theta) $\n",
        "$ - n_A (\\theta - \\hat \\theta_A)^T \\hat{\\mathcal{I}}_{\\hat \\theta_A} (\\theta - \\hat \\theta_A) $\n",
        "2. $ \\hat \\theta = \\arg \\max_\\theta  \\log f_X(X; \\theta) $ \n",
        "$ - n_A (\\theta - \\hat \\theta_A)^T \\hat{\\mathcal{I}}_{\\hat \\theta_A} (\\theta - \\hat \\theta_A) $ \n",
        "$ - \\lambda \\| \\hat{\\mathcal{I}}_\\theta \\|_{FL} $\n",
        "\n",
        "Where $X$ has $n_B$ samples observed, and $\\hat \\theta_A$ was produced by memorizing $n_A$ samples. \n",
        "\n",
        "As we'll illustrate (TODO!), a frontal lobe is practically motivated in deep learning, \n",
        "because freezing all parameters in memory effectively consumes all dimensional space. \n",
        "By regularly relieving dimensional space, a necessary working space can be retained. \n",
        "\n",
        "Experimental design scratch: Repetitively add memorization tasks (ex. introducing more MNIST digits). \n",
        "- Control 1: No memorization. Expectation: catastrophic forgetting. \n",
        "- Control 2: Memorize, but do not include frontal lobe clearing. Expectation: limited memorization capacities in later iterations. \n",
        "- Experimental 1: Memorize with frontal lobe clearing. Expectation: improved memorization iterations. \n",
        "- Experimental X: Adjust Krylov dimension and frontal lobe hyper-parameter. \n",
        "  - Expectation 1: Higher Krylov dimensions increase memory space, but cost memory and computation. \n",
        "  - Expectation 2: $n_A$ may be a natural parameter for the frontal lobe. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## references \r\n",
        "\r\n",
        "\\[1\\] Kirkpatrick et al. (2017) \"Overcoming catastrophic forgetting in neural networks\", PNAS"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK V2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}