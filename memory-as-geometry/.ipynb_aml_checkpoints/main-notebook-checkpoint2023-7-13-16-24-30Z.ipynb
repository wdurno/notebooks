{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Memory as geometry\n",
        "\n",
        "Content:\n",
        "- Asymptotically, memories are equivalent to Lagrangian-form regularizers, and thus equivalent to a point on a parameter space manifold. \n",
        "- Proof of how all Lagrangian-form regularizers work.\n",
        "- Corollary: Asymptotic sufficient statistics exist universally. \n",
        "- Desirable geometric properties of parameter space manifolds. \n",
        "- Result: Data is the most-efficient regularizer. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definitions\n",
        "\n",
        "- $f_X(x; \\theta)$ is a density function, where $\\theta \\in \\Theta \\subset \\mathbb{R}^p$. \n",
        "- $\\theta_0$ is the _true value_ of $\\theta$, so that we expect $X_i \\sim_{iid} f_X(x, \\theta_0)$.\n",
        "- $\\hat \\theta_0$ is the maximum likelihood estiamte (MLE) of $\\theta_0$, so that $\\hat \\theta_0 = \\arg \\max_{\\theta \\in \\Theta} n^{-1} \\log f_X(X; \\theta) $. \n",
        "\n",
        "### Memory\n",
        "\n",
        "A memory is a series of observations $\\{X_1, X_2, \\ldots\\}$ such that $X_i \\sim_{iid} f_X(x; \\theta_0)$."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memories are equivalent to Lagrangian-form regularizers \n",
        "\n",
        "This equivalency proof is via cyclical implications, $A \\Rightarrow B \\Rightarrow C \\Rightarrow A$ where \n",
        "- $A$: the estimate is memory-regularized. \n",
        "- $B$: the estimate is Lagrangian-regularized. \n",
        "- $C$: the estimate is sub-manifold constrained. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memories are Lagrangian-form regularizers \n",
        "\n",
        "For fixed $\\lambda$ define $\\hat \\theta_\\lambda = \\arg\\max_{\\theta \\in \\Theta} \\log f_X(X;\\theta) - \\lambda g(\\theta)$ be a regularized estimate with $g(\\theta)$ a Lagrangian-form regularizer. \n",
        "Under mild regularity assumptions, maximum likelihood estimates (MLEs) have similar asymptotic behavior.\n",
        "\n",
        "$$ \\hat \\theta = \\arg\\max_{\\theta \\in \\Theta} \\log f_X(X;\\theta) \\to_{a.s.} \\arg\\max_{\\theta \\in \\Theta} \\mathbb{E}\\log f_X(X;\\theta_0) - n 2^{-1} (\\theta - \\theta_0)^T \\mathcal{I}_{\\theta_0} (\\theta - \\theta_0) \\text{ as } n \\to \\infty $$\n",
        "\n",
        "Recognizing this, we may take $n = n_A + n_B$ where $n_A$ becomes large.\n",
        "\n",
        "$$ \\log f_X(X;\\theta) = \\sum_{j=1}^{N_B} \\log f_X(X_j; \\theta) + \\sum_{i=1}^{n_A} \\log f_X(X_i; \\theta) = \\log f_X(X_B; \\theta) + \\log f_X(X_A; \\theta) \\to_{a.s.} \\log f_X(X_B; \\theta) - n_A 2^{-1} (\\theta - \\theta_0)^T \\mathcal{I}_{\\theta_0} (\\theta - \\theta_0) $$\n",
        "\n",
        "Applying MLE consistency and the Strong Law of Large Numbers, we further recognize \n",
        "- $\\hat \\theta_A \\to_{\\mathbb{P}} \\theta_0$ where $\\hat \\theta_A = \\arg\\max_{\\theta \\in \\Theta} \\log f_X(X_A; \\theta)$ by MLE consistency, and\n",
        "- $\\hat{\\mathcal{I}}_A \\to_{a.s.} \\mathcal{I}_{\\theta_0}$ where $\\hat{\\mathcal{I}}_A = n_A^{-1} \\left(\\nabla_\\theta \\log f_X(X_A;\\theta)\\right) \\left(\\nabla_\\theta \\log f_X(X_A;\\theta)\\right)^T$.\n",
        "\n",
        "Thus we are free to use the following approximation when $n_A$ large. (TODO check convergence mode!)\n",
        "\n",
        "$$ \\log f_X(X; \\theta) \\to_{\\mathbb{P}} \\log f_X(X_B; \\theta) - n_A 2^{-1} (\\theta - \\hat\\theta_A)^T \\hat{\\mathcal{I}}_A (\\theta - \\hat\\theta_A) $$\n",
        "\n",
        "Thus, for $n_A$ large, all information from the entire sample $X_A$ is stored within $\\left( \\hat\\theta_A , \\hat{\\mathcal{I}}_A \\right)$. \n",
        "So, we may rightfully refer to $n_A 2^{-1} (\\theta - \\hat\\theta_A)^T \\hat{\\mathcal{I}}_A (\\theta - \\hat\\theta_A)$ as the _memory term_.\n",
        "\n",
        "Define $\\hat\\theta_B = \\arg \\max_{\\theta \\in \\Theta} \\log f_X(X_B; \\theta) - \\lambda 2^{-1} (\\theta - \\hat\\theta_A)^T \\hat{\\mathcal{I}}_A (\\theta - \\hat\\theta_A)$ to be the _memory regularized MLE_.\n",
        "\n",
        "Thus, our memory term is a Lagrangian-form regularizer. \n",
        "\n",
        "#### Corollary: Universal asymptotic sufficient statistics \n",
        "\n",
        "The Fisher-Neyman definition of a sufficient statistic $T$ requires the likelihood factorize $f_X(X;\\theta) = h(X) g(T(X); \\theta)$. \n",
        "If we relax this factorization to $f_X(X;\\theta) \\to_{\\mathbb{P}} h(X) g(T(X); \\theta)$ as $n \\to \\infty$ or simply $f_X(X;\\theta) \\approx h(X) g(T(X); \\theta)$, \n",
        "then we may uncover that $\\left( \\hat\\theta(X) , \\hat{\\mathcal{I}}(X) \\right)$ is indeed a near-universal sufficient statistic at large sample sizes, \n",
        "only requiring modest regularity assumptions to apply. \n",
        "\n",
        "Recognizing that $ \\log f_X(X; \\theta) \\approx \\mathbb{E} \\log f_X(X; \\theta) - n 2^{-1} (\\theta - \\hat\\theta(X))^T \\hat{\\mathcal{I}}(X) (\\theta - \\hat\\theta(X)) $, \n",
        "we uncover $g(T(X), \\theta) = \\exp \\left[ c(\\theta) - T(X) \\right]$ and $h = 1$, \n",
        "where \n",
        "- $c(\\theta) = \\mathbb{E}\\log f_X(X; \\theta)$, \n",
        "- $T(X) = n 2^{-1} (\\theta - \\hat\\theta(X))^T \\hat{\\mathcal{I}}(X) (\\theta - \\hat\\theta(X))$, \n",
        "- $\\hat \\theta(X) = \\arg\\max_{\\theta \\in \\Theta} \\log f_X(X; \\theta)$, and \n",
        "- $\\hat{\\mathcal{I}}(X) = n^{-1} \\left(\\nabla_\\theta \\log f_X(X;\\theta)\\right) \\left(\\nabla_\\theta \\log f_X(X;\\theta)\\right)^T$."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lagrangian estimates are sub-manifold estimates \n",
        "\n",
        "Let a sub-manifold estimate $\\hat \\theta_H = \\arg \\max_{\\theta \\in H} \\log f_X(X; \\theta) $ where $H \\subset \\Theta$ is a submanifold of $\\Theta$. \n",
        "\n",
        "Under mild regularity assumptions, any Lagrangian-constrained estimates $\\hat \\theta_\\lambda$ is the solution point to\n",
        "$\\nabla_\\theta \\log f_X(X;\\theta) = \\lambda \\nabla_\\theta g(\\theta) $. \n",
        "While not yet explicitly the solution to a Lagrangian Multiplier program, \n",
        "notice that $\\nabla_\\theta \\log f_X(X;\\theta) = \\lambda \\nabla_\\theta g(\\theta) = \\lambda \\nabla_\\theta \\left(c - g(\\theta) \\right)$. \n",
        "Choosing $c = g\\left(\\hat \\theta_\\lambda \\right)$ causes $\\frac{\\partial}{\\partial \\lambda} ( \\log f_X(X;\\theta) - \\lambda (c - g(\\theta) ) )$. \n",
        "So, our estimate is implicitly the solution to a Lagrange Multiplier program $\\arg \\max_{\\theta \\in \\Theta} \\log f_X(X;\\theta) \\text{ s.t. } g(\\theta) = c$, \n",
        "which has Lagrangian $\\mathcal{L} = \\log f_X(X;\\theta) - \\lambda (c - g(\\theta) )$.\n",
        "\n",
        "Notice that $g(\\theta) = c$ constrains $\\theta$ to a subspace $H \\subset \\Theta$. \n",
        "Hence, for Lagrangian-constrained estimate $\\hat \\theta_\\lambda$ there exists submanifold $H$ such that the following holds.\n",
        "\n",
        "$$ \\hat \\theta_\\lambda = \\arg\\max_{\\theta \\in \\Theta} \\log f_X(X; \\theta) - \\lambda g(\\theta) = \\arg \\max_{\\theta \\in H} \\log f_X(X; \\theta) = \\hat\\theta_H $$"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sub-manifold estimates are memory regularized estimates\n",
        "\n",
        "Let $H \\subset \\Theta$ be a sub-manifold with locally constant rank Jacobians. \n",
        "Define $\\hat\\theta_H = \\arg\\max_{\\theta \\in H} \\log f_X(X; \\theta)$ to be a sub-manifold estimate. \n",
        "\n",
        "TODO "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Corollary: every point on a parameter space sub-manifold is a memory \n",
        "\n",
        "TODO "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OLD CONTENT BELOW \n",
        "\n",
        "#### Example: Information geometry informs data size requirements \n",
        "\n",
        "We've made great progress in large models by increasing sample size $n$ via advances in data collection and computation. \n",
        "The goal is to get our estimate $\\hat \\theta$ as close to truth $\\theta_0$ as possible. \n",
        "Asymptotically, maximum likelihood estimates (MLEs) will yield variance of $\\mathcal{I}_{\\theta_0}^{-1}/n$, \n",
        "which describes how close $\\hat \\theta$ is to $\\theta_0$. \n",
        "Thus, our journey toward powerful large models is essentially one of shrinking $\\mathcal{I}_{\\theta_0}^{-1}/n$.\n",
        "We control $n$, so it makes sense that we started there. \n",
        "We don't control $\\mathcal{I}_{\\theta_0}^{-1}$, but we should take advantage of its structure. \n",
        "Statistics' interest in geometry is in discovering low-dimensional, large or unbounded, approximating sub-manifolds with high curvature.\n",
        "\n",
        "$\\mathcal{I}_{\\theta_0}$ is a Riemann metric, so may have high-magnitude tangent-space dimensions thereby bestowing significant information or mutual information. \n",
        "In turn, this information makes our estimator variance $\\mathcal{I}_{\\theta_0}^{-1}/n$ smaller. \n",
        "If we discover high-information dimensions, we may over-weight them to have the same result as increasing $n$, at least for target dimensions. \n",
        "So, we'll replace the usual regularizing scalar $\\lambda$ with a function to and from information matrices. \n",
        "\n",
        "The following example illustrates these mechanisms in play, aiding estimation.\n",
        "\n",
        "$\\hat \\theta_A := \\arg \\max_{\\theta \\in \\Theta} \\log f_X(X_B; \\theta) - n_A (\\theta - \\hat \\theta_A)^T \\lambda \\left( \\hat{\\mathcal{I}}_{X_A} \\right) (\\theta - \\hat \\theta_A)$.\n",
        "\n",
        "$\\hat \\theta_B := \\arg \\max_{\\theta \\in \\Theta} \\log f_X(X_A; \\theta) $.\n",
        "\n",
        "$\\hat{\\mathcal{I}}_{X_A} := n_A^{-1} \\sum_{i=1}^{n_A} \\left( \\nabla_\\theta \\log f_X(X_A; \\theta) \\right) \\left( \\nabla_\\theta \\log f_X(X_A; \\theta) \\right)^T $\n",
        "$ \\to_{a.s.} \\mathcal{I}_{\\theta_0} \\text{ as } n \\to \\infty$\n",
        "\n",
        "$\\theta = (\\mu_1, \\mu_2, \\text{logit}(\\rho), \\log \\sigma, \\log \\tau)^T \\in \\Theta = \\mathbb{R}^p, \\; p = 5 $.\n",
        "\n",
        "$\\eta = (\\mu, \\mu, \\text{logit}(\\rho), \\log \\sigma, \\log \\tau)^T \\in H \\subset \\Theta$, so $H$ is the submanifold where $\\mu = \\mu_1 = \\mu_2$. \n",
        "\n",
        "$\\theta_0 = (\\mu_0, \\mu_0, \\text{logit}(\\rho_0), \\log \\sigma_0, \\log \\tau_0)^T \\in H \\text{ and } X \\sim N_2\\left( \\begin{bmatrix} \\mu_0 \\\\ \\mu_0 \\end{bmatrix}, \\begin{bmatrix} \\sigma_0^2 & \\sigma_0 \\tau_0 \\rho_0 \\\\ \\sigma_0 \\tau_0 \\rho_0 & \\tau_0^2 \\end{bmatrix} \\right)$\n",
        "\n",
        "$\\lambda : P \\to P$ where $P \\subset \\mathbb{R}^{p \\times p}$ is a set of positive semi-definite (PSD) matrices. \n",
        "\n",
        "Since $\\hat{\\mathcal{I}}_{X_A} \\to_{a.s.} \\mathcal{I}_{\\theta_0} $, we expect high-information $|\\mu_0 - \\hat \\mu_1| \\to 0$ as $\\lambda$ is chosen optimally."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## foundational definitions \n",
        "import numpy as np \n",
        "from scipy.stats import multivariate_normal\n",
        "from scipy.special import expit, logit\n",
        "from scipy.optimize import minimize, approx_fprime \n",
        "from numpy.linalg import inv, eig, eigh  \n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in subtract\")\n",
        "\n",
        "def sample_data(n, mu0=0., rho0=0., sigma0=1., tau0=1.): \n",
        "    'generates sample matrix x' \n",
        "    mean = np.array([mu0, mu0]) \n",
        "    cov = np.array([[sigma0*sigma0, sigma0*tau0*rho0], [sigma0*tau0*rho0, tau0*tau0]]) \n",
        "    return multivariate_normal.rvs(mean=mean, cov=cov, size=n) \n",
        "\n",
        "def unpack_theta(theta): \n",
        "    'unpack from (mu1, mu2, logit(rho*.5 + .5), log sigma, log tau) in R^5'\n",
        "    mu1 = theta[0] \n",
        "    mu2 = theta[1] \n",
        "    rho = 2.*expit(theta[2]) - 1. \n",
        "    sigma = np.exp(theta[3]) \n",
        "    tau = np.exp(theta[4]) \n",
        "    theta_vec = np.array([mu1, mu2, rho, sigma, tau]) \n",
        "    mean = np.array([mu1, mu2]) \n",
        "    cov = np.array([[sigma*sigma, sigma*tau*rho], [sigma*tau*rho, tau*tau]]) \n",
        "    return theta_vec, mean, cov \n",
        "\n",
        "def info_estimator(x, theta): \n",
        "    'estimate info matrix via averaged outer produts of log lik grads calculated from numerical stencils' \n",
        "    estimate = 0. \n",
        "    n = x.shape[0] \n",
        "    def log_lik(theta_vec, x=None): \n",
        "        _, mean, cov = unpack_theta(theta_vec) \n",
        "        g = multivariate_normal.logpdf(x=x, mean=mean, cov=cov, allow_singular=True) \n",
        "        return g \n",
        "    eps = [np.sqrt(np.finfo(float).eps)]*5 \n",
        "    for i in range(n): \n",
        "        grad = approx_fprime(theta, log_lik, eps, x[i,:]).reshape([-1,1]) \n",
        "        estimate += np.matmul(grad, np.transpose(grad)) \n",
        "        pass \n",
        "    return estimate / n\n",
        "\n",
        "def mle(x): \n",
        "    'x is a sampled matrix'\n",
        "    def loss(theta): \n",
        "        _, mean, cov = unpack_theta(theta) \n",
        "        l = multivariate_normal.logpdf(x=x, mean=mean, cov=cov, allow_singular=True) \n",
        "        l = sum(l) \n",
        "        return -l  \n",
        "    theta0 = [0., 0., 0., 0., 0.] \n",
        "    result = minimize(loss, theta0) \n",
        "    theta_A_estimate = result.x \n",
        "    inv_info_estimate = result.hess_inv \n",
        "    return theta_A_estimate, inv_info_estimate \n",
        "\n",
        "def memory_mle(x, info, theta_A): \n",
        "    '''\n",
        "    info is an information matrix, already adjusted by regularizer and sample size \n",
        "    theta_A is the memorized estimate\n",
        "    '''\n",
        "    def loss(theta): \n",
        "        theta_vec, mean, cov = unpack_theta(theta) \n",
        "        \n",
        "        memory_term = np.transpose(theta_vec - theta_A) \n",
        "        memory_term = np.matmul(memory_term, info) \n",
        "        memory_term = np.matmul(memory_term, theta_vec - theta_A) \n",
        "        l = multivariate_normal.logpdf(x=x, mean=mean, cov=cov, allow_singular=True) \n",
        "        l = sum(l) \n",
        "        l = l - memory_term \n",
        "        return -l \n",
        "    theta0 = [0., 0., 0., 0., 0.] \n",
        "    return minimize(loss, theta0).x \n",
        "\n",
        "print('Examples...') \n",
        "n_A = 30 \n",
        "n_B = 10 \n",
        "x_A = sample_data(n_A, rho0=-.99) \n",
        "x_B = sample_data(n_B, rho0=-.99) \n",
        "print(f'sample_data({n_A})[:3,]: {x_A[:3,].round(3)}') \n",
        "theta_A_estimate, inv_info_estimate = mle(x_A) \n",
        "print(f'initial estimate: {unpack_theta(theta_A_estimate)[0].round(3)}') \n",
        "info_estimate = info_estimator(x_A, theta_A_estimate) \n",
        "print(f'info estimate * inv estimate: {np.matmul(n_A*info_estimate, inv_info_estimate).round(3)}') \n",
        "print(f'info estimate: {info_estimate.round(3)}')\n",
        "e = eig(info_estimate)\n",
        "print(f'info eigenvals: {e[0].round(3)}')\n",
        "print(f'info eigenvecs: {e[1].round(3)}')\n",
        "theta_B_estimate = memory_mle(x_B, info_estimate, theta_A_estimate) \n",
        "print(f'updated estimate: {unpack_theta(theta_B_estimate)[0].round(3)}') "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Examples...\nsample_data(30)[:3,]: [[ 0.965 -0.874]\n [ 0.587 -0.347]\n [ 0.581 -0.661]]\ninitial estimate: [-0.061  0.075 -0.988  0.867  0.872]\ninfo estimate * inv estimate: [[ 1.865 -0.632  3.379  0.597  0.774]\n [ 1.122  0.09   2.909  0.799  1.011]\n [ 0.137 -0.097  0.606  0.198  0.222]\n [ 0.59  -0.565  0.756  0.036 -0.914]\n [-0.556  0.667  0.546  0.568  1.598]]\ninfo estimate: [[ 55.743  54.779   0.947   5.031  -0.669]\n [ 54.779  55.148   0.923   2.541   2.111]\n [  0.947   0.923   0.547   0.65    0.437]\n [  5.031   2.541   0.65   41.197 -40.221]\n [ -0.669   2.111   0.437 -40.221  42.614]]\ninfo eigenvals: [110.753  81.896   1.879   0.583   0.138]\ninfo eigenvecs: [[ 0.706 -0.052  0.09   0.692 -0.106]\n [ 0.699 -0.097 -0.03  -0.698  0.119]\n [ 0.012 -0.    -0.466  0.18   0.866]\n [ 0.103  0.692 -0.63  -0.023 -0.335]\n [-0.046 -0.713 -0.614  0.022 -0.334]]\nupdated estimate: [-0.039 -0.01  -0.966  0.589  0.59 ]\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1689172636060
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When $\\sigma = \\tau = 1$, $\\mu$'s information is $2(1-\\rho)/(1 - \\rho^2)$. This illustrates the effect of high-information dimensions. \n",
        "- As $\\rho \\to -1$, information becomes infinite, so a single sample reveals $\\mu_0$ exactly via $\\hat \\mu_0 = (X_1 + X_2)/2$. \n",
        "- If $\\rho = 0$, information is 2, so we need only half the sample size. \n",
        "- As $\\rho \\to 1$, information becomes 1, so multidimensionality lends no benefit. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## plot of mu's info \n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "rho_val_list = np.linspace(-.75, .75).tolist() \n",
        "info = lambda rho : 2.*(1. - rho)/(1. - rho*rho) \n",
        "info_val_list = [info(rho) for rho in rho_val_list] \n",
        "\n",
        "plt.plot(rho_val_list, info_val_list)\n",
        "plt.ylabel('mu info')\n",
        "plt.xlabel('rho')\n",
        "plt.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1689172637570
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we'll take $\\rho \\approx -1$ to purposefully create substantial mutual information between parameters, \n",
        "thereby making low-dimensional representations accurate. \n",
        "Since the memory regularizer is already asymptotically optimal for variance reduction by the Cramer-Rao bound, \n",
        "naive scaling by $\\lambda$ can only cause harm. \n",
        "We will reduce $\\mathcal{I}_{\\theta_0}^{-1}/n$ by reparameterizing to a (nearly) lower-dimensional representation. \n",
        "This parameterization will be informed by the initial dataset $X_A$ and allow for more-efficient application of $X_B$. \n",
        "We'll replace our memory regularizer $n_A \\lambda (\\theta - \\hat \\theta_A)^T \\hat{\\mathcal{I}}_{X_A} (\\theta - \\hat \\theta_T)$ \n",
        "with $n_A \\lambda (\\theta - \\hat \\theta_A)^T (JJ^T + \\Lambda) (\\theta - \\hat \\theta_T)$, where \n",
        "- $J$ is a subset of eigenvectors, scaled by their respective eigenvalues.\n",
        "- $\\Lambda = \\text{diag}(\\hat{\\mathcal{I}}_{X_A} - JJ^T)$. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO Approximate null spaces REALLY doesn't work. I think I've tried this with MNIST too and got the same result. \n",
        "\n",
        "N=10 \n",
        "n_A=50 \n",
        "n_B=10 \n",
        "rho0=-.999 \n",
        "\n",
        "def lmbda(info, weight=10., cutoff=None): \n",
        "    'High-info dimensions are known apriori, so the info matrix will not be investigated, only modified' \n",
        "    d = np.diag(info).tolist() \n",
        "    if cutoff is not None: \n",
        "        d[1] = 0. \n",
        "        d[4] = 0. \n",
        "        #for i in range(len(d)): \n",
        "        #    if np.absolute(d[i]) < cutoff: \n",
        "        #        d[i] = 0. \n",
        "        #        pass \n",
        "        pass \n",
        "    return weight*np.diag(np.diag(info))\n",
        "\n",
        "def get_error_experimental(n_A, n_B, rho0, lmbda_w, cutoff=None): \n",
        "    x_A = sample_data(n_A, rho0=-0.99) \n",
        "    x_B = sample_data(n_B, rho0=-0.9) \n",
        "    theta_A_estimate, _ = mle(x_A) \n",
        "    info_estimate = info_estimator(x_A, theta_A_estimate) \n",
        "    theta_B_estimate = memory_mle(x_B, n_A*lmbda(info_estimate, weight=lmbda_w, cutoff=cutoff), theta_A_estimate) \n",
        "    return float(np.absolute(unpack_theta(theta_B_estimate)[0][2])) ## rho \n",
        "\n",
        "def get_error_control(n_A, n_B, rho0): \n",
        "    x_A = sample_data(n_A, rho0=-0.99) \n",
        "    x_B = sample_data(n_B, rho0=-0.9) \n",
        "    x_AB = np.concatenate((x_A, x_B)) \n",
        "    theta_AB_estimate, _ = mle(x_AB) \n",
        "    return float(np.absolute(unpack_theta(theta_AB_estimate)[0][2])) \n",
        "\n",
        "def get_avg_err(N, n_A, n_B, rho0, lmbda_w): \n",
        "    experimental_1_error = sum([get_error_experimental(n_A, n_B, rho0=rho0, lmbda_w=lmbda_w, cutoff=None) for _ in range(N)])/N \n",
        "    experimental_2_error = sum([get_error_experimental(n_A, n_B, rho0=rho0, lmbda_w=lmbda_w, cutoff=1.) for _ in range(N)])/N \n",
        "    control_error = sum([get_error_control(n_A, n_B, rho0=rho0) for _ in range(N)])/N \n",
        "    return experimental_1_error, experimental_2_error, control_error \n",
        "\n",
        "lmbda_w_list = np.linspace(0.01, 1000.).tolist() \n",
        "experimental_1_error_list = [] \n",
        "experimental_2_error_list = []  \n",
        "control_error_list = [] \n",
        "for lmbda_w in lmbda_w_list: \n",
        "    experimental_1_error, experimental_2_error, control_error = get_avg_err(N, n_A, n_B, rho0=rho0, lmbda_w=lmbda_w) \n",
        "    experimental_1_error_list.append(experimental_1_error) \n",
        "    experimental_2_error_list.append(experimental_2_error) \n",
        "    control_error_list.append(control_error) \n",
        "    pass \n",
        "\n",
        "plt.plot(lmbda_w_list, experimental_1_error_list, label='experimental 1') \n",
        "plt.plot(lmbda_w_list, experimental_2_error_list, label='experimental 2') \n",
        "plt.plot(lmbda_w_list, control_error_list, label='control') \n",
        "plt.plot([0,1000], [.9,.9], label='actual') \n",
        "plt.ylabel('average absolute error') \n",
        "plt.xlabel('lambda weight') \n",
        "plt.legend() \n",
        "plt.show() "
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAAsTAAALEwEAmpwYAABeNUlEQVR4nO3dd1RURxvA4d/Qe7cjghW7RqyJvScaa2JMs8cU040xpvdE0z6NJmqixpJqS2LsvYtgVyyoCCgoTXrdne+PuyDKAouyIjDPORzZ23buLt733invCCkliqIoinIri7IugKIoinJvUgFCURRFMUoFCEVRFMUoFSAURVEUo1SAUBRFUYyyKusClBYvLy/p6+tb1sVQFEUpV4KDg2OllFWMraswAcLX15egoKCyLoaiKEq5IoS4VNg6VcWkKIqiGKUChKIoimKUChCKoiiKUSpAKIqiKEapAKEoiqIYpQKEoiiKYpQKEIqiKIpRKkAoimJ2x2OOszNyZ1kXQykhFSAURTGrI9eOMHbDWF7Y8gJ/nvmzrIujlECFGUmt3LksXRb7ruxjc/hmajnVYkLzCVhaWN728dKy0/jx6I8kZSVhb2WPvZU9dlZ22FnaYW9tT+dananuWN2kY8VnxHPk2hE61OiAg7XDbZepPEvMTORC4gVCr4dy4foFYtJjGNNsDE09m97RcaNTozmbcJbOtTojhCil0mpCE0J5YcsLVHWoSm2X2ny8/2OklIzwH1Gq71NhJEXBvK4waDY06F3WpVEBorLL1mWzP2o/68PWsy18G8nZyThaO5KancqJ2BNM7zL9ti7IOr2ON3e+yY7IHXjZe5Gek056Tjo6qcvbxsvei/m951PfvX6Rx4pKiWL8xvGEJ4fjYOVArzq9GFB3AO2qt7ujAFYenE04yzdB33A24Swx6TF5y+2t7LESVuy9spf5febfdpA4HnOcSVsnEZ8Rz8C6A3m347vYW9kXu198RjyuNq5Ffv5RKVFM3DwRG0sb5vaeS1WHqry2/TU+OfAJevSM9B95W2Uu76SUJGUlkZqdSg3HGjcH5RMrIOUqbP8C6veCUg7YJSUqypSjAQEB8l7IxSSlLPW7sFw6vY5lIcs4k3CGh/weokPNDliIktcSXkm5wsHogwRGB7I9YjtJWUk4WzvTw6cHfX370qFmB1aeXcnngZ/TwL0Bs3rMMvlOH7TP4PPAz/nt9G9Maz/tpgtBti6bdF064UnhvLT1JbL12fzY+8dCL3DhSeGM3zielKwU3mj7BkdijrAxbCMp2SlUdajKQ34PMbDeQBq4Nyjx52BOpfF3EJMWw8j/RpKtz+aBWg9Q360+9dzqUc+tHjUcaxCVGsXY9WNJyU5hfp/5NPFsUqLjbwnfwtSdU/G096SXTy8Wn1pMfff6fNftO3xcfIzuE50azTdB37AubB0N3BswOWAynWp2KrBdQkYCT697mrj0OBb2W0gjj0aA9pT6+vbX2R65nbfavcXjjR8v+QdTiFXnVpGWk0Zf37542XuV2nFN8d+F/wi+GoyVhRXWFtZYWVjl/Z6ly+Jq2lXtJ1X7Nz0nHQAPOw/aVGtDm2ptCKgWQIMVz2ERdRT0OTBmPdTpaPayCyGCpZQBRtepAHFnci+2B6MPEnQ1iOuZ13mmxTM81fgprC2tS/V9pu2eRvDVYOyt7EnPSaeWUy2GNhjK4PqDqepQ1eh+Or2OKylXCL4WzMHogwRfDeZyymUAXG1d6VyrM319+9KpZidsLG1u2nf35d1M3jEZRytHZvWcZfIF6JeTv/BV0FeMajKKyW0nF7pdRFIE4zeOJykriTm95tC6auub1ocmhDJh0wR0eh1ze8+lsWdjADJyMtgeuZ0159ew5/IecmQOY5qN4aXWL2FlUfRDcUJGAtMPTsfR2pHxzceXKPAZk5qdysXEiwV+IlMiaVu9LS/f9zL+Hv4lPm56Tjpj1o/hQuIFlvRfkneBvdXllMuMWT+G1OxUfurzU95nVJwlp5Yw4+AMmns1Z2aPmXjae7L78m6m7pqKTq/j0wc+pYdPj5vKs+jkIhYcX4BEMrTBUHZG7uRyymU61+rM5IDJ1HWrC2hVi+M2jOPc9XPM7T2XNtXa3PTe2bpsXt/xOtsitjG13VSeaPxEiT+fWy0+uZgZQTMAsBAWtK/enofqPkRPn5442Tjd8fGLkvv37mzjjECQrc8mR59Dtj47rzxV7KtQ3bE61Ryq5f1rZ2XH0ZijBEUHcSX1CgAuOh1tnHwYFXmONjXaw8hfzVp2UAGiVGTrsrmccpmI5AjCk8MJiQsh6GpQ3sXWzdaNttXbkpGTwa7Lu/Bz9eOtdm/RsabxO4D0nHQ2XdrEhrAN+Lr4MrTBUOq51SuwnZSSNRfW8NmBz5BI3mr3Fv39+rMlfAsrzq7gQPQBLIUlXb270qtOL+Iz4olIjiAyJZLI5Egup1wmR58DgLutu3anUj1Au1txb1DsE8jZhLNM2jKJ65nX+bLzl3T36V7k9psubeL17a/Tq04vvur6VbHHj06NZvzG8VxLu8bMHjPpUKMDAKfiTjFx00SsLayZ32e+0c8GtKqO7w9/z19n/6J99fZM7zodDzsPo9sejD7I1F1TSchIQCKxwIJHGz3KuObjbuuOc+W5lXyy/5O8C4GlsKS2c238XP2o6lCVdRfXkZSVRH+//rzY6kVqu9Q26bh6qWfyjslsvrSZmT1m0q12tyK3j0yOZMyGMaTnpPNTn5+KDEg6vY7pB6fz6+lf6enTk887f35TldLllMu8tv01TsWdYlyzcUxqPYnN4Zv5JugbolKj6Ovbl9favEZNp5pk6bL4NeRX5h6bS3pOOo80fIQJLSbwzu53CIwO5Ntu3xb695Kty2byjslsjdjK5IDJPN3k6dt+4lpxdgUf7PuA3nV682zLZ1l/cT1rL67lcsplbC1t6erdlXbV2910/Nzrnq2VLY09GlPPrV6xNxfG5AaHPnX68EWXL7C2uHFTKKVEJ3UIRLFVoVdSrhC86zOCzv/HLi8fYjLjeTAlldcH/UZV7/YlLldJqABRBCklKdkpxKXHEZseS1xGXN7v8RnxeUEhKjUKvdTn7ZcbEAKqBdC2elvqudXLuxjujNzJF4FfEJEcQe86vZnSdkreneqZ+DOsOLeCNefXkJydTA3HGsSkx5Cjz6FFlRYMqT+E/n79cbR2JDEzkY/3f8yGsA20rtqazx74DG9n75vKfynpEivOreDv0L+Jz4gHwNnaGW9nb2o71877aVmlJXXd6t5WlVRseiwvbnmRk3Eneb7V8wxvONzoBfXItSOM3zgefw9/furzE3ZWdiYff8LGCYQnhfNNt29wtXXl+c3P42zjzE99fjLpwro6dDUf7/sYD3sPvu32Lc28muWt0+l1zD02l7nH5lLbuTYzuszA1daVucfm8nfo39hY2jDSfyRjmo7Bzc6t2PeSUjLn6Bx+PPojHWt0ZIT/CPxc/ajtVPump8akrCQWnVjEklNLyNHnMKzhMJ5t+WyxwWjmoZnMPz6fyQGTGdV0VLHlAYhIjmDshrGk56Tzc5+fjT5xpGWn8ebON9keuZ2nmzzNa21eM3rhytRl8kXgFyw/uxxPO0/iMuJo5N6Iqe2mElA9AJ1e8sP2UIbe501NN3viM+KZc2QOy88uRyLRSz0fdfqIIQ2GFFnmbH02U3ZMYXP4ZjrV7MS09tOo41LHpPPNtf7ieqbsnEKnWp2Y2X1m3lOwlJKjMUdZe3EtG8I25P3fKIydpR1NPJvQ1Kspzb2a09yreYH/a7cqKjjclnndAEH62LX8FDyTRSFLsBJWPNvmZZ5s/GSp1kjkpwJEEa6lXaPnXz0LLLcUlrjbuVPDsUbeRdbHxSfvd087zyLveDJ1mSw6sYifjv+EEIJhDYZxNOYox2OPY2NhQ686vRjecDgB1QJIyEzg3/P/surcKs4nnsfeyp6ePj0JjA4kPj2e51s9z9hmY4u8C8nWZXMh8QLVHavjauta4s+hOOk56by9+202XdqEQNCySkt6+PSgp09PfFx8CE8K58m1T+Js48zSB5fibudeouNfz7jOs5uf5Uz8GawtranqUJX5vedTw6mGycc4GXeS17a9Rkx6DG+3f5thDYcRnRrN1F1TCb4azMC6A3m7w9s4Wjvm7XMp6RI/HP2BtRfW4mDtwFNNnuLJxk8W+hlm67P5aN9HrA5dzeD6g3mv43vFXhhi0mKYe2wuK86uwNrSmmENhjGkwRAaujcssO0/5//h7d1vM6zBMN7v+H6J7qojkiIYs2EMmbpMBtYbSHxGPHHpccRlxBGfHk9CZgIAU9tNNamBeNW5VSw+tZiR/iMZ1mBY3t/f3vOxPD7/AI+08WbGIy3ztj9//Tw/HP2BNtXamNwArdPr+P3M73x/+HuydFmMaz6Occ3HYWtpW+y+OyJ28Mq2V2hRpQU/9v6x0Mb1HH0OcelxCCEQaJ9n7ueakpXCqbhTHI89zonYE4TEh5CpywTgPs+mPNl8HN1rdy/wdFHqwSH+AsxsDb0/hvtfAiBi1QSmR+9gu4Mtvi6+vNXuLTrVKtjec6dUgChCjj6HZSHL8LDzwMveC097T7zsvXCzdbutu+1bXU65zPTA6WyN2Eo913oMaziMgXUHGr1TlVJyLPYYq86tYt3FdVRzrMbnnT+/426MpUVKybnr59gSvoVt4dsIiQ8BoL5bfdKy00jLSWPZg8sKbeAsTkpWCq9seyWvTeJ2qn2uZ1xnys4p7IvaR+86vQmMDiRLl8U7Hd7h4XoPF7pfaEIos4/MZnP4ZhytHXnc/3GeavLUTYEuNTuV17e/zp4re3iu5XM81/K5El3Aw5PCmX1kNhsvbSRHn0NTz6YMrj+Y/n79cbV1JfhqMOM3jqdN1Tb80PuH27rohCeF89zm57iWdg1Pe0887TzxsPfA084TT3tPOtboSEB1o9cCk73/9wl+2XcJGysL9k3tgadT8Rfz4sSkxTDj4AzWha3Dx9mHl1pO4WxYTcY84IeTbcGqn8CoQJ7b/BwN3BvwU5+fSq2dIVufTejyUey/vJM/XN25bAk1HWvyeOPHGdJgCC42LiULDmc3wMWd0OeTonsk7foGtnwIrxwHN8P/n9hz8H0AO9s9xZcZFwlPDqd99fY81eQpOnt3LpXrE6gAcU9IyEjAzdbN5AtKtj4bK2Flth5RpeFKyhW2hm9la8RWQhNCmdljJq2qtrqjY+b+Pd7Jeev0OmYfmc384/Px9/BnRpcZ+Lr6mrTvmfgzzDs2j02XNmFnZcdjjR7j6aZPI6XkhS0vcDbhLO91fI+hDYbedvniM+JZe2Etq0NXcybhDDYWNnT36c6BqAO42bqx9MGld/QUWBqfYWH0ekmnL7bi5mDN6ehk3ujbiBe6F91NuST2XtnLp/s/Izz5EtlJTelWJ4B+TerjYeeBh50H7nbuRKVE8fyW56nlVIuFfReaVC1osuPLYcU4aDkS3em1bHd1Z4lPU4LjjmNvZU+76u3YEbnDtOCQfBVmt4WMRHjkF2g6uPBtf3wArOxg/Oabl/82EsL3k/XSYX49v5olIUu4lnYNH2cfHm/8OIPrD77pifh2qAChVEphiWHUdKpZoHeWKc5fP8+8Y/NYH7YeGwsbnGycSM1O5euuX9PZu3OplTEkLoTVoav57+J/CARLH1xa4nr4u+lIxHUGz97D14+0ZNXhy4ReS2HXm92xtiy9pAyzt4Xwv+B52HruAotMo9vUdq7NL/1+oYqD0amUb8/1CPjhfqjSUOtiGnUEFg8G52qEDP4fSy+tY93FdfTw6cHnnT8v/glv+TgI+QdcagESXggEKyNPW7Gh8H0b6PsZdHzh5nWX9sLC/vDgV9BuAtn6bLZc2sKSkCUcizmGk7UTQxsMZaT/yGLbTAqjAoSi3KawxDB+Ov4Tx2KP8fkDn9PUyzzVfVm6LDJ1mTjbOJvl+KXly/WnmbfzAsHv9CIoLIHxi4OY/fh9PNTC9LaiohwKT+DRH/fRu0k1ujeqwpRVQcx6oj4+VSUJGQkkZCSQkp1Cnzp9qOZYrVTeEwC9HhY/DJcPwXO7wUPrskv4flgyFFy9YfR/pNk6Ym9lX/zT2bnNsGwYdHsLvANg6bCbAsC1pAw2nrrKyHY+WO76CrZ9Aq+eAtdaNx9HSvipJ6TFw4vBkK8d8ljMMZaGLGVT2CbquNRh1aBVt/XUWFSAUCOpFaUIvq6+fPLAJ2Z/HxtLm9t60rmbpJRsOBFNx7qeuDnY0N2/Kj4eDizae7FUAkRiejYv/XaYai52fDGsBTaWFnz8XwhbT+j5dkSrOz+Bouz7HsJ2wcPf3wgOAD4d4Ik/YelwWDwIh1H/QnGZBbLS4L/XwLMBPPCq9tRQryfsmA4tR5Jq6cLohQc5FZVENRc7ep9cBbU7FAwOoLVbdHoJ/hoFp/+DJjfa0VpUacH0KtOJbhPN1bSrZqlSVMn6FKUcklISei2ZUqkBOLkK1ryq3a0WIfRaChdiU+nbVLtzt7QQPN2xDgfDEjhxOfGOiiClZOqKY0QnZjB7eD1co/dhf3wJr9aLZufxUK6nZd3R8YsUfRy2fAT+A6D1kwXX+z4Aj/8O8edh8SDtbr4oO76E65dg4Hc3qpT6fAKZSeh3TOel3w5z5moyTrZW7DuwF66dhKZFdAluPBDc6sDemUa/o+qO1WlZpaWRHe+ceoJQ7rprSRmMXniQh1rUKNUGzspCSsm3m88xc8s53nmoMeM71y1+p8KE7YYV47XUDi1GaHfMhdhwMhqAPk1vjD5/JKA232w6y6K9YXz1SOEXqajEdK4lZdLC2/XGna5eB8nREBfKoQPbePDMPr5wvYzr0oi8/cYCY60gaZY3+N4HNVpC9RZg6wS6bK3ceh3oc9hzLpoLjq3o364pXqb2rMpOhxUTwMEDBs4svKdR3W7w2DKt0XhuFxg6D+rc6HK66nAkGdl6RtROwmLf99DqSS2w5KrWBFo/iQycT2hGAz4Y1Ivz11JwO/gN0lIgmgwqvIwWltBxEqx7A86sg0b971qOJhUglLsqMS2bp34O5MzVZCLi0xjVyddoN0bFOCkl3246y8ytoTjZWvHjjvM83t4HB5vb+AzjzsMfT2pVKomX4civRQaI9Sejae3jRjWrNJjTA3q8g6v/gwy7z5s/giJ4q7+/0S6v564m89a8FXTO3M4VmwSaOCZSQ8ZinRaFMIzybwP42FTDpU47qDkaarQCz3oQf57FK//BNzuUztHHESH/FFq++4Eq+lp02fw5nRvXYkTb2nRpUAWrohrQN38IMSHw5Apw9Cz686rfS2u8XjEOFj0EnV+Hrm+y52Iir/15FKSeAJdPqWfrikWfjwvs/qfLKB7S/8EP1f6lSYdxHAlPwDFoHzEebajqUkwVXesnYNfX8PtIcKoG9Xpo1Vb1uoOj+fJOqf+Zyl2TlpXDmEWBXIxNZUq/Rkxff4blQRGMvt+vrIt2W/R6yfaz1wiLTeNqUgZRiRlEJ2VwNSmD2ORM3n6oCY+3v70xIcZIKflm01lmbQ1lREBthrXx5tG5+1i6/xLPdDGeiiS/raevAtDDvxqkJ8CvIwABj/+h1Y+fXAX9vwTrggPOIhPSOHE5ibf6+8PhJVq1yL8vQ52OjOpUhyX7L/H7wYgCT4TnribzzLzN/Kn7AE+rRK4LTy4kenBY1ibFrh1VfeuzOcqBw9k+/PbKQ4hbA4yHH9bdGvL0yuOseLITbaoKuBYCuiywsAILK7IkvPznCWpmXuTdnFn86LuDV8P6s+HkVaq52DK8jTePt69DLbdbzuvcZjjwA7SbqF38TeHdBp7dBevehJ0zyD63henXxlGvSm0+qXWABiEhvM0kOoZmMKDFjd22nb7G1A1Xsak2ksHXF8GlfbS0c0FYXGaBbghji3tfG0d4bo82ruL8Fu3fo79p31+NltB4AHR5w7RzKAHVBqHcFVk5eiYuCeZIxHVmjmzF893q06q2G7/su4ReX/560qVk5vDMkiDGLgriozWnWLg3jCMR10FCC283arrZ893ms2Tm6Io9limklHy9UQsOj7WtzedDm9POz4PODbyYu+MCaVk5Re5/KS6V55cdYuqK4+izs+DPUZAQBiOWak8QrR6HzCStIdSIDSe14NK3cRU4+DN41oe0ONj8AfWrOtO5gRdL9l0iW3cjHc3Zq8mMnL+f1/WL8BJJWDyzDY93Q6k/dTeZD89lfbWJPBvSkr+uN+DDx7oUWi00sGVNHG0s+T0wHOzdtAyndbuC7/3g057ZZ91ZF1edBx55CZo/QpfoxewfX5Mfn7yPJjVc+GH7eQZ9v5ukjOwbB02MhJUToGoT6P1hib4LbJ1h8Bz0wxaSFX2GX3Ne59fmh+lwYRZp3g9wwqs/k349zKt/HCEpI5tTV5KY9OshGtdwofe4j8C5Bmx8G3FiJXos+OFaE65cTy/2bTeG5XC8ygAYvgDeCIUJW6H721pAv3y4ZOdgIvUEoZidTi959c8j7DoXy5fDmtOvmfY4PeZ+X17+/Qjbz17T7mrLifC4NMYvPsj5mFTeG9CEwa1r4e5gfVMvkj2hsTzx0wFWBF++46cIKSVfbTzD7G3nGdmuNp8Obo6FAI79yWc1LrHsQignfttCu9pO2p21LgeqNwf/h8DOBSklb608Tka2nozsDOKWv0yViztg0BztIgtQ5wFw9dGqmZoPL1CGDSej8a/ujO/1fVoD7PAFWpfQfd9Di8cY3aku434JYsPJaAa0qKkFh3n76cIhBsjt2t1tzVYAuDnYMKKtDyPa+hCbkklMciaNa7gUev5OtlY83Komqw9f4d2BTXCxuzH+4OzVZOZsD2VQq5p0b1QVvL+A0C1Yr3mJfuM20q9ZDQ6HJzD0h73M2nKOtx9qAjlZ8NdorQ3j0SUFnpjSs3Qs3hfGg81rUNuj8B5L/4tuxl/pn/N3zV+ouud9sLTFYchMVrj58f22UGZtDSXwYjx6KXG2s+bnUW1xdLaDHu/C38/D1ZNkeXckJtSNv49c4bluhT8FnricyMSlwQhg3AN+vNa7Efa12kCtNtD1Da2brhmoJwjFJDk6PR/8c5Ihc/aQnP9OrBhSSt79+wT/HYti2oP+jGh742L5YPMaVHOxZeGeMDOU2Dz2no/l4dm7uZqUyeKx7Rj7gB8ejjYFuhh2qudJy9pu/LjjPDm62//Pe3Nw8NGCg4WA0M2wcgK1Az9hqvXvtLv4A3LnV7D/RwhaAKufhRn14fcn2PfPfA6dv8Kb/fwZZ7WBKmd+hftf0eq1c1lYQMsRcGGbNqtZPrEpmRwMi9capwPng1N18B8I3adpQeXfl+lez5U6ng4s2hOWFxxcRRoz7BZod+mFVH94OdkWGRxyPdbWh/RsHX8fuZK3TKeXvLniGE62Vrw3wJCK3tEL+k+Hy0EQOA+A1j7uPNqmNov2hnExNhU2vQeRB2HQLPAq2Eniy/Wn+Xzdafp9t5NfD4Qb7Sm2JzSWmVvP0fG+VlSZtAn6fQlD54JnPawsLXilV0OWP9sRa0tBYno2P40KoLqrIXlly8e0AJ6TgV2r4bSp486qw5GF9kiTUvLZ2hBc7a0Z0bY283ddpP//drL/QtyNjSzMcylXAUIpVlJGNmMWHWSRoRrl/X9OmrzvVxvP8OuBcJ7rVq9APbm1pQVPdajDrnOxnLuaXNrFLrFD4Qn8GRTBicuJZOUUvKgv2X+Jp38OxMvJlr9fuJ/76xfeOCgyEpnUpQ7h8Wn8dzyq0O2K82dQRL7g0EwLDgDBi8DBC6Zc5MjTp6mbsZQfugXBu9fg7SgYtxkCxqKLOEinw29wxP45nr3yNm9bLWWXZXtkz/cKvlnLkSD1cOyPmxZvPnUVKWFg7QwtMLUZDVY2Wr34gG8g9gwW+2bydEdfgi4lMPyHvVhaCFbX/w+rtBht+kxjI4hLoIW3K01quPBbvgv2kn1hHA6/znsDm9zcON58ODToo3VdTbgEwOS+jbC1smT9nz9o7Q7tnzPatXTv+VgW7Q1j6H21aFnbjWmrjjNq4UGiEzPytrmWnMHLvx+hXhUnPh7cVOtl1OHZAsdr7ePO+le6sHNKd5rVypc6xcISHvwavNtBk8EMbl2Ls1dTCIky/n9g+5kY9p6P4+WeDfh8aAt+m9ABCTw2bz/vrD5eohu2klIBQilSRHwaw3/Yy77zcXw5rDkv9WjAykOX+efolWL3/WVvWN7FbUpf4xPejGzng42VBQv3hpVyyU13JOI6Ty8IZOicvUxZfowBs3bT7P0NPPz9bt5aeZyVOw+xbMFM3l19nC4Nq7Dq+U74ehWR/ybqGHzThF6b+vKu63qWbjlUeDuLlFo6hb8naY2++e4ir6dl8cW607Tz87g5OCRfhbPrtXYDBw9a1a1Bl0bVmLfzAimZOVoXyNptof8XTKq2lCd175LVZDgi4gDxLo15JnUioTFpBcviWQ9qt9eqmfKVY/3JaGp72FP/0h/axa3N6Bv7NOgNzYbBzhmM8EvH0cYSO2tL/u6bhsvpP+CBV6DWfaZ/GYUQQjCyXW1ORSVx/HIil6+nM33DGbo0rMLgVrVu3Rge+gaEBax5BaSkirMtb3ew4qmrM0jyagW9PyrwHimZObzx1zH8vBz5dHBzlo5rz0eDmnLwYjx9vt3BqsOR6PSSV34/QkpmNnOeuK/Y3mN21pbG21Z82sP4TeDgwYDmNbC2FKw6HFlgsxydns/WhuDr6cAT7bUULB3rebL+5S6Mf8CPZQfC6fvtTrafuWbqR1kiKkAohToUnsCQOXuITsxg8dh2jGjrw4s96nOfjxtvrzpOZIKRi4zBhpPRfPDvSXo3qcYng5sVOsrT08mWIa1qsfJQZNGDoY4v11Ie5BjPzXM7TlxOZNyigwyevYfjkdd5q78/m17twqyRrRlzv9b9NuzYTjptGcoT4e+ytN4W5j8dgLNdETl4UmLg98fBzhXhWZdxmYtZkjSGqCXjtcCRKyFMm3d4Zist187R37Sngos78jb5euNZEtOz+fDhpjeCA8CRZVr///tuzBXxSq+GXE/L5pd8gXb9iSjWnYqhU8/BuDwyG94IJWfMJtKxY+Opq8bL33IkxJ6BK4cASM7IZm9oHAP83RCHl2qDyW7tktnvC7C2x2njG6x6vhP/PdOCGjvehCr+0PXNor+EEhjUuhZ21hb8FhjB26uOA/DZkEL+ttxqQ68P4PxWOPo7ZKUx4sLb5Ahrnst8kRxR8ML+6X+niEpM56tHWmBvY4mFheDpjr6sfbkzDao58+ofR+nz7Q72no/jo0HNaFitdNKiuDva0K1RVf4+cgXdLTcSfwRFcO5aClP7N8bG6sbl2t7GkncGNGHFc51wsLXi201nzdLZQwUIxag1x64wct5+HGysWPn8/XQyVKdYWVrwv8daIyW89sfRAn/QoAWWl347TEtvN2Y+1hpLi6IH9Yx5wJeMbD2/H4wwvsH1CO3u+vwWbVL3OxR6LYWJS4IYMGs3QZcSeKNvI3a92YOJXevRoJozA1vW5K0HG/Nrh0h+tfoIL1cnkuoO4IHLC7AM/rnwA+dkwZ9PQ2qMNlXkqH/JmbiX9dY98Lz4L8ztDAv6wcIH4X8ttQDhVgeGzIXJ57S6/V1fA3DqShLLDlziqQ51bq6j1+vh0C9ao3K++vNWtd3o4V+VeTsvkJyRTWJaNu/+fZImNVyYkDuQzsKS6u6OtKztxkbDoLcCmg4BS1s48hsA287EkKXT86h9IGRch3YTCu7jVFWbx+DSbhpe+Zsqez+ClGgYPOeOq5byc7GzZkCLmvxxMJztZ2KY3KcR3u5FpL0IGKelsNjwFqyaiEXMaUI7f8ueGHt+DQy/adPtZ67xW2AEE7rUpU2dm2ck9PNy5M+JHZn2oD8R8ekMb+PNI21uLzFeYYa0rsW15Ez2nb/RrpCSmcO3m87S1tc9b/T6re7zcee/lx7gx6fa3HwTUUpUgFAKWLL/EpN+PUzzWq6sfuF+6le9Odd+bQ8HPh7clMCweH7YHnrTurDYVMb/EkR1Vzt+HhWAvU3RUy0C+Fd3oWNdTxbvDSvYoCuloZpAD+6+sH9OsSkhipKRreOxefvZGxrHK70asOvN7rzQvf7Ng/X0eq3+esU4RK02WE3cjssTv0DDfvDfZDhVyGCt9W9C+F4tn09NbX5tqxpNSek1g3YZs7jYZpoWPFJjtJ4srxyHUf9ojZYOHloit4s7kZFBvP/PCdwcbHit9y1Vc2E7taePNgVnmnulVwMS07NZtCeMz9aGEJ+axfThLQpkWu3TpBpHIxOJSjTStdLeTev9dGI55GSy4UQ0Xo42+F74Fao0hjr3Gz/31k+BTydtfMDhJVr+oFptjG97B0a2q41eagFxVCffoje2sICHZ0FWqpZVteubtOkxnE71PPlm09m8J9bEtGzeXHGMhtWceLVXwUmcQEsr8kyXehx8pxfTh7Uo9bxHPfyr4mxrxarDl/OWzd1xntiULKY92LjI97O1sqSGq/HJku6UChAV3JXr6Yz/5SBhsakm7/Pj9vO08/Vg2YT2eDgaTyA3uFUtHm5Zk283n+NwuDZTWVxKJqMWBgKwaEy7Ek0kM+Z+X64kZuT1t89z9HetYbTn+/DAa1renEt7TD7urf4KiqBP+lrWN9vKK7XO4JIVc/MGmcna6OJdX2tVOE+t1nrGWFrB8IVaZs4V47V2g/wO/qz1Hrr/ZWjxyE2rhrfxxtbZk3evdtMyck46CF0ma9Ug+QWMATs3otZ8xsGwBKb0bYSrwy3VWcG/gJ0bNC44+VELbzd6Na7KnO3n+SMogvGd/W5uHDXoa0iVsamwaqZWT0B6Alkh69h25hrj/GIRUUe1p4fCLlQWFlruIV0WeDXUspiawX0+7nw0qCmzRhb/ZApoqbsHfKs9TXSdghCC9wY2ISk9m+82nwPgw39PEpuSxdePtMLOuugbGld7a7PcqdtZW/Jg8xqsPxFFepaOqMR05u+6wMCWNWntU7LZGUuTChAVWFaOnkm/HmJzyDXWHCu+URm0EbOXr6fzYPPq2FoV/p9FCMHHg5tR3cWOl38/QkxyJmN/CSI6MYOfRgXgV1QjrhE9G1ejtoc9C/dcvLEw+Sqsn6pVE7R7Blo8CvYesP+HEh07V45OT9rWGXxm/TM1T83TAsE3jeFrf/j9Cdj5FfzcR2sA7j8dBv5P662Ty8YBRv6hzfj122PaiF6AsD2wbgrU760FslvYWVsyobMfu0NjtcF0hbF1JqvNeGpGb+Gh6ok8GnBLAEmNg9NrtCcOa+Pzfb/csyHp2Tp8PR0KvRuuX9WJulUc83IrFVCvOzhVJ27PQtKydAzNWQu2LlqupqJUaQTjNsDT/xRavjslhNYuUNT4hAJaP6n1tjKkyvav7sLIdj4s2X+J2dtCWXn4MpO616e5d+lP1VsSg1vXIjVLx8ZT0Xy14Sx6PYV27rhbzBoghBD9hBBnhBChQoipRtbXEUJsEUIcE0JsF0J451unE0IcMfwUnoBFKdQX605zKPw6zrZW7L9gJANlzFnt8Tufg2Hadu38islLg3Y39d1jrYhMSKPXNzs4Fnmd/z3Wmvtu447H0kIwytBN8nikITPo2te1ZGqDvtfuUK3tIWCsNto3/mLRBzTixJrvmZi9lCs+AxHTrsD4LVog8OsCMadh68eQdBmeXA7tJxq/W3b01PL2WNlrOf7D98OfT4G7Hwz76aZ8/fk93r4OrvbWzNkWanR9rjnpvUiTtnzktangnerR37Q79PsKVi/lau7tyqyRrflpVNsi74b7Nq3O/gvxJKYZ6SJpYUlW0+FUid7FQ9USqBqxXmu8tjVhWs9abQo2Yt+DXuvdEAcbS2ZsOEPTmi5M6lH2SSPb+3lQ09WO77eGsvJwJGPuL2EgNAOzBQghhCUwG+gPNAFGCiGa3LLZV8BiKWUL4CPg83zr0qWUrQw/hU8mrBi19ngUC/ZcZHQnX4a18SboUvzNffsv7IA5HbTslPlGYQZejMfFzopG1U3rodHW14NJ3euTmJ7N+wOa0K9Z9eJ3KsSjbWvjaGPJgj0X4eRqCPkXuk0Frwb53nC8dhE+MLdEx5Yha2h++H0CLVtT7amftWDjHaAFgqHztKqfN8O0SVvq9Sj6YO51tCCSmQwL+mojl0f+ptXfF8LJ1orRnXzZeOoqZwsZ83E+JoXZBxII8nwYz/N/5/Xh105Aao3T3u20zKBFGNiyZoF2o1v1aVINnV6y5bTxaqYFKR2xQsfX8iuELkv73CsQTydb3uznj4udFV8/2rJUZ8S7XRYWgkGta3HuWgqu9tY8fw9kOjbnp9IOCJVSXpBSZgG/A7fmtG0CbDX8vs3IeuU2XIxNZcryY7Sq7ca0BxvToa4nGdl6jkZe1zaIv6hNQGLnonWrPDg/b98DF+Np6+thWv2uwau9G7LjjW4lS7oXe06bajEfFztrHmvnw86jZ8hZ87qWhKzTSzfv51IDmg6Fw0shI8m097q0F/3ysRzX+xHR60csrQtpG7F3N+0uGbSRsI8t0/IYDV9wcxArxOhOvjjYWDJm4UGmLD/KrwfCCYlKQqeXSCn58N9T2FlZ0nT421of/r2zbuwcvg9izxptnL4dLb3dqOZiy8Zb23yA09FJzDhsQaR9I+ySLoJfV60uv4J5skMdgt/tjX/14kdy3y3D7quFpYXgtd4NcbUvZkrTu8CcAaIWkL/fYqRhWX5HgdzZ34cAzkKI3LoNOyFEkBBivxBisLE3EEI8Y9gmKCYmxtgmlU56lo7nlgZjZSmY/cR92FhZ0N7PAyFg//k47a73t5HaHen4LdCgr5Z6IOYsMcmZXIhJpZ2fR/FvlI8QgjqeJWhzyEiC+T20eXi/bwubP4CIg6DX82zXerxntQSRnqCNwLU0MhCpw3OQlawFieJcPQm/PsZVUYUptu8woG3xF3KT+XWBlw5DA9Mygbo72jDzsdY0qu7MplNXmbbqOP3/t4sWH2xgyJy97Dwbw6u9G+JZ009rZzi8BFIMA6CCf9HaAYqaWKYELCwEvZtUY8fZGDKybyQUlFLy3uqTuNhZ4dFptLbQWNfWCuJeeHLIr35VZ/a91YOnOtwb85KX9aczGegqhDgMdAUuA7l/rXUM86Q+DnwnhCiQyUpKOU9KGSClDKhSpRQnLy/H3vv7BGeuJvPdiFZ56Y3dHW3wr+7CgQsxsHKidif66C/ayNmHZ2lTKK56hqAL2t1koQEiK+226v4LOPq7ljm08+taZsu9s+DnXvCNP1XWPcMgi13MzhnIKX0h/0lq3ac1XB/4EfQ64/XooFXRLBlKlqU9j6S8wbAHWhbZ8H439GpSjQWj23Lo3d5sn9yNb0e0ZHgbb/RScn99T57qaDjn+1/RBgXu/0FLzX1qtZZCwqZkjf9F6du0OunZOnadi81btvLQZQLD4pna3x+HThO0Rnn/AaX2nkrxqjrbmWX60Nthzmyul4H83TC8DcvySCmvYHiCEEI4AcOklNcN6y4b/r0ghNgOtAbOm7G85d6fByP4KziSl3rUp1ujqjet61DXgyoHZ4DFf1pisbrdtBXO1bRugH+Nwm7/d9hb9zHaNZLYUPjjCa0xt15P6PSidoyS/iHr9VoStVoBkJsPKD0Bzm3SGp9DN6Or0pRfYh7h2KYz/DSqrfHjdHgO/hrFrjWLeWpvVSZ2qcub/fxvNOxeOQzLx0JOOl9W/YakNOdSnZvhTgkh8PVyxNfLkSGtjQy68qoPTQbBwZ+03lM5GUU2Tt+O9n6eONtZseFkNL2bVCMxPZvP14XQ2seNR9rUBgsBjfqV6nsq5Ys5nyAOAg2EEH5CCBvgMeCm3khCCC8hRG4Z3gIWGJa7CyFsc7dBmyzqlBnLWq5l6/Rs3LWH1H+n8FKtM7zcpeDk54OsD/C8xSpiGjyqNczm13QwtBhBl6iFDKtxreBj95l1ML+7Vt3R6SW4egKWDNamXjz2l5Y22VQXt0PcOa3bai57d60L66O/wJSLWE7cxtiu/mwOucYhwxiLAvwHkO3kjU3wXKo62zJ35wUmLg0mNTVFmyVsfk/ISuPKQ0tYcM6eJzvUKTpFxr2o82vak9a2z7T2GEO67NJiY2VBT/+qbAm5So5OzzcbzxCfmsXHg5qZpa+/Uv6YLUBIKXOAScAGIAT4U0p5UgjxkRAit1dSN+CMEOIsUA341LC8MRAkhDiK1nj9hZRSBYhbpGXlsHDPRQZPX43/plGMsVjLa3EfYvlVPW22sOBF2py/UUdpGTSNIH1D/qz6itG7/sTun3FNuvFa8tda11LQ7va3fa71+ffwg4k7oM/H2gjgh2dpd7Urx8PM1lqvIlNy0gfO17KQNh1sfL2VDVjZMrqTL15ONny14Yzxc9fBz9m9aS9C2DDCjQ8GNiHu9G7ivu4Au7/RumW+sJ9Z5zywtrRgzP2+pnyk95YaLbWZzqS+1J8ecvVpWp2EtGwW7Q1jyX4ttYfRJ0ilUjLrhEFSyrXA2luWvZfv9+XAciP77QWam7Ns5VlcSia/7LvE4n1hpKel8o/zl9SyTkb/1Hos9JnaHf+ZtdqALwBrB4SDB187vovFpWReMHLM4Ks6fs6eyLL0z7U78O5vwcpntGO0HKlVQ+VOrGJlC/c9rU3Mfm4j7PmfNlDM0lobp1CYhEta2Tq/XmyOHkdbK57vVp+P1pxiT2hsgdTa7/19ko1JnZjg+Afuh2cz2qUmo2znEK1z5yWrdxgfMIHqOXasOBTJ8DbeVHU2z8Ats+v5PljaaE9YZtC1YRVsrCz45L8QvJxsea1P2Q7MUu4taka5cmbRnot8sf40Gdl6evtX4Uv5Mx6XzsCIJeDbUduobjctw+a1U1qgCD8APd/FP8iK3wLDyczRFWisPXAxnkDRgpyAZ7A68IM2Yjc5CvrPKDzFgoWFVkfdsC8sfhg2vQ8N+xc+UCroZ637ZlFBJJ/H2/vw064LzNhwhk71PPMa7lYER7I8OJKXerTEMufJvIlhRMBYUlq8QfCvp9k4dx8BdTzI0el5JjdZXXlUo4U2xsJMHG2t6Fzfiy2nrzHtQf97omulcu8o615MSgn8HhjOB/+eomNdTza/1oX5Nf/F49J66PMJNB5488ZCQLWm2kxeTy6HGi3zxkMcyx2pnE/gxXhaerth1edD8Gqk9aAZ9S+0f6b4hmghYMB32ijfdVOMb5OdDocWa5OruxZsIzHGztqSl3o24EjEdTaHaN09z8ek8O7fJ2jn58FLPRtouY8aP6yldxjwLQ18avL3pPtpUsOF3aGx9G9Wo+i5GxSe716fSd3rM6S1ad+LUnmoAFFOrD0exbRVx+nasApznwqgfvhfsHemNsK1o7FKo4JuGg+RT1pWDscjE7XurTYOMGELvHQI6nQyvYCe9bTc/yH/GJ/4/vhyrbdS/sZpEwxr442flyNfbzxDepaOF5YdwtbKgpmPtcbK0gJcvbWnp7pd8/bxcrLl1wkdeOehxrwzoHGJ3q8yalPHncl9G90zXSuVe4cKEOXArnMxvPz7YVr7uPPjk22wubhVSzvdoI/WZdXE/9huDtp4iP0Xbw4Qh8Ovk6OXN8Y/2DprPyXV6UWo1kwrW/5RzlJC4FxtbuLC0kUXwtrSgld6NeB0dDLDftjL6ehkvnm01Y35fQthZ23J+M51zZYGWVEqAxUg7lVSQkYSJ04d55vFKxjidoHFHa9if3ShliajWhMt/bSxkcZF6FDXg+BLCWTm3Bg9e+BiPBZCu5O8I5bWMHCm1naxJd+UjhGBWpruotJFF2Fgi5r4V3fmVFQSE7vUpbt/1eJ3UhTljqlG6nuNXg8nV8K2TyH+As2AVZZAKrDasI2bDzz+p+l5g/LpUNeThXvCOBaZSFtf7Ykh8GIcTWq6lM44Ae822jiLA3O1nje122lPD7auxaeLLoSFhWD68Bb8e/QKk8s4/bGiVCYqQNxLzm+Dze9D1FGyPJvwo8VTJAlnnu3XBi+vamDnqmUMdap+8zwFJZC/HaKtrweZOToOh1/PmxC9VPR4B0LWaNOEPv4HnPob2k28ozQRLbzdaOHtVnplVBSlWEUGCKG1WnlLKQuZLFgpFVeOaAnrLmwD19ok9/+egTtqch0dfz7TEa9SmhwdtHaIxtVd2Hchjhd7NuDE5UQyc/QlTtBXJFtneOhr+G0ELB4E+hxoO670jq8oyl1RZBuElFJyy0A3pXCJ6dlsP3PN9B30Olj9AszrClFHoO9npE88wFNBdYlKymLB6LY0LMXgkKtDXc+8dogDF7UJgtr6lvK0ho36aZlH4y9oM615Fsi1qCjKPc6URupDQohCMqYp+S3eG8bohQcJCjMye5sxQQvgyFLo8AK8fBRd++d5eXkIR+9gZjZTdKjrQWaOnqMRiQRejKdBVacSzR9tsn5fahPcdJlc+sdWFMXsTAkQ7YF9QojzhqlBjwshjpm7YOXRqSita+fMrUVPKwlAUpSW0qJeD+j7KdLWhY/XnGLjqat3PDNbcdoZ2iH2hMYSFJZQutVL+TlXg/GbwKeDeY6vKIpZmdJI3dfspaggTkcnY2Npwc6zMRyJuE6r2m6Fb7xuCuiztbp6Ifhp5wUW7Q1j/AN+JZuZ7TbktkP8GhhOSmaO+QKEoijlWrFPEFLKS4AbMNDw42ZYpuSTlpVDWFwqo+/3xc3BmllbzhW+8Zl12ojjrlPAoy7/HYvi07UhPNi8OtMevDsjfzvU9SQmORMoYoIgRVEqtWIDhBDiZWAZUNXws1QI8aK5C1benL2agpTaYLNx9/ux5fQ1TlwumPOIzBRY+4Y2qrjTSxwMi+fVP48QUMedbx5tddfy8HeoqwUFHw8HNdpYURSjTGmDGAe0l1K+Z0jV3QGouJPU3qYQQ/tD4+oujLrfF2c7K2ZtNfIUsf1zSIyAAd9xIT6TCYuD8HazZ/7TAdhZ373pMNv7eSIEeYPlFEVRbmVKgBDcmCcaw+8qq9ctTkcl4Whjibe7PS521oy5348NJ6/mBQ5AG++wfw60GUO8Z2vGLjqIhRAsHNMWd8fbG/h2u1wdrJn9+H283LPBXX1fRVHKD1MCxELggBDiAyHEB8B+4GezlqocColOxr+GS14V0dj7fXGyteL7bYYeTXqdNrLYwYvM7u8xcUkQVxIzmP90G+p4lk066geb18DH06FM3ltRlHtfkQHCMF/0fmAMEG/4GSOl/M78RSs/pJScjkrCv/qNQW1uDjY83bEOa49Hce5qsjapTdQRZL8vmPpfOAfDEvjqkZa0qaOqeBRFuTcV2c1VSqkXQsyWUrYGDt2lMpU7UYkZJGXk0Efsh0/7gIUVWNnxupU9Q2102Cx0gJwIqN+b/0U3Y9XhUCb3acjDLWuWddEVRVEKZco4iC1CiGHASkPqDeUWp6O1doZWMWu0PERNh0JOOpbZ6eRYXCMyNh6Pht3Z7fcq3/0byrD7vHmhe/0yLrWiKErRTAkQE4HXgBwhRAZaA7WUUrqYtWTlSEhUMo6k4xK9V5sxre+nees8kzMZPH0rTVNcOb42jg51Pfh8aHM1e5eiKPc8U9og+kkpLaSUNlJKFymlswoONwuJSmKoSwhClwX+D920roqzLY+3q0PwpQS83e21GeGs1DxNiqLc+0xpg/geaH2XylMunY5O5hPrw2DlCbXbF1j/Qvd6pGfreLZrXdwc7m53VkVRlNtlyq3sFiHEMKHqRIzKyNYRHnOdVhkHoGF/sCg42M3TyZbPhzYvs+6siqIot8OUADER+AvIEkIkCSGShRBJxe1UWYReS6GtCMFOl1KgeklRFKU8K7aRWkpZ+jPWVCAhUUn0sQhCb2WPRb3uZV0cRVGUUmNKsj4hhHhSCPGu4XVtIUQ78xetfDgdlURfy2BE/Z5grZLeKYpScZhSxTQH6Ag8bnidAsw2W4nKmcyIYKqLeIT/gLIuiqIoSqkyZRxEeynlfUKIwwBSygQhhOqKg5Zio07MdnRYYtlQzaukKErFYsoTRLYQwhKQAEKIKoDerKUqJ2KSM+msC+Sae2twUDmVFEWpWEwJEDOBVUBVIcSnwG7gM7OWqpy4eO44/hYRZNbvX9ZFURRFKXWm9GJaJoQIBnqipdkYLKUMMXvJygH9qf8A8LhvcNkWRFEUxQxMaYNASnkaOG3mspQ7Va9s4azwpWENlXhPUZSKRyUFul0pMfimn+CUS+eyLomiKIpZqABxm3JOr8MSPQk+fcq6KIqiKGZhUoAQQtQRQvQy/G4vhKj0o6vTj/9NpPTCo+59ZV0URVEUszBlJPUEYDkw17DIG1htxjLd+7JScYjYxUZdAI1rupZ1aRRFUczClCeIF4D7gSQAKeU5oKo5C3XPO78VS30m22hLXS+VoVVRlIrJlACRKaXMyn0hhLDCMGiuOEKIfkKIM0KIUCHEVCPr6wghtgghjgkhtgshvPOtGyWEOGf4GWXK+5ldVhoc/Bk2TCPFwpnrVQKwslTNOIqiVEymdHPdIYSYBtgLIXoDzwP/FreTYfT1bKA3EAkcFEL8I6U8lW+zr4DFUspfhBA9gM+Bp4QQHsD7QABaMAo27JtQkpMrNclX4eB8LTikx0ONVryZPIaGNdzLpDiKoih3gym3v1OBGOA42twQa6WUb5uwXzsgVEp5wfAE8jsw6JZtmgBbDb9vy7e+L7BJShlvCAqbgH4mvGfpijsPf78A3zWDnV+BT0cYs464xzfwX2ojGteo9G31iqJUYKY8QbwopfwfMD93gRDiZcOyotQCIvK9jgRunY/zKDAU+B8wBHAWQngWsm8tE8p6W6I/+4zMkILjAOWVw5CdjnBuCC414bwVrP2RxPRsvoxKonGoC5fmWZurWIqiKCaxbexP9WnTSv24pjxBGKv/H11K7z8Z6GrIFNsVuAzoTN1ZCPGMECJICBEUExNTSkW6QWalE613IzjFg9OxOUQkpBOflkVSRjYADjYFpxdVFEWpKAp9ghBCjESbA8JPCPFPvlXOQLwJx74M1M732tuwLI+U8graEwRCCCdgmJTyuhDiMtDtln233/oGUsp5wDyAgIAAkxrOjTEaebNS4bOa7HQcwFGf0Ry/nMj5mBT0hnep4mzLwbd73e5bKoqi3POKqmLaC0QBXsDX+ZYnA8dMOPZBoIEQwg8tMDzGjUmHABBCeAHxUko98BawwLBqA/CZECK3FbiPYf3dkxoLQJWqNflmRCsA0rJyCIlK4nhkIr6qe6uiKBVcoQFCSnkJuIQ2m1yJSSlzhBCT0C72lsACKeVJIcRHQJCU8h+0p4TPhRAS2Ik25gIpZbwQ4mO0IAPwkZTSlKeWUpOVdBUbQO/glbfMwcaKNnU8aFNHzf2gKErFV2wjtRAimRvjHmwAayBVSulS3L5SyrXA2luWvZfv9+Voo7SN7buAG08Ud11m4jVsABy9ittUURSlQjJlPoi8vpxCCIHWFbWDOQt1L8hOugaAcKpSxiVRFEUpGyUaBiw1q9HGKVRo2UlaryhrZxUgFEWpnEypYhqa76UF2ujmDLOV6B4hU6+RLm2wdyy2Jk1RFKVCMmWg3MB8v+cAYRQcEV3hyNQ44nDB0U4NhFMUpXIypQ1izN0oyL3GMj2Wa9IFJ1uTZmVVFEWpcIoaKDeLIrK2SilfMkuJ7hFW6XHES2caqQChKEolVdTVL+iuleIeZJMZTxwNcVQBQlGUSqqogXK/5H9tSIWBlDLF3IUqc1Jil51ArHTBUeVbUhSlkjJlytFmhmR6J4FTQohgIURT8xetDGWlYKXPJNnCVU0IpChKpWXK1W8e8JqUso6U0gd4nXypvyskQx6mFCs1IZCiKJWXKQHCUUq5LfeFlHI7ULEz1RkCRIa1W9mWQ1EUpQyZ0gJ7QQjxLrDE8PpJ4IL5inQPSNMCRKadZxkXRFEUpeyY8gQxFqgCrDT8eBmWVVypWpqNbBuVtVVRlMrLlIFyCcBLAEIIS7QqpyRzF6xMGaqY9PbqCUJRlMrLlF5MvwohXIQQjsBxtJ5Mb5i/aGUoNZY07LC2dyrrkiiKopQZU6qYmhieGAYD6wA/4ClzFqrMpcUSjwtOdmqQnKIolZcpAcJaCGGNFiD+kVJmU0QKjgohNYY46azyMCmKUqmZEiDmomVwdQR2CiHqABW6DUKmxBCjd8HRRgUIRVEqr2IDhJRyppSylpTyQcOEQZeA7nehbGVGpsYSL1UVk6IolZspjdSeQoiZQohDhjQb/wNc70LZyoaUiLRY4nDByVblYVIUpfIypYrpdyAGGAYMN/z+hzkLVaYykxD6bOKki8rkqihKpWbKFbCGlPLjfK8/EUKMMFeBypxhDEScmixIUZRKzpQniI1CiMeEEBaGn0eBDeYuWJkxBIh4VIBQFKVyK2pGuWS07qwCeAVYalhlAaQAk81duDJhSLMRq6qYFEWp5IqaMMj5bhbknmFI1BevqpgURankTLoCCiHcgQaAXe4yKeVOcxWqTBmeIOJRA+UURancir0CCiHGAy8D3sARoAOwD+hh1pKVldQ4siwdycRGVTEpilKpmdJI/TLQFrgkpewOtAaum7NQZSo1hlQrd2ysLLCxUtONKopSeZlyBcyQUmYACCFspZSngUbmLVYZSosl2dJVVS8pilLpmXIVjBRCuAGrgU1CiATgkjkLVaZSY0m0cMNRjaJWFKWSM2XCoCGGXz8QQmxDS7Ox3qylKkupsSTgg5OtdVmXRFEUpUyVqB5FSrnDXAW5J0ipzQXh4KzyMCmKUumpVtj8Mq6DPkdL9a3aIBRFqeRUgMjPkGbjms5JBQhFUSo9kwKEEKKOEKKX4Xd7IUTFHGVtGCQXleOMswoQiqJUcqbMBzEBWI42sxxoA+ZWm7FMZcfwBHEl21E9QSiKUumZ8gTxAnA/hmlGpZTngKrmLFSZMTxBRGapKiZFURRTAkSmlDIr94UQwgoty2vFkxYHQAKqiklRFMWUALFDCDENsBdC9Ab+Av41b7HKSGoMeltXsrFSTxCKolR6pgSIqWjTjB4HJgJrgXdMObgQop8Q4owQIlQIMdXIeh8hxDYhxGEhxDEhxIOG5b5CiHQhxBHDz4+mn9IdSI0lx84DQI2kVhSl0jNlJLUemG/4MZkQwhKYDfQGIoGDQoh/pJSn8m32DvCnlPIHIUQTtODja1h3XkrZqiTvecdSY8iy8wTA2U49QSiKUrmZku77OAXbHBKBIOATKWVcIbu2A0KllBcMx/kdGATkDxAScDH87gpcMb3oZpAWR6ZtLQAcbVSAUBSlcjPlKrgO0AG/Gl4/BjgA0cAiYGAh+9UCIvK9jgTa37LNB2hzXr8IOAK98q3zE0IcRus99Y6UctetbyCEeAZ4BsDHx8eEUylGagxpjs0BVBuEoiiVnilXwV5SyvvyvT4uhDgkpbxPCPHkHb7/SGCRlPJrIURHYIkQohkQBfhIKeOEEG2A1UKIplLKpPw7SynnAfMAAgIC7qxnlV4PaXGkWLkCqopJURTFlEZqSyFEu9wXQoi2QG4Lbk4R+10Gaud77W1Ylt844E8AKeU+tClNvaSUmblVV1LKYOA80NCEst6+9ASQepIt3QD1BKEoimJKgBgP/CyEuCiECAN+BiYIIRyBz4vY7yDQQAjhJ4SwQaua+ueWbcKBngBCiMZoASJGCFHF0MiNEKIu2nzYF0w/rduQpo2ivi60Jwg1YZCiKJWdKb2YDgLNhdCunFLKxHyr/yxivxwhxCRgA9oTxwIp5UkhxEdAkJTyH+B1YL4Q4lW0BuvRUkophOgCfCSEyAb0wLNSyvjbPEfTGEZRx+OKlYXAVk03qihKJWfSbbIQ4iGgKWAnhABASvlRcftJKdeidV3Nv+y9fL+fQkvjcet+K4AVppSt1BjyMMVLZxxtrcg9T0VRlMrKlG6uP6L1WuoO/AQMBwLNXK67z/AEcVXvjJOtCg6Koiim1KN0klI+DSRIKT8EOmLuBuOyYMjDdC3HUbU/KIqiYFqAyDD8myaEqAlkAzXMV6QykhoD9u4kZwmVZkNRFAXT2iD+FUK4ATOAQ2iNySVKu1EupMaCgxfJmTm4qDEQiqIoRQcIIYQFsEVKeR1YIYRYA9jd0pOpYkiNBccqpF7PoZabXVmXRlEUpcwVWcVkSNQ3O9/rzAoZHECrYnL0JDUzR+VhUhRFwbQ2iC1CiGGiovf7TNOeIFIyctQoakVRFEwLEBPRJgnKEkIkCSGShRBJxe1Uruh1kBaPdPAiNStH9WJSFEXBtJHUznejIGUqLR6QZNt5oJfgpBqpFUVRTBooJ4AnAD8p5cdCiNpADSllxRksZxgkl26dO5ucChCKUpjs7GwiIyPJyMgofmPlnmFnZ4e3tzfW1tYm72PKlXAOWj6kHsDHQApaw3Xb2ynkPcmQqC/Vyg3IwUmNg1CUQkVGRuLs7Iyvr69KSVNOSCmJi4sjMjISPz8/k/czpQ2ivZTyBQwD5qSUCYDN7RXzHmV4gkixcgfAydb0CKsolU1GRgaenp4qOJQjQgg8PT1L/NRnSoDINqTeloY3qoL2RFFxpGppNhKFNvupGkmtKEVTwaH8uZ3vzJQAMRNYBVQVQnwK7AY+K/E73ctSYwDBdcP02KoXk6IoigkBQkq5DJiCNjlQFDBYSvmXuQt2V6XFgoMHqdnarKUqQCiKkuu9995j8+bNZn2PRYsWceXKlWK3Gz16NMuXLy+w/K+//qJp06ZYWFgQFBRUauUypRfTTOB3KeXs4rYtt1JjtEFymdoMqipAKIoCoNPp+OijYqe+uWOLFi2iWbNm1KxZ87b2b9asGStXrmTixImlWi5TroTBwDtCiEZoVU2/SylLL0TdC1LjwMErL0Cobq6KYpoP/z3JqSulO262SU0X3h/YtND1S5cuZebMmWRlZdG+fXvmzJnDoUOHGDduHIGBgeh0Otq1a8cff/xBbGws7733Hs7OzoSGhtK9e3fmzJmDhYUFGzdu5P333yczM5N69eqxcOFCnJyc8PX1ZcSIEWzatIkpU6awfv16BgwYwPDhw/H19WXkyJGsW7cOKysr5s2bx1tvvUVoaChvvPEGzz77LAAzZszgzz//JDMzkyFDhvDhhx8SFhZG//79eeCBB9i7dy+1atXi77//5r///iMoKIgnnngCe3t79u3bx4wZM/j3339JT0+nU6dOzJ07t8g2hMaNG5fqd5DLlCqmX6SUD6J1az0DfCmEOGeW0pSV1Bhw9CI1MwchwMFGNVIryr0oJCSEP/74gz179nDkyBEsLS1ZtmwZbdu25eGHH+add95hypQpPPnkkzRr1gyAwMBAZs2axalTpzh//jwrV64kNjaWTz75hM2bN3Po0CECAgL45ptv8t7H09OTQ4cO8dhjjxUog4+PD0eOHKFz5855VT779+/n/fffB2Djxo2cO3eOwMBAjhw5QnBwMDt37gTg3LlzvPDCC5w8eRI3NzdWrFjB8OHDCQgIYNmyZRw5cgR7e3smTZrEwYMHOXHiBOnp6axZs+YufLoFleRWuT7gD9QBQsxTnDKSFguO2hOEk42ablRRTFXUnb45bNmyheDgYNq21YZhpaenU7VqVUBrK2jbti12dnbMnDkzb5927dpRt25dAEaOHMnu3buxs7Pj1KlT3H+/NuNxVlYWHTt2zNtnxIgRhZbh4YcfBqB58+akpKTg7OyMs7Mztra2XL9+nY0bN7Jx40Zat24NQEpKCufOncPHxwc/Pz9atWoFQJs2bQgLCzP6Htu2bWP69OmkpaURHx9P06ZNGThw4G18YnfGlDaI6cAQ4DzwB/CxIf13xaDLhvQErQ0iRiXqU5R7mZSSUaNG8fnnnxdYFxcXR0pKCtnZ2WRkZODo6AgU7N4phEBKSe/evfntt9+Mvk/uvsbY2toCYGFhkfd77uucnByklLz11lsF2gPCwsJu2t7S0pL09PQCx8/IyOD5558nKCiI2rVr88EHH5TZqHVTurmeBzpKKftJKRdWqOAAhjxMgIOnlqhP5WFSlHtWz549Wb58OdeuXQMgPj6eS5cuATBx4kQ+/vhjnnjiCd588828fQIDA7l48SJ6vZ4//viDBx54gA4dOrBnzx5CQ0MBSE1N5ezZs6VSxr59+7JgwQJSUlIAuHz5cl55C+Ps7ExycjJAXjDw8vIiJSXFaK+lu8WUZH1zhRDuQoh2gF2+5TvNWrK7xdELXj4Gdi6knDinniAU5R7WpEkTPvnkE/r06YNer8fa2prZs2ezY8cOrK2tefzxx9HpdHTq1ImtW7diYWFB27ZtmTRpUl4j9ZAhQ7CwsGDRokWMHDmSzMxMAD755BMaNmx4x2Xs06cPISEheVVWTk5OLF26FEvLwts2R48ezbPPPpvXSD1hwgSaNWtG9erV86rTirJq1SpefPFFYmJieOihh2jVqhUbNmy443MRUsqiNxBiPPAy4A0cAToA+6SUPe743UtRQECAvNP+v0Pn7MHexpJl4zuUUqkUpeIJCQkxW6+Z0rZ9+3a++uqrMmvkvdcY++6EEMFSygBj25tSxfQyWg+mS1LK7kBr4PodlvOelJqpU2MgFEVRDEy5GmZIKTOEEAghbKWUpw1jIiqclEzVSK0oFUm3bt3o1q1bWRej3DLlahgphHADVgObhBAJwCVzFqqspGSq2eQURVFymdJIPcTw6wdCiG2AK7DerKUqA1JKUtUThKIoSp4SXQ2llDvMVZCylpmjJ0cv1ROEoiiKgSmN1JWCStSnKIpyMxUgDFJVoj5FUYwoD+m+33jjDfz9/WnRogVDhgzh+vXrpVIuFSAM1BOEoii3yk333atXL7O+j6kBojC9e/fmxIkTHDt2jIYNGxpNRXI71NXQICVDBQhFKbF1UyH6eOkes3pz6P9FoatVuu+C+vTpk/d7hw4dSi09h3qCMEjNyq1iUqm+FeVepdJ9F2/BggX079//Tj7mPOp22SAlUweAs0rWpyimK+JO3xxUuu+iffrpp1hZWfHEE08Uu60p1NXQILeKSTVSK8q9S6X7LtyiRYtYs2YNW7ZsKbU5bVQVk4HqxaQo9z6V7tu49evXM336dP755x8cHBzu8AxuUFdDg7z5qG3UR6Io9yqV7tu4SZMmkZmZSe/evQGtofrHH3+843MpNt13eXGn6b4/XnOK3wLDOfVRv1IslaJUPCrdd/lljnTflYLKw6QoinIzswYIIUQ/IcQZIUSoEGKqkfU+QohtQojDQohjQogH8617y7DfGSFEX3OWE7QqJmcVIBSlQunWrZt6ergDZrsiCiEsgdlAbyASOCiE+EdKeSrfZu8Af0opfxBCNAHWAr6G3x8DmgI1gc1CiIZSSp25yqvmglAURbmZOZ8g2gGhUsoLUsos4Hdg0C3bSMDF8LsrkDvWfBDwu5QyU0p5EQg1HM9stComNUhOURQllzkDRC0gIt/rSMOy/D4AnhRCRKI9PbxYgn0RQjwjhAgSQgTFxMTcUWFTMnU42Vrf0TEURVEqkrJupB4JLJJSegMPAkuEECaXSUo5T0oZIKUMqFKlyh0VJDUzByf1BKEoipLHnAHiMlA732tvw7L8xgF/Akgp9wF2gJeJ+5Yq1QahKJVHWFgYv/76a4n3W7RoEZMmTTJDie5N5gwQB4EGQgg/IYQNWqPzP7dsEw70BBBCNEYLEDGG7R4TQtgKIfyABkCgGcuq5qNWlEqkqACRk5Nzl0tz7zLbFVFKmSOEmARsACyBBVLKk0KIj4AgKeU/wOvAfCHEq2gN1qOlNnLvpBDiT+AUkAO8YM4eTFk5erJy9CpAKEoJfRn4JafjT5fqMf09/Hmz3ZtFbrN48WK++uorhBC0aNGCjz/+mLFjxxIbG0uVKlVYuHAhPj4+jB49GhcXF4KCgoiOjmb69OkMHz6cqVOnEhISQqtWrRg1ahTu7u6sXLmSlJQUdDodq1atYuzYsVy4cAEHBwfmzZtHixYtSvU8ywOzXhGllGvRGp/zL3sv3++ngPsL2fdT4FNzli+XysOkKOXHyZMn+eSTT9i7dy9eXl7Ex8czatSovJ8FCxbw0ksvsXr1agCioqLYvXs3p0+f5uGHH2b48OF88cUXN42wXrRoEYcOHeLYsWN4eHjw4osv0rp1a1avXs3WrVt5+umnOXLkSNmddBlRV0TUbHKKcruKu9M3h61bt/LII4/g5eUFgIeHB/v27WPlypUAPPXUU0yZMiVv+8GDB2NhYUGTJk24evVqocft3bs3Hh4eAOzevZsVK1YA0KNHD+Li4khKSjLXKd2zyroX0z0hd7IgJzUXhKJUOPlTbBeVe66oFN+VlQoQqLkgFKU86dGjB3/99RdxcXGAlvK7U6dO/P777wAsW7aMzp07F3mM/Om1jencuTPLli0DtIR/Xl5euLi4FLp9RaWuiOSvYlLjIBTlXte0aVPefvttunbtiqWlJa1bt2bWrFmMGTOGGTNm5DVSF6VFixZYWlrSsmVLRo8ejbu7+03rP/jgA8aOHUuLFi1wcHDgl19+Mecp3bNUum/gv2NRvPDrITa80oVG1Z1LuWSKUrGUp3Tfys1Uuu/bkJKZDaByMSmKouSjAgRaHiZQvZgURVHyUwECNQ5CURTFGBUg0Bqpba0ssLZUH4eiKEoudUVE5WFSFEUxRgUIDKm+1SA5RVGUm6gAgTZQztFGBQhFqWi2b9/O3r177+gYTk5OpVSa8kcFCFQVk6JUVKURICozdVVEy8VU1dmurIuhKOVO9GefkRlSuum+bRv7U33atCK3GTx4MBEREWRkZPDyyy/zzDPPsH79eqZNm4ZOp8PLy4uff/6ZH3/8EUtLS5YuXcqsWbP4+eefGTBgAMOHDwe0p4OUlBRSUlIYNGgQCQkJZGdn88knnzBo0KBSPa/ySAUItComPy/1UShKebFgwQI8PDxIT0+nbdu2DBo0iAkTJrBz5078/PyIj4/Hw8ODZ599FicnJyZPngzAzz//bPR4dnZ2rFq1ChcXF2JjY+nQoQMPP/wwQoi7eVr3HHVVRBsop/IwKUrJFXenby4zZ85k1apVAERERDBv3jy6dOmCn58fQF7ablNJKZk2bRo7d+7EwsKCy5cvc/XqVapXr17qZS9PVIDA0ItJtUEoSrmwfft2Nm/ezL59+3BwcKBbt260atWK06eLr+qysrJCr9cDoNfrycrKArQMsDExMQQHB2NtbY2vry8ZGRlmPY/yoNI3Uufo9KRn69QoakUpJxITE3F3d8fBwYHTp0+zf/9+MjIy2LlzJxcvXgS0FOBQMK23r68vwcHBAPzzzz9kZ2fnHbNq1apYW1uzbds2Ll26dJfP6t5U6QNEapbKw6Qo5Um/fv3IycmhcePGTJ06lQ4dOlClShXmzZvH0KFDadmyJSNGjABg4MCBrFq1ilatWrFr1y4mTJjAjh07aNmyJfv27cubJOiJJ54gKCiI5s2bs3jxYvz9/cvyFO8ZlT7dd2JaNm+vPs4jAbXp2rCKGUqmKBWLSvddfpU03Xelv212dbDm+8fvK+tiKIqi3HMqfRWToiiKYpwKEIqilFhFqZquTG7nO1MBQlGUErGzsyMuLk4FiXJESklcXBx2diXLGFHp2yAURSkZb29vIiMjiYmJKeuiKCVgZ2eHt7d3ifZRAUJRlBKxtrbOG7GsVGyqiklRFEUxSgUIRVEUxSgVIBRFURSjKsxIaiFEDHAnCVS8gNhSKk55UdnOubKdL6hzrizu5JzrSCmNppGoMAHiTgkhggobbl5RVbZzrmznC+qcKwtznbOqYlIURVGMUgFCURRFMUoFiBvmlXUBykBlO+fKdr6gzrmyMMs5qzYIRVEUxSj1BKEoiqIYpQKEoiiKYlSlDxBCiH5CiDNCiFAhxNSyLk9pEULUFkJsE0KcEkKcFEK8bFjuIYTYJIQ4Z/jX3bBcCCFmGj6HY0KIcjmLkhDCUghxWAixxvDaTwhxwHBefwghbAzLbQ2vQw3rfcu04HdACOEmhFguhDgthAgRQnSsyN+zEOJVw9/0CSHEb0IIu4r4PQshFgghrgkhTuRbVuLvVQgxyrD9OSHEqJKUoVIHCCGEJTAb6A80AUYKIZqUbalKTQ7wupSyCdABeMFwblOBLVLKBsAWw2vQPoMGhp9ngB/ufpFLxctASL7XXwLfSinrAwnAOMPycUCCYfm3hu3Kq/8B66WU/kBLtPOvkN+zEKIW8BIQIKVsBlgCj1Exv+dFQL9blpXoexVCeADvA+2BdsD7uUHFJFLKSvsDdAQ25Hv9FvBWWZfLTOf6N9AbOAPUMCyrAZwx/D4XGJlv+7ztyssP4G34T9MDWAMItNGlVrd+38AGoKPhdyvDdqKsz+E2ztkVuHhr2Svq9wzUAiIAD8P3tgboW1G/Z8AXOHG73yswEpibb/lN2xX3U6mfILjxx5Yr0rCsQjE8VrcGDgDVpJRRhlXRQDXD7xXhs/gOmALoDa89getSyhzD6/znlHe+hvWJhu3LGz8gBlhoqFr7SQjhSAX9nqWUl4GvgHAgCu17C6bif8+5Svq93tH3XdkDRIUnhHACVgCvSCmT8q+T2i1FhejnLIQYAFyTUgaXdVnuMivgPuAHKWVrIJUb1Q5Ahfue3YFBaIGxJuBIwWqYSuFufK+VPUBcBmrne+1tWFYhCCGs0YLDMinlSsPiq0KIGob1NYBrhuXl/bO4H3hYCBEG/I5WzfQ/wE0IkTsxVv5zyjtfw3pXIO5uFriURAKRUsoDhtfL0QJGRf2eewEXpZQxUspsYCXad1/Rv+dcJf1e7+j7ruwB4iDQwNADwgatseufMi5TqRBCCOBnIERK+U2+Vf8AuT0ZRqG1TeQuf9rQG6IDkJjvUfaeJ6V8S0rpLaX0Rfset0opnwC2AcMNm916vrmfw3DD9uXuLltKGQ1ECCEaGRb1BE5RQb9ntKqlDkIIB8PfeO75VujvOZ+Sfq8bgD5CCHfD01cfwzLTlHUjTFn/AA8CZ4HzwNtlXZ5SPK8H0B4/jwFHDD8PotW/bgHOAZsBD8P2Aq1H13ngOFovkTI/j9s8927AGsPvdYFAIBT4C7A1LLczvA41rK9b1uW+g/NtBQQZvuvVgHtF/p6BD4HTwAlgCWBbEb9n4De0dpZstCfFcbfzvQJjDecfCowpSRlUqg1FURTFqMpexaQoiqIUQgUIRVEUxSgVIBRFURSjVIBQFEVRjFIBQlEURTFKBQilQhJCpJTScT4QQkw2YbtFQojhxW1XCuUJEELMLGYb3/wZQG9ZN1oIUdM8pVMqGqviN1EU5V4hpQxCG/Nwu0ajjR+4UioFUio09QShVGhCCCchxBYhxCEhxHEhxCDDcl/D/AmLhBBnhRDLhBC9hBB7DHnz2+U7TEshxD7D8gmG/YUQ4nuhzSWyGaia7z3fE0IcNMxXMM8w4jd/mSyFEBcNx3ATQuiEEF0M63YKIRoIIRwN8wEEGpLw5Za7m7gx10UVw5wAJw1J+i4JIbwMb2MphJhvWLdRCGFveMIJAJYJIY4IIezN9LErFYQKEEpFlwEMkVLeB3QHvs53wa4PfA34G34eRxuBPhmYlu8YLdByO3UE3jNU0QwBGqHNI/I00Cnf9t9LKdtKbb4Ce2BA/gJJKXVo6ZibGN7vENBZCGEL1JZSngPeRksL0c5Q7hmGLK35vW/YpilaDiaffOsaALMN664Dw6SUy9GePp6QUraSUqab8gEqlZeqYlIqOgF8ZrhD16OlOs5NkXxRSnkcQAhxEm0iFimEOI6Whz/X34aLaboQYhvaxCtdgN8MF/srQoit+bbvLoSYAjigzVtwEvj3lnLtMhzDD/gcmADsQMsPBlrOnIfztX/YcXMAAC24DAGQUq4XQiTkW3dRSnnE8HvwLeejKCZRTxBKRfcEUAVoI6VsBVxFu9gCZObbTp/vtZ6bb55uzUdTaH4aIYQdMAcYLqVsDszP93757QQ6owWbtYAbWg6pXbmHQrvrb2X48ZFShhg5TmHyn5sOdTOo3AYVIJSKzhVtnohsIUR3oM5tHGOQ0OY99kS7iB9Eu8CPMLQn1ECrBoIbwSBWaHNxFNazKRCtWkovpcxAS6Y40XBc0DJuvphbHSaEaG3kGHuARw3r+6Al6StOMuBswnaKogKEUuEtAwIM1UZPo2UBLaljaOmk9wMfSymvAKvQMmqeAhYD+wCklNfRnhpOoF3kDxo5HlLKTLSZvvYbFu1Cu3AfN7z+GLAGjhmqvz42cpgP0VI5nwAeQZthLLmYc1kE/KgaqRVTqGyuilJOGRq1dVLKHCFER7RZ5VqVcbGUCkTVSypK+eUD/CmEsACy0Bq6FaXUqCcIRVEUxSjVBqEoiqIYpQKEoiiKYpQKEIqiKIpRKkAoiqIoRqkAoSiKohj1fwZRZexst/z4AAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1689175317791
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## verify 2 highly-correlated dimensions, thus motivating the use of a 5-2 dimensional model \n",
        "x_A = sample_data(n_A, rho0=-0.99) \n",
        "theta_A_estimate, _ = mle(x_A) \n",
        "info_estimate = info_estimator(x_A, theta_A_estimate) \n",
        "print(f'info_estimate: {info_estimate}') "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "info_estimate: [[ 45.44668771  42.96010321  -0.16197166   0.54482754  -3.60707697]\n [ 42.96010321  41.59693554  -0.19406581   3.09979998  -6.01018428]\n [ -0.16197166  -0.19406581   0.31397915  -0.72758164   1.43955665]\n [  0.54482754   3.09979998  -0.72758164  46.2160224  -48.49211761]\n [ -3.60707697  -6.01018428   1.43955665 -48.49211761  54.81152986]]\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1689172979048
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#[experimental_error_list[i] - control_error_list[i] for i in range(len(control_error_list))]\n",
        "#experimental_error_list\n",
        "control_error_list"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "[0.12168117813910324,\n 0.19057423837990878,\n 0.16792209898301666,\n 0.08219251775215146,\n 0.11315518159246966,\n 0.11325148903476545,\n 0.10566052003711997,\n 0.1194434947324807,\n 0.12013099571496424,\n 0.11200247448403695,\n 0.1664933611124623,\n 0.1242826631495946,\n 0.13208303795970372,\n 0.0929838478024042,\n 0.11384343680846336,\n 0.11691376034365582,\n 0.12966677930546247,\n 0.10397936461084227,\n 0.06600280782778072,\n 0.13840210329585015,\n 0.11354650361884219,\n 0.101842195418174,\n 0.15913129835363135,\n 0.12030035532925745,\n 0.1262393387390544,\n 0.13034010140064917,\n 0.1056886115438107,\n 0.1047740535090689,\n 0.09913982379091649,\n 0.1423387103157461,\n 0.09439452469638708,\n 0.15606887452475032,\n 0.13602470342361958,\n 0.12184859285529878,\n 0.17407383353801095,\n 0.103265326161278,\n 0.09992746795452526,\n 0.18487069543353152,\n 0.09321120456767616,\n 0.11228290796224812,\n 0.14386869130233576,\n 0.11641504332108474,\n 0.1283352023259768,\n 0.1377939130922191,\n 0.09533052117508928,\n 0.11627069181511367,\n 0.06715193026243269,\n 0.17926036759904412,\n 0.13252193381124286,\n 0.12785883247586355]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1689050878072
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "THE FOLLOWING IS OLD CONTENT TO BE REVISED\n",
        "\n",
        "# memory as geometry \n",
        "\n",
        "Under asymptotic analysis and mild regularity assumptions, all statistical models have a Lagrangian-form regularizer equivalent to memory \\[1\\]. \n",
        "We will mathematically generalize this concept to show a breadth of geometries equivalent to memory. \n",
        "As a corollary, we'll construct universal sufficient statistics. \n",
        "Finally, given large models' recent immense productivity, it is natural to expect hardware limitations to soon motivate miniaturization (TODO CITE). \n",
        "We'll leverage memory's geometric properties to construct miniaturization techniques for deep learning, including \n",
        "memory merges, regularizers as memory, and a _frontal lobe_ concept. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## result: Langrangian-regularized estimates are MLEs \n",
        "\n",
        "The MLE's powerful theoretical guarantees provide an important foundation for Deep Learning's success. \n",
        "So, we should be curious as to when our new estimation paradigms are equivalent to MLEs. \n",
        "Theory is a guide to the statistical researcher. \n",
        "\n",
        "**LEMMA 1:** \n",
        "\n",
        "Let the following solution exist uniquely. \n",
        "\n",
        "$$ \\hat \\theta_L = \\arg \\max_{\\theta \\in \\Theta} n^{-1} \\log f_X(X;\\theta) - \\lambda g(\\theta) $$\n",
        "\n",
        "Then then there exists linear subspace $H \\subset \\Theta$ \n",
        "such that $\\hat \\theta_L$ is the solution to the following optimization program. \n",
        "\n",
        "$$ \\hat \\theta_L = \\arg \\max_{\\theta \\in H} \\log f_X(X; \\theta) $$\n",
        "\n",
        "**PROOF:** \n",
        "\n",
        "Define $\\hat{\\mathcal{L}} := n^{-1} \\log f_X(X;\\theta) - \\lambda g(\\theta)$. \n",
        "Since $\\hat \\theta_L = \\arg \\max_{\\theta \\in \\Theta} \\hat{\\mathcal{L}}$ exists uniquely, \n",
        "and $\\log f_X, g$ are differentiable, so we are guaranteed that \n",
        "\n",
        "1. $\\nabla_\\theta \\hat{\\mathcal{L}} = 0$ and \n",
        "2. $\\nabla_\\lambda \\hat{\\mathcal{L}} = 0$. \n",
        "\n",
        "There exists $c = g(\\hat \\theta_L)$ \n",
        "and $\\mathcal{L} := n^{-1} \\log f_X(X;\\theta) - \\lambda( g(\\theta) - c )$ such that \n",
        "\n",
        "3. $\\nabla_\\theta \\hat{\\mathcal{L}} = \\nabla_\\theta \\mathcal{L} = 0$ and \n",
        "4. $\\nabla_\\lambda \\mathcal{L} = c$. \n",
        "\n",
        "By (4), $\\theta \\in g^{-1}(c)$. \n",
        "For $\\theta$ sufficiently near $\\hat \\theta_L$, $g^{-1}(\\theta) \\approx \\left( J_{g, \\hat \\theta_L} \\right)^{-1} \\left(\\theta - \\hat \\theta_L \\right)$ by the inverse function theorem (TODO: check calcs, and use Constant Rank Theorem instead). \n",
        "Define this local linear subspace as $H := \\left\\{ \\theta \\; : \\; \\eta \\; s.t. \\; \\exists \\theta \\; \\& \\; \\eta = \\left( J_{g, \\hat \\theta_L} \\right)^{-1} \\left(\\theta - \\hat \\theta_L \\right) \\right\\}$ or $H$-space. \n",
        "\n",
        "Since $\\nabla_\\theta \\hat{\\mathcal{L}} = 0$ also holds, \n",
        "$\\mathcal{L}$ is a Langrangian constraining $\\theta$ to linear subspace $H \\subset \\Theta$. \n",
        "\n",
        "$\\square$"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RESULT 1:** \n",
        "\n",
        "If $\\hat \\theta_{MLE}:= \\arg\\max_{\\theta \\in \\Theta} \\log f_X(X;\\theta) $, \n",
        "then $\\hat \\theta_L$ is an MLE. \n",
        "\n",
        "**PROOF:** \n",
        "\n",
        "Since $\\hat \\theta_{MLE}$ is an MLE, $f_X$ is a likelihood function. \n",
        "By LEMMA 1, the estimation program is constrained to well-defined linear subspace $H \\subset \\Theta$. \n",
        "Hence $\\hat \\theta_L$ is indeed an MLE. \n",
        "\n",
        "$\\square$ "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Clarify $J$. \n",
        "\n",
        "**COROLLARY 1:** \n",
        "\n",
        "Define $J := \\left[ \\partial \\theta_0 / \\partial \\eta_j \\right]$, \n",
        "$J^+$ to be the pseudo-inverse matrix, \n",
        "and $\\mathcal{I}_H :=  J^+ \\mathcal{I}_{\\theta_0} J^{+T}$\n",
        "\n",
        "If $X_i \\sim  f_X(x; \\theta_0)$ and $\\hat \\theta_L$ is sufficiently near $\\hat \\theta_{MLE}$ and $n$ sufficiently large, \n",
        "then $\\hat \\theta_L$ has a corresponding estimate $\\hat \\eta$ in the $H$-space basis,  \n",
        "and $\\sqrt{n} \\left( \\hat \\eta - J^+ \\theta_0 \\right) \\sim N(0, \\mathcal{I}_H )$. \n",
        "\n",
        "**PROOF:** \n",
        "\n",
        "$0 = \\nabla_\\theta \\log f_X(X; \\theta) $\n",
        "\n",
        "$\\approx \\nabla_\\theta \\log f_X(X; \\theta_0) - \\left( \\theta - \\theta_0 \\right)^T \\nabla_\\theta^T \\nabla_\\theta \\log f_X(X; \\theta_0) $ by Taylor expansion \n",
        "\n",
        "$ \\Rightarrow \\sqrt{n}^{-1} \\nabla_\\theta \\log f_X(X; \\theta_0) \\approx \\frac{\\sqrt{n}}{n} \\left( \\theta - \\theta_0 \\right)^T \\nabla_\\theta^T \\nabla_\\theta \\log f_X(X; \\theta_0) $\n",
        "\n",
        "$ \\Rightarrow N \\mathcal{I}_{\\theta_0}^{1/2} =_d \\sqrt{n} \\left( \\theta - \\theta_0 \\right)^T \\mathcal{I}_{\\theta_0} $ \n",
        "where $=_d$ is equivalence in distribution and $N \\sim N\\left(0, I_{\\dim(\\Theta)} \\right)$ \n",
        "\n",
        "Apply lemma 1 by parameterizing $H$-space via $\\theta \\gets J \\eta$.\n",
        "\n",
        "$ \\Rightarrow \\sqrt{n} \\left( J \\eta - \\theta_0 \\right) \\sim N(0, \\mathcal{I}_{\\theta_0}) $\n",
        "\n",
        "$ \\Rightarrow \\sqrt{n} \\left( \\eta - J^+ \\theta_0 \\right) \\sim N(0, J^+ \\mathcal{I}_{\\theta_0} J^{+T} ) $ \n",
        "\n",
        "$\\square$ \n",
        "\n",
        "Since $J^+$ is also a projection matrix, the distribution of $\\hat \\eta$ is equivalent to the $\\hat \\theta_{MLE}$ projection, \n",
        "$\\sqrt{n} J^+ \\left( \\hat \\theta_{MLE} - \\theta_0 \\right) \\sim N(0, \\mathcal{I}_H )$. \n",
        "\n",
        "**COROLLARY 2:** \n",
        "\n",
        "$\\text{rank}(J^+) > 0 \\Rightarrow \\mathcal{I}_H < \\mathcal{I}_{\\theta_0}$ \n",
        "\n",
        "**PROOF:** \n",
        "\n",
        "Since $\\mathcal{I}_{\\theta_0} > 0$ (positive semi-definite), there is matrix $A$ such that $\\mathcal{I}_{\\theta_0} = AA^T $. \n",
        "Since $J^+$ is a projection matrix, we have the following.\n",
        "\n",
        "$ \\mathcal{I}_{\\theta_0} = \\mathcal{I}_H + (I - J^+)AA^TJ^{+T} + J^+AA^T(I - J^+)^T + (I - J^+)AA^T(I - J^+)^T$ \n",
        "\n",
        "$ = \\mathcal{I}_H + 0 + 0 + (I - J^+)AA^T(I - J^+)^T $ by linear independence of complementrary projections \n",
        "\n",
        "Recognizing $(I - J^+)AA^T(I - J^+)^T > 0$ completes the proof. \n",
        "\n",
        "$\\square$ \n",
        "\n",
        "If $\\hat \\theta_L \\not= \\hat \\theta_{MLE}$ for all $n$, then $\\theta_0 \\not \\in H$, \n",
        "and we may interpret $H$ as the tangent space of a biased sub-manifold in $\\Theta$. \n",
        "However, since dimensional reductions cause $\\mathcal{I}_H < \\mathcal{I}_{\\theta_0}$, \n",
        "a sufficiently-unbiased manifold produces more-efficient estimators, \n",
        "particularly so if $\\theta_0 \\in \\lim_{n \\to \\infty} H$. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## result: elliptical sub-manifolds in $\\Theta$-space are memories\n",
        "\n",
        "Define a _memory_ as all information associated with samples $X_1, X_2, \\ldots, X_n$ under a model. \n",
        "Concretely, we'll use approximate MLE sufficient statistics $\\left( \\hat \\theta, \\hat{\\mathcal{I}} \\right)$.\n",
        "\n",
        "WLOG, assume $\\mathcal{I}_{\\theta_0} > 0$. \n",
        "If not, perfect correlations exist, so the model can be reparameterized to a lower dimension. \n",
        "$\\mathcal{I}_{\\theta_0} > 0 \\Rightarrow \\mathcal{I}_{\\theta_0} = AA^T \\; \\& \\; \\exists A^{-1}$. \n",
        "For all $B \\in \\mathbb{R}^{p \\times q}$ where $p = \\dim(\\Theta)$ and $q \\leq p$, \n",
        "there exists $J = A^{-1}B$ mapping from an unbiased, linear $H$-space to $\\Theta$-space, \n",
        "and $(\\theta - \\theta_0)^T AA^T (\\theta - \\theta_0) = (\\eta - \\eta_0)^T J^T AA^T J (\\eta - \\eta_0) = (\\eta - \\eta_0)^T BB^T (\\eta - \\eta_0)^T$.\n",
        "This is true for any choice of $B$, so covers all PSD matrices $BB^T \\geq 0$. \n",
        "\n",
        "Choosing $g(\\theta) = (\\theta - \\theta_0)^T BB^T (\\theta - \\theta_0)$ and applying lemma 1, \n",
        "we recognize the existence of a elliptical sub-manifold in $\\Theta$-space corresponding to Lagrangian-form regularizer $g(\\theta)$. \n",
        "Hence, there exists an MLE with $\\mathcal{I} = BB^T$ when estimation is constrained to a subspace with $J = A^{-1}B$. \n",
        "So, for every elliptical sub-manifold with Riemann metric $\\mathcal{I} = BB$, there exists another linear sub-manifold $H$ \n",
        "such that $\\arg\\max_{\\theta \\in H} \\log f_X(X;\\theta)$ has approximate sufficient statistics $(\\hat \\theta, BB^T)$. \n",
        "\n",
        "Since we interpret $(\\hat \\theta, BB^T)$ as a memory, \n",
        "every elliptical sub-manifold $(\\eta - \\eta_0)^T BB^T (\\eta - \\eta_0)^T$ \n",
        "has an associated memory. \n",
        "\n",
        "TODO: look for a 1-1 correspondence, tying each manifold to a unique memory. \n",
        "Example consideration: $(\\hat \\eta, BB^T)$ and $(\\eta - \\hat \\eta)^T BB^T (\\eta - \\hat \\eta)^T$. \n",
        "\n",
        "## likely decent definition for memory \n",
        "\n",
        "$(\\theta_H, H)$ for $\\theta_H \\in \\Theta \\subset \\mathbb{R}^p$ and smooth, $q$-dimensional sub-manifold $H \\subset \\Theta$\n",
        " with local tangent space $T_{\\theta_H}$ and associated total derivative $D_{\\theta_H} \\in \\mathbb{R}^{p \\times q}$, \n",
        " so that $\\hat \\theta_H = \\arg \\max_{\\theta \\in H} \\log f(X; \\theta) \\; \\& \\; X \\sim f(X; \\theta_H)$, \n",
        " and $\\mathcal{I}_{\\theta_H} = D_{\\theta_H}D_{\\theta_H}^T$. \n",
        "\n",
        " This $(\\theta_H, H)$ definition works, because it allows me to sneak-in other kinds of subspaces via Lagrangian constraints like \"$-\\lambda \\|\\hat{\\mathcal{I}}_\\theta \\|$\"."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## frontal lobe \n",
        "\n",
        "In humans, the frontal lobe operates on a longer-term timeline than Hippocampus-based short-term memory, \n",
        "but on a shorter horizon than remaining cortical regions, \n",
        "so functions as a mid-term horizon or _working space_ (TODO: CITE!). \n",
        "Observing miniaturization requirements on deep learning imposes parameter space restrictions, \n",
        "which deny infinite growth strategies like `net2net` (TODO: CITE!). \n",
        "Instead, we'll allocate dimensional parameter space to a flex capacity, \n",
        "so operating in similar capacity to a biological frontal lobe. \n",
        "Mathematically, we'll achieve this by all filling dimensional space with information, \n",
        "then periodically reducing information stored in the frontal lobe parameters. \n",
        "The net result is a temporary increase in dimensional space available to the model, \n",
        "which routinely moves information into long-term memory. \n",
        "This results in the following two-step optimization procedure.\n",
        "\n",
        "1. $ \\hat \\theta = \\arg \\max_\\theta \\log f_X(X; \\theta) $\n",
        "$ - n_A (\\theta - \\hat \\theta_A)^T \\hat{\\mathcal{I}}_{\\hat \\theta_A} (\\theta - \\hat \\theta_A) $\n",
        "2. $ \\hat \\theta = \\arg \\max_\\theta  \\log f_X(X; \\theta) $ \n",
        "$ - n_A (\\theta - \\hat \\theta_A)^T \\hat{\\mathcal{I}}_{\\hat \\theta_A} (\\theta - \\hat \\theta_A) $ \n",
        "$ - \\lambda \\| \\hat{\\mathcal{I}}_\\theta \\|_{FL} $\n",
        "\n",
        "Where $X$ has $n_B$ samples observed, and $\\hat \\theta_A$ was produced by memorizing $n_A$ samples. \n",
        "\n",
        "As we'll illustrate (TODO!), a frontal lobe is practically motivated in deep learning, \n",
        "because freezing all parameters in memory effectively consumes all dimensional space. \n",
        "By regularly relieving dimensional space, a necessary working space can be retained. \n",
        "\n",
        "Experimental design scratch: Repetitively add memorization tasks (ex. introducing more MNIST digits). \n",
        "- Control 1: No memorization. Expectation: catastrophic forgetting. \n",
        "- Control 2: Memorize, but do not include frontal lobe clearing. Expectation: limited memorization capacities in later iterations. \n",
        "- Experimental 1: Memorize with frontal lobe clearing. Expectation: improved memorization iterations. \n",
        "- Experimental X: Adjust Krylov dimension and frontal lobe hyper-parameter. \n",
        "  - Expectation 1: Higher Krylov dimensions increase memory space, but cost memory and computation. \n",
        "  - Expectation 2: $n_A$ may be a natural parameter for the frontal lobe. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## references \n",
        "\n",
        "\\[1\\] Kirkpatrick et al. (2017) \"Overcoming catastrophic forgetting in neural networks\", PNAS"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK V2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}