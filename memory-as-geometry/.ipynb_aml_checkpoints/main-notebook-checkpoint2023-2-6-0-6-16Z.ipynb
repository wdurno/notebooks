{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# memory as geometry \r\n",
        "\r\n",
        "Under asymptotic analysis and mild regularity assumptions, all statistical models have a Lagrangian-form regularizer equivalent to memory \\[1\\]. \r\n",
        "We will mathematically generalize this concept to show a breadth of geometries equivalent to memory. \r\n",
        "As a corollary, we'll construct universal sufficient statistics. \r\n",
        "Finally, given large models' recent immense productivity, it is natural to expect hardware limitations to soon motivate miniaturization (TODO CITE). \r\n",
        "We'll leverage memory's geometric properties to construct miniaturization techniques for deep learning, including \r\n",
        "memory merges, regularizers as memory, and a _frontal lobe_ concept. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## result: Langrangian-regularized estimates are MLEs \n",
        "\n",
        "The MLE's powerful theoretical guarantees provide an important foundation for Deep Learning's success. \n",
        "So, we should be curious as to when our new estimation paradigms are equivalent to MLEs. \n",
        "Theory is a guide to the statistical researcher. \n",
        "\n",
        "**LEMMA 1:** \n",
        "\n",
        "Let the following solution exist uniquely. \n",
        "\n",
        "$$ \\hat \\theta_L = \\arg \\max_{\\theta \\in \\Theta} n^{-1} \\log f_X(X;\\theta) - \\lambda g(\\theta) $$\n",
        "\n",
        "Then then there exists linear subspace $H \\subset \\Theta$ \n",
        "such that $\\hat \\theta_L$ is the solution to the following optimization program. \n",
        "\n",
        "$$ \\hat \\theta_L = \\arg \\max_{\\theta \\in H} \\log f_X(X; \\theta) $$\n",
        "\n",
        "**PROOF:** \n",
        "\n",
        "Define $\\hat{\\mathcal{L}} := n^{-1} \\log f_X(X;\\theta) - \\lambda g(\\theta)$. \n",
        "Since $\\hat \\theta_L = \\arg \\max_{\\theta \\in \\Theta} \\hat{\\mathcal{L}}$ exists uniquely, \n",
        "and $\\log f_X, g$ are differentiable, so we are guaranteed that \n",
        "\n",
        "1. $\\nabla_\\theta \\hat{\\mathcal{L}} = 0$ and \n",
        "2. $\\nabla_\\lambda \\hat{\\mathcal{L}} = 0$. \n",
        "\n",
        "There exists $c = g(\\hat \\theta_L)$ \n",
        "and $\\mathcal{L} := n^{-1} \\log f_X(X;\\theta) - \\lambda( g(\\theta) - c )$ such that \n",
        "\n",
        "3. $\\nabla_\\theta \\hat{\\mathcal{L}} = \\nabla_\\theta \\mathcal{L} = 0$ and \n",
        "4. $\\nabla_\\lambda \\mathcal{L} = c$. \n",
        "\n",
        "By (4), $\\theta \\in g^{-1}(c)$. \n",
        "For $\\theta$ sufficiently near $\\hat \\theta_L$, $g^{-1}(\\theta) \\approx \\left( J_{g, \\hat \\theta_L} \\right)^{-1} \\left(\\theta - \\hat \\theta_L \\right)$ by the inverse function theorem (TODO: check calcs, and use Constant Rank Theorem instead). \n",
        "Define this local linear subspace as $H := \\left\\{ \\theta \\; : \\; \\eta \\; s.t. \\; \\exists \\theta \\; \\& \\; \\eta = \\left( J_{g, \\hat \\theta_L} \\right)^{-1} \\left(\\theta - \\hat \\theta_L \\right) \\right\\}$ or $H$-space. \n",
        "\n",
        "Since $\\nabla_\\theta \\hat{\\mathcal{L}} = 0$ also holds, \n",
        "$\\mathcal{L}$ is a Langrangian constraining $\\theta$ to linear subspace $H \\subset \\Theta$. \n",
        "\n",
        "$\\square$"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RESULT 1:** \n",
        "\n",
        "If $\\hat \\theta_{MLE}:= \\arg\\max_{\\theta \\in \\Theta} \\log f_X(X;\\theta) $, \n",
        "then $\\hat \\theta_L$ is an MLE. \n",
        "\n",
        "**PROOF:** \n",
        "\n",
        "Since $\\hat \\theta_{MLE}$ is an MLE, $f_X$ is a likelihood function. \n",
        "By LEMMA 1, the estimation program is constrained to well-defined linear subspace $H \\subset \\Theta$. \n",
        "Hence $\\hat \\theta_L$ is indeed an MLE. \n",
        "\n",
        "$\\square$ "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Clarify $J$. \n",
        "\n",
        "**COROLLARY 1:** \n",
        "\n",
        "Define $J := \\left[ \\partial \\theta_0 / \\partial \\eta_j \\right]$, \n",
        "$J^+$ to be the pseudo-inverse matrix, \n",
        "and $\\mathcal{I}_H :=  J^+ \\mathcal{I}_{\\theta_0} J^{+T}$\n",
        "\n",
        "If $X_i \\sim  f_X(x; \\theta_0)$ and $\\hat \\theta_L$ is sufficiently near $\\hat \\theta_{MLE}$ and $n$ sufficiently large, \n",
        "then $\\hat \\theta_L$ has a corresponding estimate $\\hat \\eta$ in the $H$-space basis,  \n",
        "and $\\sqrt{n} \\left( \\hat \\eta - J^+ \\theta_0 \\right) \\sim N(0, \\mathcal{I}_H )$. \n",
        "\n",
        "**PROOF:** \n",
        "\n",
        "$0 = \\nabla_\\theta \\log f_X(X; \\theta) $\n",
        "\n",
        "$\\approx \\nabla_\\theta \\log f_X(X; \\theta_0) - \\left( \\theta - \\theta_0 \\right)^T \\nabla_\\theta^T \\nabla_\\theta \\log f_X(X; \\theta_0) $ by Taylor expansion \n",
        "\n",
        "$ \\Rightarrow \\sqrt{n}^{-1} \\nabla_\\theta \\log f_X(X; \\theta_0) \\approx \\frac{\\sqrt{n}}{n} \\left( \\theta - \\theta_0 \\right)^T \\nabla_\\theta^T \\nabla_\\theta \\log f_X(X; \\theta_0) $\n",
        "\n",
        "$ \\Rightarrow N \\mathcal{I}_{\\theta_0}^{1/2} =_d \\sqrt{n} \\left( \\theta - \\theta_0 \\right)^T \\mathcal{I}_{\\theta_0} $ \n",
        "where $=_d$ is equivalence in distribution and $N \\sim N\\left(0, I_{\\dim(\\Theta)} \\right)$ \n",
        "\n",
        "Apply lemma 1 by parameterizing $H$-space via $\\theta \\gets J \\eta$.\n",
        "\n",
        "$ \\Rightarrow \\sqrt{n} \\left( J \\eta - \\theta_0 \\right) \\sim N(0, \\mathcal{I}_{\\theta_0}) $\n",
        "\n",
        "$ \\Rightarrow \\sqrt{n} \\left( \\eta - J^+ \\theta_0 \\right) \\sim N(0, J^+ \\mathcal{I}_{\\theta_0} J^{+T} ) $ \n",
        "\n",
        "$\\square$ \n",
        "\n",
        "Since $J^+$ is also a projection matrix, the distribution of $\\hat \\eta$ is equivalent to the $\\hat \\theta_{MLE}$ projection, \n",
        "$\\sqrt{n} J^+ \\left( \\hat \\theta_{MLE} - \\theta_0 \\right)$. \n",
        "\n",
        "**COROLLARY 2:** \n",
        "\n",
        "$\\text{rank}(J^+) > 0 \\Rightarrow \\mathcal{I}_H < \\mathcal{I}_{\\theta_0}$ \n",
        "\n",
        "**PROOF:** \n",
        "\n",
        "Since $\\mathcal{I}_{\\theta_0} > 0$ (positive semi-definite), there is matrix $A$ such that $\\mathcal{I}_{\\theta_0} = AA^T $. \n",
        "Since $J^+$ is a projection matrix, we have the following.\n",
        "\n",
        "$ \\mathcal{I}_{\\theta_0} = \\mathcal{I}_H + (I - J^+)AA^TJ^{+T} + J^+AA^T(I - J^+)^T + (I - J^+)AA^T(I - J^+)^T$ \n",
        "\n",
        "$ = \\mathcal{I}_H + 0 + 0 + (I - J^+)AA^T(I - J^+)^T $ by linear independence of complementrary projections \n",
        "\n",
        "Recognizing $(I - J^+)AA^T(I - J^+)^T > 0$ completes the proof. \n",
        "\n",
        "$\\square$ \n",
        "\n",
        "If $\\hat \\theta_L \\not= \\hat \\theta_{MLE}$ for all $n$, then $\\theta_0 \\not \\in H$, \n",
        "and we may interpret $H$ as the tangent space of a biased sub-manifold in $\\Theta$. \n",
        "However, since dimensional reductions cause $\\mathcal{I}_H < \\mathcal{I}_{\\theta_0}$, \n",
        "a sufficiently-unbiased manifold produces more-efficient estimators, \n",
        "particularly so if $\\theta_0 \\in \\lim_{n \\to \\infty} H$. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## references \r\n",
        "\r\n",
        "\\[1\\] Kirkpatrick et al. (2017) \"Overcoming catastrophic forgetting in neural networks\", PNAS"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK V2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}