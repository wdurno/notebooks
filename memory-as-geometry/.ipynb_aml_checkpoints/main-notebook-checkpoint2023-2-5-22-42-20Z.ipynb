{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# memory as geometry \r\n",
        "\r\n",
        "Under asymptotic analysis and mild regularity assumptions, all statistical models have a Lagrangian-form regularizer equivalent to memory \\[1\\]. \r\n",
        "We will mathematically generalize this concept to show a breadth of geometries equivalent to memory. \r\n",
        "As a corollary, we'll construct universal sufficient statistics. \r\n",
        "Finally, given large models' recent immense productivity, it is natural to expect hardware limitations to soon motivate miniaturization (TODO CITE). \r\n",
        "We'll leverage memory's geometric properties to construct miniaturization techniques for deep learning, including \r\n",
        "memory merges, regularizers as memory, and a _frontal lobe_ concept. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## result: Langrangian-regularized estimates are MLEs \n",
        "\n",
        "The MLE's powerful theoretical guarantees provide an important foundation for Deep Learning's success. \n",
        "So, we should be curious as to when our new estimation paradigms are equivalent to MLEs. \n",
        "Theory is a guide to the statistical researcher. \n",
        "\n",
        "**LEMMA 1:** \n",
        "\n",
        "Let the following solution exist uniquely. \n",
        "\n",
        "$$ \\hat \\theta_L = \\arg \\max_{\\theta \\in \\Theta} n^{-1} \\log f_X(X;\\theta) - \\lambda g(\\theta) $$\n",
        "\n",
        "Then then there exists linear subspace $H \\subset \\Theta$ \n",
        "such that $\\hat \\theta_L$ is the solution to the following optimization program. \n",
        "\n",
        "$$ \\hat \\theta_L = \\arg \\max_{\\theta \\in H} \\log f_X(X; \\theta) $$\n",
        "\n",
        "**PROOF:** \n",
        "\n",
        "Define $\\hat{\\mathcal{L}} := n^{-1} \\log f_X(X;\\theta) - \\lambda g(\\theta)$. \n",
        "Since $\\hat \\theta_L = \\arg \\max_{\\theta \\in \\Theta} \\hat{\\mathcal{L}}$ exists uniquely, \n",
        "and $\\log f_X, g$ are differentiable, so we are guaranteed that \n",
        "\n",
        "1. $\\nabla_\\theta \\hat{\\mathcal{L}} = 0$ and \n",
        "2. $\\nabla_\\lambda \\hat{\\mathcal{L}} = 0$. \n",
        "\n",
        "There exists $c = g(\\hat \\theta_L)$ \n",
        "and $\\mathcal{L} := n^{-1} \\log f_X(X;\\theta) - \\lambda( g(\\theta) - c )$ such that \n",
        "\n",
        "3. $\\nabla_\\theta \\hat{\\mathcal{L}} = \\nabla_\\theta \\mathcal{L} = 0$ and \n",
        "4. $\\nabla_\\lambda \\mathcal{L} = c$. \n",
        "\n",
        "By (4), $\\theta \\in g^{-1}(c)$. \n",
        "For $\\theta$ sufficiently near $\\hat \\theta_L$, $g^{-1}(\\theta) \\approx \\left( J_{g, \\hat \\theta_L} \\right)^{-1} \\left(\\theta - \\hat \\theta_L \\right)$ by the inverse function theorem (TODO: check calc, and use Constant Rank Theorem instead). \n",
        "Define this local linear subspace as $H := \\left\\{ \\theta \\; : \\; \\eta \\; s.t. \\; \\exists \\theta \\; \\& \\; \\eta = \\left( J_{g, \\hat \\theta_L} \\right)^{-1} \\left(\\theta - \\hat \\theta_L \\right) \\right\\}$ or $H$-space. \n",
        "\n",
        "Since $\\nabla_\\theta \\hat{\\mathcal{L}} = 0$ also holds, \n",
        "$\\mathcal{L}$ is a Langrangian constraining $\\theta$ to linear subspace $H \\subset \\Theta$. \n",
        "\n",
        "$\\square$"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RESULT 1:** \n",
        "\n",
        "If $\\hat \\theta_{MLE}:= \\arg\\max_{\\theta \\in \\Theta} \\log f_X(X;\\theta) $, \n",
        "then $\\hat \\theta_L$ is an MLE. \n",
        "\n",
        "**PROOF:** \n",
        "\n",
        "Since $\\hat \\theta_{MLE}$ is an MLE, $f_X$ is a likelihood function. \n",
        "By LEMMA 1, the estimation program is constrained to well-defined linear subspace $H \\subset \\Theta$. \n",
        "Hence $\\hat \\theta_L$ is indeed an MLE. \n",
        "\n",
        "$\\square$ "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Corollaries: (A) calculate distribution of $\\hat \\theta_L$, and (B) illustrate variance reduction with (un)biased sub-manifolds. \n",
        "\n",
        "**COROLLARY 1:** \n",
        "\n",
        "If $X_i \\sim  f_X(x; \\theta_0)$ and $\\hat \\theta_L$ is sufficiently near $\\hat \\theta_{MLE}$ and $n$ sufficiently large, \n",
        "then $\\sqrt{n} \\left( \\hat \\theta_L - \\mathbb{E} \\hat \\theta_L \\right) \\sim TODO$. \n",
        "\n",
        "**PROOF:** \n",
        "\n",
        "Since $n$ large, $\\sqrt{n} \\left( \\hat \\theta_{MLE} - \\theta_0 \\right) \\sim N(0, \\mathcal{I}_{\\theta_0}) $ \n",
        "\n",
        "TODO \n",
        "\n",
        "$\\square$ \n",
        "\n",
        "Notice that if $\\hat \\theta_L \\not= \\hat \\theta_{MLE}$ for all $n$, then $\\theta_0 \\not \\in H$, \n",
        "and we may interpret $H$ as the tangent space of a biased sub-manifold in $\\Theta$. \n",
        "However, since dimensional reductions can cause $\\mathcal{I}_H < \\mathcal{I}_{\\theta_0}$, \n",
        "a sufficiently-unbiased manifold is capable of more-efficient estimators. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## references \r\n",
        "\r\n",
        "\\[1\\] Kirkpatrick et al. (2017) \"Overcoming catastrophic forgetting in neural networks\", PNAS"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK V2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}